{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"xInfer: Maximum Performance, Minimum Effort.","text":"<p>xInfer is a C++ inference toolkit designed for a single purpose: to run your trained deep learning models with the absolute maximum performance possible on NVIDIA GPUs.</p> <p>It is the definitive solution for C++ developers who need to deploy AI in latency-critical, high-throughput, or power-constrained environments.</p>"},{"location":"#the-problem-the-ai-deployment-wall","title":"The Problem: The AI Deployment Wall","text":"<p>Deploying AI models in production-grade C++ is brutally difficult. Developers hit a wall of complexity that forces them to choose between performance and productivity.</p> <ul> <li>High-Level Frameworks (like LibTorch): Easy to use, but their eager-mode execution has overhead, and they lack a streamlined path to state-of-the-art inference optimization.</li> <li>Low-Level Libraries (like TensorRT): Incredibly powerful, but have a notoriously steep learning curve, a verbose API, and require you to write complex, boilerplate-heavy code for even simple tasks.</li> </ul> <p>This leaves a massive gap for developers who need both the simplicity of a high-level API and the bare-metal speed of a low-level one.</p>"},{"location":"#the-solution-xinfer","title":"The Solution: xInfer","text":"<p><code>xInfer</code> fills this gap by providing a complete, two-layer solution built on top of NVIDIA TensorRT.</p>"},{"location":"#1-the-xinferzoo-the-easy-button-api","title":"1. The <code>xInfer::zoo</code> - The \"Easy Button\" API","text":"<p>A comprehensive library of pre-packaged, task-oriented pipelines for the most common AI models. With the <code>zoo</code>, you can get a hyper-performant, state-of-the-art object detector or diffusion model running in just a few lines of C++.</p> <p>It's this simple: ```cpp</p>"},{"location":"#include","title":"include","text":""},{"location":"#include_1","title":"include  <p>int main() {     // 1. Configure the detector to use your pre-built TensorRT engine     xinfer::zoo::vision::DetectorConfig config;     config.engine_path = \"yolov8n.engine\";     config.labels_path = \"coco.names\";</p> <pre><code>// 2. Initialize the detector. All complexity is handled internally.\nxinfer::zoo::vision::ObjectDetector detector(config);\n\n// 3. Load an image and predict in one line.\ncv::Mat image = cv::imread(\"my_image.jpg\");\nauto detections = detector.predict(image);\n</code></pre> <p>}</p>","text":""},{"location":"installation/","title":"Installation","text":"<p>Welcome to <code>xInfer</code>! This guide will walk you through the process of setting up your environment and building the library from source.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p><code>xInfer</code> is a high-performance library that sits on top of the NVIDIA software stack. Before you begin, you must have the following components installed on your system.</p>"},{"location":"installation/#1-nvidia-driver","title":"1. NVIDIA Driver","text":"<p>You need a recent NVIDIA driver that supports your GPU and CUDA Toolkit version. You can check your driver version by running: <pre><code>nvidia-smi\n</code></pre></p>"},{"location":"installation/#2-nvidia-cuda-toolkit","title":"2. NVIDIA CUDA Toolkit","text":"<p><code>xInfer</code> is built on CUDA. We recommend CUDA 11.8 or newer. - Verification: Check if <code>nvcc</code> is installed and in your PATH:   <pre><code>nvcc --version\n</code></pre> - Installation: If you don't have it, download and install it from the NVIDIA CUDA Toolkit Archive.</p>"},{"location":"installation/#3-nvidia-cudnn","title":"3. NVIDIA cuDNN","text":"<p>TensorRT requires the cuDNN library for accelerating deep learning primitives. We recommend cuDNN 8.6 or newer. - Verification: Check for the cuDNN header file:   <pre><code>ls /usr/include/cudnn.h\n</code></pre> - Installation: Download and install it from the NVIDIA cuDNN Archive. Make sure to follow the installation instructions for copying the header and library files to your CUDA Toolkit directory.</p>"},{"location":"installation/#4-nvidia-tensorrt","title":"4. NVIDIA TensorRT","text":"<p>TensorRT is the core optimization engine used by <code>xInfer</code>. We recommend TensorRT 8.6 or newer. - Verification: Check if the TensorRT header <code>NvInfer.h</code> exists:   <pre><code>ls /usr/include/x86_64-linux-gnu/NvInfer.h\n</code></pre>   (The path may vary depending on your installation method). - Installation: Download and install it from the NVIDIA TensorRT page. The <code>.deb</code> or <code>.rpm</code> package installation is the easiest method.</p>"},{"location":"installation/#5-xtorch-library","title":"5. <code>xTorch</code> Library","text":"<p>The <code>xInfer::builders</code> and <code>zoo</code> modules are designed to work seamlessly with models trained or defined in <code>xTorch</code>. - Installation: You must build and install <code>xTorch</code> first. Please follow the instructions at the xTorch GitHub Repository.   <pre><code>git clone https://github.com/your-username/xtorch.git\ncd xtorch\nmkdir build &amp;&amp; cd build\ncmake ..\nmake -j\nsudo make install\n</code></pre></p>"},{"location":"installation/#6-other-dependencies-compiler-cmake-opencv","title":"6. Other Dependencies (Compiler, CMake, OpenCV)","text":"<p>You will need a modern C++ compiler and the build tools.</p> <p><pre><code>sudo apt-get update\nsudo apt-get install -y build-essential cmake libopencv-dev libcurl4-openssl-dev\n</code></pre> - CMake: Version 3.18 or newer is required. - OpenCV: Required for image processing in the <code>zoo</code> and examples.</p>"},{"location":"installation/#building-xinfer-from-source-recommended","title":"Building <code>xInfer</code> from Source (Recommended)","text":"<p>This is the standard method for building and installing <code>xInfer</code>.</p>"},{"location":"installation/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<pre><code>git clone https://github.com/your-username/xinfer.git\ncd xinfer\n</code></pre>"},{"location":"installation/#step-2-configure-with-cmake","title":"Step 2: Configure with CMake","text":"<p>Create a build directory and run CMake. This command will find all the dependencies you installed and prepare the build files.</p> <pre><code>mkdir build\ncd build\ncmake ..\n</code></pre> <p>Troubleshooting Dependency Issues</p> <p>If CMake has trouble finding a specific library (especially TensorRT if you installed it from a <code>.zip</code> file), you can give it a hint with the <code>-D</code> flag. For example: <pre><code>cmake -DTensorRT_ROOT=/path/to/your/TensorRT-10.x.x.x ..\n</code></pre></p>"},{"location":"installation/#step-3-compile","title":"Step 3: Compile","text":"<p>Use <code>make</code> to compile the library. This will create the <code>libxinfer.so</code> shared library and all the example executables.</p> <pre><code># Use -j to specify the number of parallel jobs to speed up compilation\nmake -j$(nproc)\n</code></pre>"},{"location":"installation/#step-4-optional-run-tests","title":"Step 4: (Optional) Run Tests","text":"<p>After a successful build, you can run the example executable to verify that everything is working.</p> <p><pre><code># From the build directory\n./xinfer_example```\nYou should see the output from the example programs, indicating a successful build.\n\n### Step 5: (Optional) Install System-Wide\n\nIf you want to use `xInfer` as a dependency in other C++ projects on your system, you can install it. This will copy the library files and headers to system directories (like `/usr/local/lib` and `/usr/local/include`).\n\n```bash\nsudo make install\n</code></pre> After this step, other CMake projects can find and use your library with a simple <code>find_package(xinfer REQUIRED)</code> command.</p>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>Congratulations! You have successfully built and installed <code>xInfer</code>.</p> <ul> <li>Check out the \ud83d\ude80 Quickstart guide to run your first high-performance inference pipeline.</li> <li>Explore the Model Zoo API to see all the pre-packaged solutions you can use out-of-the-box.</li> </ul>"},{"location":"quickstart/","title":"\ud83d\ude80 Quickstart: Real-Time Object Detection in 5 Minutes","text":"<p>Welcome to the <code>xInfer</code> quickstart guide! This tutorial will walk you through the entire high-performance pipeline, from downloading a standard ONNX model to running it for real-time object detection in a C++ application.</p> <p>By the end of this guide, you will have a clear understanding of the core <code>xInfer</code> workflow and the power of the <code>zoo</code> API.</p> <p>What You'll Accomplish: 1.  Download a pre-trained YOLOv8 object detection model. 2.  Optimize it into a hyper-performant TensorRT engine using the <code>xinfer-cli</code> tool. 3.  Use the <code>xinfer::zoo::vision::Detector</code> in a simple C++ application to run inference on an image.</p> <p>Prerequisites: - You have successfully installed <code>xInfer</code> and its dependencies by following the Installation guide. - You have the <code>xinfer-cli</code> and <code>xinfer_example</code> executables in your <code>build</code> directory.</p>"},{"location":"quickstart/#step-1-get-a-pre-trained-model","title":"Step 1: Get a Pre-Trained Model","text":"<p>First, we need a model to optimize. We'll use the popular YOLOv8-Nano, a small and fast object detector trained on the COCO dataset. It's perfect for a quickstart guide.</p> <p>Let's download the ONNX version of the model.</p> <pre><code># From your project's root directory\nmkdir -p assets\nwget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.onnx -O assets/yolov8n.onnx\n</code></pre> <p>We also need the list of class names for the COCO dataset.</p> <pre><code>wget https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names -O assets/coco.names\n</code></pre> <p>You should now have an <code>assets</code> directory containing <code>yolov8n.onnx</code> and <code>coco.names</code>.</p>"},{"location":"quickstart/#step-2-optimize-with-xinfer-cli-the-f1-car-build","title":"Step 2: Optimize with <code>xinfer-cli</code> (The \"F1 Car\" Build)","text":"<p>This is where the magic happens. We will use the <code>xinfer-cli</code> tool to convert the standard <code>.onnx</code> file into a hyper-optimized <code>.engine</code> file. We'll enable FP16 precision, which provides a ~2x speedup on modern NVIDIA GPUs.</p> <pre><code># Navigate to your build directory where the CLI tool was created\ncd build/tools/xinfer-cli\n\n# Run the build command\n./xinfer-cli --build \\\n    --onnx ../../assets/yolov8n.onnx \\\n    --save_engine ../../assets/yolov8n_fp16.engine \\\n    --fp16\n</code></pre> <p>You will see output from the TensorRT builder as it analyzes, optimizes, and fuses the layers of the model. After a minute or two, you will have a new file: <code>assets/yolov8n_fp16.engine</code>. This file is a fully compiled, self-contained inference engine tuned for your specific GPU.</p> <p>What just happened?</p> <p>You just performed a complex, ahead-of-time compilation that would have required hundreds of lines of verbose C++ TensorRT code. <code>xinfer-cli</code> automated this entire process with a single, clear command.</p>"},{"location":"quickstart/#step-3-use-the-engine-in-c-the-easy-button-api","title":"Step 3: Use the Engine in C++ (The \"Easy Button\" API)","text":"<p>Now, we'll write a very simple C++ program to use our new engine. This demonstrates the power and simplicity of the <code>xinfer::zoo</code> API.</p> <p>Create a new file named <code>quickstart_detector.cpp</code> in your <code>examples</code> directory.</p> <p>File: <code>examples/quickstart_detector.cpp</code> <pre><code>#include &lt;xinfer/zoo/vision/detector.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n#include &lt;stdexcept&gt;\n\nint main() {\n    try {\n        // 1. Configure the detector to use our new engine and labels.\n        xinfer::zoo::vision::DetectorConfig config;\n        config.engine_path = \"assets/yolov8n_fp16.engine\";\n        config.labels_path = \"assets/coco.names\";\n        config.confidence_threshold = 0.5f;\n\n        // 2. Initialize the detector.\n        // This is a fast, one-time setup that loads the optimized engine.\n        std::cout &lt;&lt; \"Loading object detector...\\n\";\n        xinfer::zoo::vision::ObjectDetector detector(config);\n\n        // 3. Load an image to run inference on.\n        // (Create a simple dummy image for this test)\n        cv::Mat image = cv::Mat(480, 640, CV_8UC3, cv::Scalar(114, 144, 154));\n        cv::putText(image, \"xInfer Quickstart!\", cv::Point(50, 240), cv::FONT_HERSHEY_SIMPLEX, 1.5, cv::Scalar(255, 255, 255), 3);\n        cv::imwrite(\"quickstart_input.jpg\", image);\n        std::cout &lt;&lt; \"Created a dummy image: quickstart_input.jpg\\n\";\n\n        // 4. Predict in a single line of code.\n        // xInfer handles all the pre-processing, inference, and NMS post-processing.\n        std::cout &lt;&lt; \"Running prediction...\\n\";\n        std::vector&lt;xinfer::zoo::vision::BoundingBox&gt; detections = detector.predict(image);\n\n        // 5. Print and draw the results.\n        std::cout &lt;&lt; \"\\nFound \" &lt;&lt; detections.size() &lt;&lt; \" objects (this will be 0 on a dummy image).\\n\";\n        for (const auto&amp; box : detections) {\n            std::cout &lt;&lt; \" - \" &lt;&lt; box.label &lt;&lt; \" (Confidence: \" &lt;&lt; box.confidence &lt;&lt; \")\\n\";\n            cv::rectangle(image, cv::Point(box.x1, box.y1), cv::Point(box.x2, box.y2), cv::Scalar(0, 255, 0), 2);\n        }\n\n        cv::imwrite(\"quickstart_output.jpg\", image);\n        std::cout &lt;&lt; \"Saved annotated image to quickstart_output.jpg\\n\";\n\n    } catch (const std::exception&amp; e) {\n        std::cerr &lt;&lt; \"An error occurred: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n        return 1;\n    }\n\n    return 0;\n}\n</code></pre></p> <p>You will need to add this new example to your root <code>CMakeLists.txt</code> to build it: <pre><code># In your root CMakeLists.txt\n# ... after the existing add_executable(xinfer_example ...)\nadd_executable(quickstart_detector examples/quickstart_detector.cpp)\ntarget_link_libraries(quickstart_detector PRIVATE xinfer)\n</code></pre></p> <p>Now, rebuild and run your new example: <pre><code># From your build directory\ncmake ..\nmake\n\n# Run the quickstart\n./examples/quickstart_detector\n</code></pre></p>"},{"location":"quickstart/#conclusion","title":"Conclusion","text":"<p>Congratulations! In just a few minutes, you have: - Taken a standard open-source model. - Converted it into a hyper-performant engine tuned for your hardware. - Used it in a clean, simple, and production-ready C++ application.</p> <p>This is the core workflow that <code>xInfer</code> is designed to perfect. You are now ready to explore the rest of the Model Zoo or dive into the How-to Guides to learn how to optimize your own custom models.</p>"},{"location":"concepts/philosophy/","title":"Philosophy: The F1 Car and the Porsche","text":"<p><code>xInfer</code> is built on a strong, opinionated philosophy about what high-performance AI in C++ should look like. To understand the design of <code>xInfer</code>, it helps to think about the difference between two kinds of high-performance machines: a Porsche 911 and a Formula 1 car.</p>"},{"location":"concepts/philosophy/#the-porsche-911-general-purpose-frameworks","title":"The \"Porsche 911\": General-Purpose Frameworks","text":"<p>A framework like PyTorch (and by extension, its C++ frontend, LibTorch) is like a Porsche 911. It is a masterpiece of engineering.</p> <ul> <li>Incredibly Capable: It can handle any task you throw at it. You can drive it to the grocery store, on a cross-country road trip, or even take it to a track day.</li> <li>Flexible &amp; User-Friendly: It has air conditioning, a GPS, comfortable seats, and power steering. It's designed to be usable by a wide range of drivers in a wide range of conditions.</li> <li>Very, Very Fast: For a road-legal car, a Porsche is astonishingly fast. It represents the peak of general-purpose performance.</li> </ul> <p>In the AI world, LibTorch is this Porsche. It provides a massive library of flexible layers (<code>nn::Conv2d</code>, <code>nn::Linear</code>), dynamic data structures (<code>torch::Tensor</code>), and the powerful <code>autograd</code> engine. It's an amazing tool for research, training, and general development.</p> <p>But a Porsche will never win a Formula 1 race.</p>"},{"location":"concepts/philosophy/#the-f1-car-xinfer-a-specialized-inference-machine","title":"The \"F1 Car\": <code>xInfer</code> - A Specialized Inference Machine","text":"<p>A Formula 1 car is a machine with a single, uncompromising purpose: to be the fastest possible vehicle around a racetrack.</p> <ul> <li>Brutally Specialized: An F1 car has no trunk, no radio, no air conditioning. Its steering is heavy and direct. It can only run on a specific type of fuel and on perfectly smooth asphalt. It is \"bad\" at almost every task except its one, designated purpose.</li> <li>Unbeatably Fast: Within that one context\u2014the racetrack\u2014it is in a completely different dimension of performance. It is so optimized for its task that it makes even the fastest supercar look like it's standing still.</li> </ul> <p>This is the philosophy of <code>xInfer</code>.</p> <p><code>xInfer</code> is not a training library. It is not a research tool. It is an inference deployment toolkit. It is your workshop for building an F1 car for your specific, pre-trained AI model.</p>"},{"location":"concepts/philosophy/#how-xinfer-implements-the-f1-car-philosophy","title":"How <code>xInfer</code> Implements the \"F1 Car\" Philosophy","text":"<p><code>xInfer</code> makes a series of deliberate trade-offs, sacrificing generality to gain a massive advantage in performance.</p>"},{"location":"concepts/philosophy/#1-ahead-of-time-compilation-the-engine-build","title":"1. Ahead-of-Time Compilation (The Engine Build)","text":"<ul> <li>The Trade-off: We sacrifice the flexibility of a dynamic graph. The <code>xInfer</code> workflow requires a separate, offline \"build\" step where your model is compiled into a rigid, static TensorRT engine.</li> <li>The Payoff: This ahead-of-time compilation is what allows for aggressive operator fusion. TensorRT can analyze your entire model and fuse a sequence like <code>Conv -&gt; Bias -&gt; ReLU</code> into a single, monolithic CUDA kernel. This is like replacing three separate parts of the Porsche's engine with a single, perfectly machined F1 component. It drastically reduces memory traffic and kernel launch overhead.</li> </ul>"},{"location":"concepts/philosophy/#2-static-specialization-the-zoo-api","title":"2. Static Specialization (The <code>zoo</code> API)","text":"<ul> <li>The Trade-off: The high-level <code>zoo</code> classes are not generic. The <code>zoo::vision::Detector</code> is not just a \"model\"; it is a complete, hard-coded pipeline that assumes it is running a YOLO-style object detection model.</li> <li>The Payoff: Because the pipeline is specialized, we can replace slow, general-purpose components with hyper-optimized, single-purpose ones. Instead of using a slow, CPU-based post-processing function for Non-Maximum Suppression, the <code>Detector</code> uses its own custom CUDA NMS kernel. This is like replacing the Porsche's all-season road tires with specialized F1 racing slicks.</li> </ul>"},{"location":"concepts/philosophy/#3-minimal-abstraction-the-core-toolkit","title":"3. Minimal Abstraction (The Core Toolkit)","text":"<ul> <li>The Trade-off: The low-level <code>xInfer</code> API is lean. It doesn't try to hide every detail of the underlying GPU operations.</li> <li>The Payoff: It gives expert developers direct, un-abstracted control over the GPU. You can manage your own CUDA streams, allocate your own <code>core::Tensor</code> buffers, and build custom pipelines piece by piece. This is the \"manual transmission\" that gives you maximum control and performance, whereas a general framework often forces you into an \"automatic\" mode.</li> </ul>"},{"location":"concepts/philosophy/#conclusion-the-right-tool-for-the-job","title":"Conclusion: The Right Tool for the Job","text":"<p>You don't take an F1 car to the grocery store. And you don't take a Porsche to the Monaco Grand Prix.</p> <ul> <li>Use <code>xTorch</code> (or Python PyTorch) when you need a Porsche: For research, for iterating on new model ideas, and for the entire training process.</li> <li>Use <code>xInfer</code> when you need an F1 car: For the final, critical step of production deployment, where every microsecond of latency and every watt of power consumption counts.</li> </ul> <p><code>xInfer</code> is built on the belief that for deployment, performance is not just a feature\u2014it is the entire product.</p>"},{"location":"concepts/workflow/","title":"The <code>xInfer</code> Workflow: Build Once, Infer Fast","text":"<p>The core philosophy of <code>xInfer</code> is to do as much work as possible ahead of time so that your final inference pipeline is as lean and fast as humanly possible. This is achieved through a clear, two-stage workflow.</p> <p> (You would create a simple diagram for this: [Model Training] -&gt; [ONNX] -&gt; [xInfer Build Step] -&gt; [TensorRT Engine] -&gt; [xInfer Inference Step])</p>"},{"location":"concepts/workflow/#stage-1-the-build-step-offline","title":"Stage 1: The Build Step (Offline)","text":"<p>This is the \"factory\" where you create your hyper-optimized \"F1 car\" engine. This is a one-time, offline process that you run during development or as part of your CI/CD pipeline.</p> <p>The goal of this stage is to convert a trained model from a flexible, high-level format into a rigid, hardware-specific, and incredibly fast TensorRT engine file.</p>"},{"location":"concepts/workflow/#1a-start-with-a-trained-model","title":"1a. Start with a Trained Model","text":"<p>Your journey begins with a model that has already been trained in a framework like PyTorch, TensorFlow, or your own <code>xTorch</code>. The output of this training process is a set of learned weights.</p>"},{"location":"concepts/workflow/#1b-export-to-a-standard-format-onnx","title":"1b. Export to a Standard Format: ONNX","text":"<p>TensorRT, the core optimization engine used by <code>xInfer</code>, works best with a standard, open-source format called ONNX (Open Neural Network Exchange). An ONNX file describes the architecture of your model in a way that different tools can understand.</p> <ul> <li>From <code>xTorch</code>: You can use the <code>xinfer::builders::export_to_onnx()</code> function to convert your trained <code>xTorch</code> models.</li> <li>From Python: You would use the standard <code>torch.onnx.export()</code> function.</li> </ul> <p>This step gives you a portable, framework-agnostic representation of your model (e.g., <code>yolov8n.onnx</code>).</p>"},{"location":"concepts/workflow/#1c-build-the-tensorrt-engine-with-xinfer","title":"1c. Build the TensorRT Engine with <code>xInfer</code>","text":"<p>This is the most critical part of the build step and where <code>xInfer</code> provides its first major value. You use the <code>xinfer-cli</code> tool or the <code>xinfer::builders::EngineBuilder</code> C++ class to perform the final compilation.</p> <p>This step does several powerful things: - Parses the ONNX Graph: It reads your model's architecture. - Applies Optimizations: It performs graph-level optimizations like layer fusion, combining <code>Conv+BN+ReLU</code> into a single operation. - Applies Quantization: If you enable it, it will convert the model's weights to faster, lower-precision formats like FP16 or INT8. - Hardware-Specific Tuning: It selects the absolute fastest, hand-tuned CUDA kernels (called \"tactics\") for each layer on your specific GPU architecture. - Serializes the Engine: It saves the final, fully optimized, and compiled model into a single binary file (e.g., <code>yolov8n_fp16.engine</code>).</p> <p>The output of this stage is a single <code>.engine</code> file. This file is the only artifact you need to ship with your final application.</p>"},{"location":"concepts/workflow/#stage-2-the-inference-step-real-time","title":"Stage 2: The Inference Step (Real-Time)","text":"<p>This is what happens inside your final, deployed C++ application. This stage is designed to be extremely fast, lightweight, and have minimal dependencies. Your application does not need the heavy ONNX parser or TensorRT builder libraries; it only needs the much smaller TensorRT runtime.</p>"},{"location":"concepts/workflow/#2a-load-the-engine","title":"2a. Load the Engine","text":"<p>Your application uses the <code>xinfer::zoo</code> or <code>xinfer::core</code> API to load the pre-built <code>.engine</code> file. This is a very fast operation, as it simply deserializes the compiled graph into GPU memory.</p> <pre><code>#include &lt;xinfer/zoo/vision/detector.h&gt;\n\n// The application only needs the final .engine file.\nxinfer::zoo::vision::DetectorConfig config;\nconfig.engine_path = \"yolov8n_fp16.engine\";\nconfig.labels_path = \"coco.names\";\n\nxinfer::zoo::vision::ObjectDetector detector(config);\n</code></pre>"},{"location":"concepts/workflow/#2b-pre-process-input-data","title":"2b. Pre-process Input Data","text":"<p>User input (like a camera frame or a piece of audio) needs to be converted into a GPU tensor. <code>xInfer</code> provides hyper-optimized, fused CUDA kernels for these tasks in the <code>xinfer::preproc</code> module to avoid CPU bottlenecks. The <code>zoo</code> classes handle this for you automatically.</p>"},{"location":"concepts/workflow/#2c-run-inference","title":"2c. Run Inference","text":"<p>You call the <code>.predict()</code> method. This is where the magic happens. The <code>xinfer::core::InferenceEngine</code> takes your input tensor and executes the pre-compiled, hyper-optimized graph on the GPU. This is the fastest part of the entire process.</p>"},{"location":"concepts/workflow/#2d-post-process-output-data","title":"2d. Post-process Output Data","text":"<p>The raw output from the model (logits) is often not the final answer. It needs to be converted into a human-readable format. <code>xInfer</code> provides fused CUDA kernels for common post-processing bottlenecks (like NMS for object detection or ArgMax for segmentation) in the <code>xinfer::postproc</code> module.</p> <pre><code>#include &lt;opencv2/opencv.hpp&gt;\n\n// The .predict() call handles pre-processing, inference, and post-processing.\ncv::Mat image = cv::imread(\"my_image.jpg\");\nauto detections = detector.predict(image); // Returns a clean vector of bounding boxes\n</code></pre>"},{"location":"concepts/workflow/#summary-of-the-workflow","title":"Summary of the Workflow","text":"Stage What Happens Tools Used Output Build Step (Offline) A slow, one-time process of compiling and optimizing a trained model for a specific hardware target. <code>xTorch</code>/PyTorch, <code>xinfer-cli</code>, <code>xinfer::builders</code> A single <code>.engine</code> file Inference Step (Runtime) A blazing-fast, real-time process of loading the engine and running it on new data. <code>xinfer::zoo</code>, <code>xinfer::core</code>, <code>xinfer::preproc</code>, <code>xinfer::postproc</code> The final prediction <p>By separating these two concerns, <code>xInfer</code> allows you to pay the high cost of optimization once, during development, and then reap the benefits of extreme performance in your final, lightweight C++ application.</p>"},{"location":"core-api/","title":"Core Toolkit API Reference","text":"<p>Welcome to the API reference for the <code>xInfer</code> Core Toolkit.</p> <p>While the Model Zoo API provides high-level, pre-packaged solutions for common tasks, the Core Toolkit is for developers who need maximum control and flexibility. These are the powerful, low-level building blocks that the <code>zoo</code> itself is built upon.</p> <p>You should use the Core Toolkit when you are: - Building a hyper-optimized pipeline for a custom model architecture not found in the <code>zoo</code>. - Integrating <code>xInfer</code> into a complex, existing C++ application with custom data structures. - Implementing advanced asynchronous workflows with multiple CUDA streams. - Creating your own high-level <code>zoo</code>-like abstractions for a specific domain.</p>"},{"location":"core-api/#the-core-modules","title":"The Core Modules","text":"<p>The toolkit is divided into four logical modules, each responsible for a specific part of the high-performance inference pipeline.</p>"},{"location":"core-api/#1-xinfercore-the-inference-runtime","title":"1. <code>xinfer::core</code> - The Inference Runtime","text":"<p>This is the heart of the <code>xInfer</code> runtime. These classes are responsible for loading and executing your pre-built, optimized TensorRT engines.</p> <ul> <li><code>Tensor</code>: A lightweight, safe C++ wrapper for managing GPU memory. It's the primary data structure for all I/O in <code>xInfer</code>.</li> <li><code>InferenceEngine</code>: The workhorse class that loads a <code>.engine</code> file and provides simple, powerful methods for running synchronous and asynchronous inference.</li> </ul> <p>\u27a1\ufe0f Full API Reference for <code>core</code></p>"},{"location":"core-api/#2-xinferbuilders-the-optimization-toolkit","title":"2. <code>xinfer::builders</code> - The Optimization Toolkit","text":"<p>This module provides the \"factory\" tools for performing the crucial, offline \"Build Step.\" You use these classes to convert a standard model format like ONNX into a hyper-optimized TensorRT engine.</p> <ul> <li><code>EngineBuilder</code>: A fluent API that automates the entire TensorRT build process, including enabling optimizations like FP16 and INT8.</li> <li><code>ONNXExporter</code>: A convenience utility to bridge the gap from a trained <code>xTorch</code> model to the ONNX format.</li> <li><code>INT8Calibrator</code>: The interface for providing calibration data for INT8 quantization.</li> </ul> <p>\u27a1\ufe0f Full API Reference for <code>builders</code></p>"},{"location":"core-api/#3-xinferpreproc-gpu-accelerated-pre-processing","title":"3. <code>xinfer::preproc</code> - GPU-Accelerated Pre-processing","text":"<p>This module contains a library of unique, high-performance CUDA kernels designed to eliminate CPU bottlenecks during data preparation.</p> <ul> <li><code>ImageProcessor</code>: A powerful class that can perform an entire image pre-processing pipeline (<code>Resize -&gt; Pad -&gt; Normalize -&gt; HWC to CHW</code>) in a single, fused CUDA kernel.</li> <li><code>AudioProcessor</code>: A fused pipeline for converting raw audio waveforms into mel spectrograms, using <code>cuFFT</code> for maximum performance.</li> </ul> <p>\u27a1\ufe0f Full API Reference for <code>preproc</code></p>"},{"location":"core-api/#4-xinferpostproc-gpu-accelerated-post-processing","title":"4. <code>xinfer::postproc</code> - GPU-Accelerated Post-processing","text":"<p>This module provides custom CUDA kernels to accelerate the most common post-processing tasks, avoiding slow GPU-to-CPU data transfers of large, raw model outputs.</p> <ul> <li><code>detection::nms</code>: A hyper-performant, GPU-based implementation of Non-Maximum Suppression for object detection.</li> <li><code>yolo_decoder::decode</code>: A fused kernel for parsing the complex output of YOLO-family models.</li> <li><code>segmentation::argmax</code>: A GPU-based kernel for converting raw segmentation logits into a final class mask.</li> <li><code>ctc::decode</code>: A GPU-based kernel for decoding the output of speech recognition and OCR models.</li> </ul> <p>\u27a1\ufe0f Full API Reference for <code>postproc</code></p>"},{"location":"core-api/#next-steps","title":"Next Steps","text":"<p>To see how these core components are used in practice, check out the Building Custom Pipelines guide.</p>"},{"location":"core-api/builders/","title":"API Reference: Engine Builders","text":"<p>The <code>xinfer::builders</code> module is the heart of the <code>xInfer</code> optimization pipeline. It provides a high-level, fluent C++ API that wraps the immense complexity of the NVIDIA TensorRT build process.</p> <p>This is the toolkit you use to perform the crucial \"Build Step\" of the <code>xInfer</code> workflow, converting a standard model format like ONNX into a hyper-optimized, hardware-specific TensorRT engine.</p> <p>While the <code>xinfer-cli</code> tool provides a command-line interface to this module, the C++ API gives you the full power and flexibility to integrate engine building directly into your applications or build automation scripts.</p>"},{"location":"core-api/builders/#key-classes","title":"Key Classes","text":"<ul> <li><code>EngineBuilder</code>: The main class for configuring and running the optimization process.</li> <li><code>ONNXExporter</code>: A utility for converting trained <code>xTorch</code> models into the ONNX format.</li> <li><code>INT8Calibrator</code>: An interface for providing data for INT8 quantization.</li> </ul>"},{"location":"core-api/builders/#enginebuilder","title":"<code>EngineBuilder</code>","text":"<p>This is the primary class you will interact with. It uses a \"fluent\" design pattern, allowing you to chain configuration calls together in a clean, readable way.</p> <p>Header: <code>#include &lt;xinfer/builders/engine_builder.h&gt;</code></p>"},{"location":"core-api/builders/#core-workflow","title":"Core Workflow","text":"<p>The <code>EngineBuilder</code> follows a simple, three-step process: 1.  Specify the input model (<code>.from_onnx()</code>). 2.  Configure the desired optimizations (<code>.with_fp16()</code>, <code>.with_int8()</code>, etc.). 3.  Execute the build process (<code>.build_and_save()</code>).</p>"},{"location":"core-api/builders/#example-building-an-fp16-engine","title":"Example: Building an FP16 Engine","text":"<p>This is the most common use case. It takes an ONNX file and creates a TensorRT engine optimized for FP16 precision, which typically provides a 2x speedup on modern NVIDIA GPUs with Tensor Cores.</p> <pre><code>#include &lt;xinfer/builders/engine_builder.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    try {\n        std::string onnx_path = \"path/to/your/model.onnx\";\n        std::string engine_path = \"my_model_fp16.engine\";\n\n        // 1. Create the builder\n        xinfer::builders::EngineBuilder builder;\n\n        // 2. Configure the build process by chaining calls\n        builder.from_onnx(onnx_path)\n               .with_fp16()\n               .with_max_batch_size(16); // Specify the max batch size you'll use\n\n        // 3. Execute the build and save the engine\n        std::cout &lt;&lt; \"Building FP16 engine... this may take a few minutes.\\n\";\n        builder.build_and_save(engine_path);\n        std::cout &lt;&lt; \"Engine built successfully: \" &lt;&lt; engine_path &lt;&lt; std::endl;\n\n    } catch (const std::exception&amp; e) {\n        std::cerr &lt;&lt; \"Error building engine: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n        return 1;\n    }\n    return 0;\n}\n</code></pre>"},{"location":"core-api/builders/#api-overview","title":"API Overview","text":"<ul> <li> <p><code>EngineBuilder&amp; from_onnx(const std::string&amp; onnx_path)</code>   Specifies the path to the input <code>.onnx</code> file.</p> </li> <li> <p><code>EngineBuilder&amp; with_fp16()</code>   Enables FP16 precision mode. TensorRT will convert the model's layers to use half-precision floats where possible. This is highly recommended for all GPUs from the Turing architecture (sm_75) onwards.</p> </li> <li> <p><code>EngineBuilder&amp; with_int8(std::shared_ptr&lt;INT8Calibrator&gt; calibrator)</code>   Enables INT8 precision mode for maximum performance (~4x+ speedup). This requires providing a calibrator object. See the INT8 Quantization Guide for details.</p> </li> <li> <p><code>EngineBuilder&amp; with_max_batch_size(int batch_size)</code>   Specifies the maximum batch size that the optimized engine will support. This allows TensorRT to tune its kernels for a specific batch size, which can improve performance.</p> </li> <li> <p><code>void build_and_save(const std::string&amp; output_engine_path)</code>   Triggers the final build process and saves the resulting compiled engine to the specified path.</p> </li> </ul>"},{"location":"core-api/builders/#onnxexporter","title":"<code>ONNXExporter</code>","text":"<p>This is a convenience utility for developers using the <code>xTorch</code> training library. It provides a seamless bridge between a trained <code>xTorch</code> model and the ONNX format needed by the <code>EngineBuilder</code>.</p> <p>Header: <code>#include &lt;xinfer/builders/onnx_exporter.h&gt;</code></p>"},{"location":"core-api/builders/#example-exporting-an-xtorch-model","title":"Example: Exporting an <code>xTorch</code> Model","text":"<pre><code>#include &lt;xinfer/builders/onnx_exporter.h&gt;\n#include &lt;xtorch/models/resnet.h&gt; // Assuming you have an xTorch model\n#include &lt;xtorch/util/serialization.h&gt;\n\nint main() {\n    // 1. Instantiate and load your trained xTorch model\n    auto model = std::make_shared&lt;xt::models::ResNet18&gt;();\n    // xt::load(model, \"my_trained_resnet.weights\");\n    model-&gt;eval();\n\n    // 2. Define the input specification for your model\n    xinfer::builders::InputSpec input_spec;\n    input_spec.name = \"input\";\n    input_spec.shape = {1, 3, 224, 224}; // Batch, Channels, Height, Width\n\n    // 3. Call the export function\n    std::string onnx_path = \"resnet18.onnx\";\n    bool success = xinfer::builders::export_to_onnx(*model, {input_spec}, onnx_path);\n\n    if (success) {\n        std::cout &lt;&lt; \"xTorch model exported successfully to \" &lt;&lt; onnx_path &lt;&lt; std::endl;\n    }\n\n    return 0;\n}\n</code></pre>"},{"location":"core-api/builders/#build_engine_from_url-convenience-function","title":"<code>build_engine_from_url</code> (Convenience Function)","text":"<p>This high-level function combines downloading and building into a single step, perfect for use in the <code>xinfer-cli</code> or for quickly grabbing a model from an online repository.</p> <p>Header: <code>#include &lt;xinfer/builders/engine_builder.h&gt;</code></p>"},{"location":"core-api/builders/#example-downloading-and-building-from-the-web","title":"Example: Downloading and Building from the Web","text":"<pre><code>#include &lt;xinfer/builders/engine_builder.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    // 1. Define the configuration for the download-and-build process\n    xinfer::builders::BuildFromUrlConfig config;\n    config.onnx_url = \"https://github.com/onnx/models/raw/main/vision/classification/resnet/model/resnet50-v2-7.onnx\";\n    config.output_engine_path = \"resnet50_web.engine\";\n    config.use_fp16 = true;\n    config.max_batch_size = 1;\n\n    // 2. Call the function\n    std::cout &lt;&lt; \"Downloading and building ResNet-50...\\n\";\n    bool success = xinfer::builders::build_engine_from_url(config);\n\n    if (success) {\n        std::cout &lt;&lt; \"Engine created successfully at \" &lt;&lt; config.output_engine_path &lt;&lt; std::endl;\n    }\n\n    return 0;\n}\n</code></pre>"},{"location":"core-api/engine/","title":"API Reference: Core Runtime Engine","text":"<p>The <code>xinfer::core</code> module provides the fundamental classes for loading and executing pre-built, optimized TensorRT engines. These are the low-level, high-performance building blocks that the high-level <code>zoo</code> API is built upon.</p> <p>You would use this API directly if you need fine-grained control over the inference process, such as when building a custom pipeline that isn't covered by the <code>zoo</code>, or when you need to manage CUDA streams for complex asynchronous workflows.</p>"},{"location":"core-api/engine/#key-classes","title":"Key Classes","text":"<ul> <li><code>Tensor</code>: A lightweight, safe wrapper for managing GPU memory.</li> <li><code>InferenceEngine</code>: The main class for loading a <code>.engine</code> file and running inference.</li> </ul>"},{"location":"core-api/engine/#tensor","title":"<code>Tensor</code>","text":"<p>The <code>core::Tensor</code> is the primary data structure in <code>xInfer</code>. It is a C++ RAII-compliant object that safely manages the lifetime of a GPU memory buffer, abstracting away raw <code>cudaMalloc</code> and <code>cudaFree</code> calls.</p> <p>Header: <code>#include &lt;xinfer/core/tensor.h&gt;</code></p>"},{"location":"core-api/engine/#core-principles","title":"Core Principles","text":"<ul> <li>Ownership: A <code>Tensor</code> object owns its GPU memory. When it goes out of scope, its destructor is automatically called, freeing the GPU memory.</li> <li>Movable, Not Copyable: To prevent accidental, expensive GPU-to-GPU copies, <code>Tensor</code> objects cannot be copied. They can only be moved. This ensures clear and efficient ownership transfer.</li> <li>Lightweight: The class is a very thin wrapper, adding no performance overhead to the underlying GPU operations.</li> </ul>"},{"location":"core-api/engine/#example-creating-and-moving-a-tensor","title":"Example: Creating and Moving a Tensor","text":"<pre><code>#include &lt;xinfer/core/tensor.h&gt;\n#include &lt;vector&gt;\n\nvoid process_tensor(xinfer::core::Tensor t) {\n    // This function now owns the tensor memory.\n    // When `t` goes out of scope here, the memory is freed.\n}\n\nint main() {\n    // 1. Create a tensor for a batch of 16 RGB images of size 224x224.\n    xinfer::core::Tensor my_tensor({16, 3, 224, 224}, xinfer::core::DataType::kFLOAT);\n\n    // 2. The GPU memory is now allocated.\n    // You can get the raw pointer to pass to a CUDA kernel.\n    void* gpu_ptr = my_tensor.data();\n\n    // 3. To pass it to a function, you must move it.\n    process_tensor(std::move(my_tensor));\n\n    // 4. After the move, `my_tensor` is in a null state.\n    // Accessing its data would now be an error.\n    // assert(my_tensor.data() == nullptr);\n\n    return 0;\n}\n</code></pre>"},{"location":"core-api/engine/#api-overview","title":"API Overview","text":"<ul> <li> <p><code>Tensor(const std::vector&lt;int64_t&gt;&amp; shape, DataType dtype)</code>   Constructor to allocate a new GPU buffer of a specific shape and data type.</p> </li> <li> <p><code>void* data() const</code>   Returns the raw <code>void*</code> pointer to the GPU memory buffer.</p> </li> <li> <p><code>const std::vector&lt;int64_t&gt;&amp; shape() const</code>   Returns a reference to the vector describing the tensor's shape.</p> </li> <li> <p><code>void copy_from_host(const void* cpu_data)</code>   Performs a <code>cudaMemcpyHostToDevice</code> to upload data from the CPU to the GPU.</p> </li> <li> <p><code>void copy_to_host(void* cpu_data) const</code>   Performs a <code>cudaMemcpyDeviceToHost</code> to download data from the GPU to the CPU.</p> </li> </ul>"},{"location":"core-api/engine/#inferenceengine","title":"<code>InferenceEngine</code>","text":"<p>The <code>core::InferenceEngine</code> is the workhorse of the <code>xInfer</code> runtime. It loads a serialized <code>.engine</code> file created by the <code>builders</code> module and handles the execution of the model.</p> <p>Header: <code>#include &lt;xinfer/core/engine.h&gt;</code></p>"},{"location":"core-api/engine/#example-manual-inference-pipeline","title":"Example: Manual Inference Pipeline","text":"<p>This example shows how you would use the <code>InferenceEngine</code> directly to build a custom pipeline, without the <code>zoo</code>.</p> <pre><code>#include &lt;xinfer/core/engine.h&gt;\n#include &lt;xinfer/core/tensor.h&gt;\n#include &lt;xinfer/preproc/image_processor.h&gt; // Using a preproc helper\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    try {\n        std::string engine_path = \"resnet18.engine\";\n\n        // 1. Load the pre-built engine. This is a fast operation.\n        xinfer::core::InferenceEngine engine(engine_path);\n\n        // 2. Manually create the pre-processor.\n        xinfer::preproc::ImageProcessor preprocessor(224, 224, {0.485, 0.456, 0.406}, {0.229, 0.224, 0.225});\n\n        // 3. Load input data and prepare GPU tensors.\n        cv::Mat image = cv::imread(\"cat_image.jpg\");\n        auto input_shape = engine.get_input_shape(0);\n        xinfer::core::Tensor input_tensor(input_shape, xinfer::core::DataType::kFLOAT);\n\n        // 4. Run the pre-processing step.\n        preprocessor.process(image, input_tensor);\n\n        // 5. Run synchronous inference.\n        // The engine takes a vector of input tensors and returns a vector of output tensors.\n        std::vector&lt;xinfer::core::Tensor&gt; output_tensors = engine.infer({input_tensor});\n\n        // 6. Process the output.\n        // For ResNet-18, there is one output tensor with shape.\n        std::cout &lt;&lt; \"Inference successful. Output tensor has \" &lt;&lt; output_tensors.num_elements() &lt;&lt; \" elements.\\n\";\n\n        // You would now copy the output_tensors to the host and find the argmax.\n\n    } catch (const std::exception&amp; e) {\n        std::cerr &lt;&lt; \"Error in custom pipeline: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n    }\n\n    return 0;\n}\n</code></pre>"},{"location":"core-api/engine/#api-overview_1","title":"API Overview","text":"<ul> <li> <p><code>InferenceEngine(const std::string&amp; engine_path)</code>   Constructor that loads and deserializes a TensorRT engine from a file path.</p> </li> <li> <p><code>std::vector&lt;Tensor&gt; infer(const std::vector&lt;Tensor&gt;&amp; inputs)</code>   Performs a synchronous inference call. It takes a vector of input <code>Tensor</code> objects and returns a vector of newly allocated output <code>Tensor</code> objects. This is the simplest way to run inference.</p> </li> <li> <p><code>void infer_async(const std::vector&lt;Tensor&gt;&amp; inputs, std::vector&lt;Tensor&gt;&amp; outputs, cudaStream_t stream)</code>   Performs an asynchronous inference call on a specific CUDA stream. This is an advanced method for building complex pipelines where you want to overlap data transfers with computation. Note that the output tensors must be pre-allocated by the user.</p> </li> <li> <p><code>int get_num_inputs() const</code>   Returns the number of input tensors the model expects.</p> </li> <li> <p><code>std::vector&lt;int64_t&gt; get_input_shape(int index = 0) const</code>   Returns the shape of the specified input tensor. This is useful for allocating your input <code>Tensor</code> objects correctly.</p> </li> </ul>"},{"location":"core-api/postproc/","title":"API Reference: Post-processing Kernels","text":"<p>The <code>xinfer::postproc</code> module is one of the most powerful components of the <code>xInfer</code> toolkit. It provides a library of hyper-optimized, standalone CUDA kernels and functions for common post-processing tasks.</p> <p>The Philosophy: The raw output of a neural network (the logits) can be enormous. Downloading this massive tensor from the GPU to the CPU just to perform filtering (like NMS) or decoding is incredibly inefficient. The <code>postproc</code> module provides functions that perform these operations directly on the GPU, ensuring that only the final, small, human-readable result is ever transferred back to the CPU.</p> <p>This is a key strategy for eliminating performance bottlenecks in end-to-end inference pipelines.</p>"},{"location":"core-api/postproc/#detection-postprocdetection","title":"Detection: <code>postproc::detection</code>","text":"<p>Header: <code>#include &lt;xinfer/postproc/detection.h&gt;</code></p> <p>This module is essential for any object detection task.</p>"},{"location":"core-api/postproc/#stdvectorint-nms","title":"<code>std::vector&lt;int&gt; nms(...)</code>","text":"<p>Performs high-performance Non-Maximum Suppression on the GPU.</p> <pre><code>#include &lt;xinfer/postproc/detection.h&gt;\n\nstd::vector&lt;int&gt; nms(\n    const core::Tensor&amp; decoded_boxes,    // GPU tensor of shape [N, 4] with [x1, y1, x2, y2]\n    const core::Tensor&amp; decoded_scores,   // GPU tensor of shape [N] with confidence scores\n    float nms_iou_threshold             // The IoU threshold for suppression\n);\n</code></pre> <ul> <li>Description: This is a custom CUDA implementation of NMS. It takes the decoded boxes and scores (which are still on the GPU), sorts them, and efficiently suppresses overlapping boxes.</li> <li>Returns: A <code>std::vector&lt;int&gt;</code> containing the indices of the boxes that survived the suppression. This list is typically very small, making the final data transfer to the CPU minimal.</li> <li>Why it's an \"F1 Car\": A standard NMS implementation on the CPU would require downloading all <code>N</code> boxes and scores first. The <code>xInfer</code> version is 10x-20x faster by keeping the entire process on the GPU.</li> </ul>"},{"location":"core-api/postproc/#detection-decoders-postprocyolo","title":"Detection Decoders: <code>postproc::yolo</code>","text":"<p>Header: <code>#include &lt;xinfer/postproc/yolo_decoder.h&gt;</code></p> <p>This module provides decoders for specific model families.</p>"},{"location":"core-api/postproc/#void-decode","title":"<code>void decode(...)</code>","text":"<p>Parses the complex, raw output tensor of a YOLO-family model.</p> <pre><code>#include &lt;xinfer/postproc/yolo_decoder.h&gt;\n\nvoid decode(\n    const core::Tensor&amp; raw_output,         // Raw GPU tensor from the model\n    float confidence_threshold,             // Filter out low-confidence boxes on the GPU\n    core::Tensor&amp; out_boxes,                // Pre-allocated GPU tensor for boxes\n    core::Tensor&amp; out_scores,               // Pre-allocated GPU tensor for scores\n    core::Tensor&amp; out_classes               // Pre-allocated GPU tensor for class IDs\n);\n</code></pre> <ul> <li>Description: A YOLO model typically outputs a single large tensor of shape <code>[1, 4 + NumClasses, NumPriors]</code>. This fused CUDA kernel efficiently processes this tensor in parallel. For each of the thousands of priors, it finds the class with the highest score, checks it against the confidence threshold, converts the box coordinates, and writes the filtered results to the output tensors.</li> <li>Why it's an \"F1 Car\": It avoids a massive GPU-to-CPU transfer of the raw logits. All filtering and decoding happens on the GPU, which is massively parallel and perfectly suited for this task.</li> </ul>"},{"location":"core-api/postproc/#segmentation-postprocsegmentation","title":"Segmentation: <code>postproc::segmentation</code>","text":"<p>Header: <code>#include &lt;xinfer/postproc/segmentation.h&gt;</code></p> <p>Provides essential tools for semantic and instance segmentation.</p>"},{"location":"core-api/postproc/#void-argmax","title":"<code>void argmax(...)</code>","text":"<p>Performs a fast, parallel ArgMax operation across the channel dimension of a segmentation model's output.</p> <pre><code>#include &lt;xinfer/postproc/segmentation.h&gt;\n\nvoid argmax(\n    const core::Tensor&amp; logits,       // GPU tensor of shape [1, NumClasses, H, W]\n    core::Tensor&amp; output_mask         // Pre-allocated GPU tensor of shape [1, H, W] (INT32)\n);\n</code></pre> <ul> <li>Description: This CUDA kernel finds the index of the maximum value for each pixel, converting the probability map (logits) into a final class ID map.</li> <li>Why it's an \"F1 Car\": The <code>logits</code> tensor for a high-resolution image can be huge (e.g., 21 classes * 1024 * 1024 pixels &gt; 88MB). Downloading this to the CPU is very slow. This kernel produces a much smaller integer mask (<code>~4MB</code>) directly on the GPU.</li> </ul>"},{"location":"core-api/postproc/#cvmat-argmax_to_mat","title":"<code>cv::Mat argmax_to_mat(...)</code>","text":"<p>A convenience function that wraps <code>argmax</code> and handles the download to an OpenCV Mat.</p> <pre><code>#include &lt;xinfer/postproc/segmentation.h&gt;\n\ncv::Mat argmax_to_mat(const core::Tensor&amp; logits);\n</code></pre>"},{"location":"core-api/postproc/#sequence-decoding-postprocctc","title":"Sequence Decoding: <code>postproc::ctc</code>","text":"<p>Header: <code>#include &lt;xinfer/postproc/ctc_decoder.h&gt;</code></p> <p>Provides decoders for sequence models like speech recognition and OCR.</p>"},{"location":"core-api/postproc/#stdpairstdstring-float-decode","title":"<code>std::pair&lt;std::string, float&gt; decode(...)</code>","text":"<p>Performs a greedy Connectionist Temporal Classification (CTC) decode on the GPU.</p> <pre><code>#include &lt;xinfer/postproc/ctc_decoder.h&gt;\n\nstd::pair&lt;std::string, float&gt; decode(\n    const core::Tensor&amp; logits,                 // GPU tensor of shape [1, Timesteps, NumClasses]\n    const std::vector&lt;std::string&gt;&amp; character_map // The vocabulary mapping\n);\n</code></pre> <ul> <li>Description: This function launches a CUDA kernel to find the most likely character (the <code>argmax</code>) at each timestep in the sequence. It then performs the final CTC collapsing logic (removing blanks and duplicates) on the CPU to produce the final string.</li> <li>Why it's an \"F1 Car\": It performs the most parallelizable part of the decoding (the per-timestep <code>argmax</code>) on the GPU, avoiding the download of the large <code>logits</code> tensor.</li> </ul>"},{"location":"core-api/postproc/#generative-ai-postprocdiffusion","title":"Generative AI: <code>postproc::diffusion</code>","text":"<p>Header: <code>#include &lt;xinfer/postproc/diffusion_sampler.h&gt;</code></p> <p>Provides the core building block for diffusion model pipelines.</p>"},{"location":"core-api/postproc/#void-sampling_step","title":"<code>void sampling_step(...)</code>","text":"<p>Executes one full step of the DDPM denoising process in a single fused kernel.</p> <pre><code>#include &lt;xinfer/postproc/diffusion_sampler.h&gt;\n\nvoid sampling_step(\n    core::Tensor&amp; img,                  // The noisy image (modified in-place)\n    const core::Tensor&amp; predicted_noise,  // The U-Net output\n    const core::Tensor&amp; random_noise,     // Random noise for this step\n    const core::Tensor&amp; alphas,           // Scheduler constants\n    const core::Tensor&amp; alphas_cumprod,   // Scheduler constants\n    const core::Tensor&amp; betas,            // Scheduler constants\n    int timestep,\n    cudaStream_t stream\n);\n</code></pre> <ul> <li>Description: This custom CUDA kernel implements the entire, multi-part DDPM sampling equation.</li> <li>Why it's an \"F1 Car\": A standard framework would execute this equation as a sequence of 5-6 separate math operations, each launching its own CUDA kernel. This is extremely inefficient inside the hot loop of the diffusion process. The <code>xInfer</code> fused kernel executes the entire equation in a single launch, dramatically reducing overhead.</li> </ul>"},{"location":"core-api/preproc/","title":"API Reference: Pre-processing Kernels","text":"<p>The <code>xinfer::preproc</code> module is dedicated to solving one of the most common bottlenecks in real-world AI applications: data pre-processing.</p> <p>The Philosophy: Before a model can be run, input data (like an image from a camera or an audio clip) must be transformed into a correctly formatted tensor. Performing these steps on the CPU\u2014resizing, normalizing, converting data types, and changing memory layouts\u2014and then transferring the result to the GPU is incredibly inefficient.</p> <p>The <code>preproc</code> module provides high-performance, fused CUDA kernels that perform this entire pipeline directly on the GPU. This minimizes CPU-GPU data transfers and leverages the GPU's massive parallelism for a significant speedup.</p>"},{"location":"core-api/preproc/#image-processing-preprocimageprocessor","title":"Image Processing: <code>preproc::ImageProcessor</code>","text":"<p>Header: <code>#include &lt;xinfer/preproc/image_processor.h&gt;</code></p> <p>This is the universal tool for all image-based tasks in <code>xInfer</code>. It is designed to take a standard <code>cv::Mat</code> from the CPU and efficiently produce a model-ready tensor on the GPU.</p>"},{"location":"core-api/preproc/#core-feature-the-fused-pipeline","title":"Core Feature: The Fused Pipeline","text":"<p>A single call to <code>process()</code> executes a monolithic CUDA kernel that performs the following steps in one operation: 1.  Upload: Copies the <code>cv::Mat</code> data from the CPU to the GPU. 2.  Resize: Resizes the image to the model's required input dimensions using bilinear interpolation. 3.  (Optional) Letterbox/Pad: Adds padding to maintain the aspect ratio, a common requirement for models like YOLO. 4.  Layout Conversion: Converts the image from OpenCV's interleaved <code>HWC</code> (Height, Width, Channel) layout to the <code>CHW</code> (Channel, Height, Width) layout required by deep learning models. 5.  Normalization: Converts the 8-bit integer pixel values to 32-bit floats and applies the specified normalization formula: <code>(pixel / 255.0 - mean) / std</code>. 6.  Output: Writes the final, model-ready tensor directly to the destination GPU buffer.</p> <p>This fused approach is 5x-10x faster than a traditional pipeline using a chain of OpenCV calls on the CPU.</p>"},{"location":"core-api/preproc/#example-preparing-an-image-for-a-classifier","title":"Example: Preparing an Image for a Classifier","text":"<pre><code>#include &lt;xinfer/preproc/image_processor.h&gt;\n#include &lt;xinfer/core/tensor.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;vector&gt;\n\nint main() {\n    // 1. Configure the pre-processor for a standard ImageNet model.\n    int input_width = 224;\n    int input_height = 224;\n    std::vector&lt;float&gt; mean = {0.485, 0.456, 0.406};\n    std::vector&lt;float&gt; std = {0.229, 0.224, 0.225};\n\n    xinfer::preproc::ImageProcessor preprocessor(input_width, input_height, mean, std);\n\n    // 2. Load an image from disk.\n    cv::Mat image = cv::imread(\"my_image.jpg\");\n\n    // 3. Create a destination tensor on the GPU.\n    xinfer::core::Tensor input_tensor({1, 3, input_height, input_width}, xinfer::core::DataType::kFLOAT);\n\n    // 4. Run the entire pre-processing pipeline.\n    preprocessor.process(image, input_tensor);\n\n    // `input_tensor` is now ready to be passed to an InferenceEngine.\n\n    return 0;\n}\n</code></pre>"},{"location":"core-api/preproc/#api-overview","title":"API Overview","text":"<ul> <li> <p><code>ImageProcessor(int width, int height, const std::vector&lt;float&gt;&amp; mean, const std::vector&lt;float&gt;&amp; std)</code>   Constructor for a standard resizing and normalization pipeline.</p> </li> <li> <p><code>ImageProcessor(int width, int height, bool letterbox = false)</code>   A convenience constructor for models like YOLO that require letterbox padding and simple <code>[0, 1]</code> scaling instead of mean/std normalization.</p> </li> <li> <p><code>void process(const cv::Mat&amp; cpu_image, core::Tensor&amp; output_tensor)</code>   Executes the full pipeline on a CPU-based <code>cv::Mat</code>.</p> </li> <li> <p><code>void process(const core::Tensor&amp; gpu_image, core::Tensor&amp; output_tensor)</code>   An advanced overload that processes an image that is already on the GPU, avoiding the CPU-GPU upload step entirely.</p> </li> </ul>"},{"location":"core-api/preproc/#audio-processing-preprocaudioprocessor","title":"Audio Processing: <code>preproc::AudioProcessor</code>","text":"<p>Header: <code>#include &lt;xinfer/preproc/audio_processor.h&gt;</code></p> <p>This class is the universal entry point for all audio-based tasks, such as speech recognition and audio classification. It efficiently converts a raw audio waveform into a mel spectrogram.</p>"},{"location":"core-api/preproc/#core-feature-the-fused-spectrogram-pipeline","title":"Core Feature: The Fused Spectrogram Pipeline","text":"<p>A single call to <code>process()</code> uses a chain of custom CUDA kernels and the NVIDIA <code>cuFFT</code> library to perform the entire spectrogram generation pipeline on the GPU:</p> <ol> <li>Upload: Copies the raw audio waveform from the CPU to the GPU.</li> <li>Framing &amp; Windowing: A CUDA kernel splits the audio into overlapping frames and applies a windowing function (e.g., Hann).</li> <li>FFT: The NVIDIA <code>cuFFT</code> library, the fastest available FFT implementation, is used to compute the Short-Time Fourier Transform (STFT).</li> <li>Power &amp; Mel Scaling: A final fused kernel calculates the power spectrum, applies a Mel filterbank to the frequencies, and converts the result to a log scale.</li> </ol>"},{"location":"core-api/preproc/#example-creating-a-mel-spectrogram","title":"Example: Creating a Mel Spectrogram","text":"<pre><code>#include &lt;xinfer/preproc/audio_processor.h&gt;\n#include &lt;xinfer/core/tensor.h&gt;\n#include &lt;vector&gt;\n\nint main() {\n    // 1. Configure the audio processor.\n    xinfer::preproc::AudioProcessorConfig config;\n    config.sample_rate = 16000;\n    config.n_fft = 400;\n    config.hop_length = 160;\n    config.n_mels = 80;\n\n    xinfer::preproc::AudioProcessor preprocessor(config);\n\n    // 2. Load a raw audio waveform (e.g., from a .wav file).\n    std::vector&lt;float&gt; waveform; // Assume this is loaded with audio data\n\n    // 3. Create a destination tensor on the GPU.\n    // The output shape depends on the waveform length and config.\n    int n_frames = (waveform.size() - config.n_fft) / config.hop_length + 1;\n    xinfer::core::Tensor mel_spectrogram({1, config.n_mels, n_frames}, xinfer::core::DataType::kFLOAT);\n\n    // 4. Run the entire spectrogram generation pipeline.\n    preprocessor.process(waveform, mel_spectrogram);\n\n    // `mel_spectrogram` is now ready to be passed to a model like Whisper.\n\n    return 0;\n}\n</code></pre>"},{"location":"guides/building-engines/","title":"How-To Guide: Building TensorRT Engines","text":"<p>The core of the <code>xInfer</code> workflow is the TensorRT engine. An engine is a hyper-optimized version of your neural network, compiled and tuned for a specific GPU architecture and precision. This ahead-of-time compilation is what gives <code>xInfer</code> its incredible speed.</p> <p>This guide will walk you through the two primary methods for building an engine using the <code>xInfer</code> toolkit:</p> <ol> <li>The Easy Way: Using the <code>xinfer-cli</code> command-line tool.</li> <li>The Powerful Way: Using the <code>xinfer::builders::EngineBuilder</code> C++ API.</li> </ol> <p>The Goal: To convert a standard <code>.onnx</code> model file into a high-performance <code>.engine</code> file that can be loaded by the <code>xInfer</code> runtime.</p>"},{"location":"guides/building-engines/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, you need a trained model saved in the ONNX (Open Neural Network Exchange) format. ONNX is a universal format that <code>xInfer</code>'s builder uses as its starting point.</p>"},{"location":"guides/building-engines/#exporting-from-pytorchxtorch","title":"Exporting from PyTorch/xTorch","text":"<p>If you have a model trained in PyTorch or <code>xTorch</code>, you can export it to ONNX easily.</p> <p>Python (<code>PyTorch</code>): <pre><code>import torch\n\n# Load your trained model\nmodel = YourModelClass()\nmodel.load_state_dict(torch.load(\"my_model.pth\"))\nmodel.eval()\n\n# Create a dummy input with the correct shape and batch size\ndummy_input = torch.randn(1, 3, 224, 224)\n\n# Export the model\ntorch.onnx.export(\n    model,\n    dummy_input,\n    \"my_model.onnx\",\n    input_names=[\"input\"],\n    output_names=[\"output\"],\n    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\n)\n</code></pre></p> <p>C++ (<code>xTorch</code>): You can use the <code>xinfer::builders::export_to_onnx</code> utility for this. See the Core API: Builders reference for an example.</p>"},{"location":"guides/building-engines/#method-1-the-xinfer-cli-recommended","title":"Method 1: The <code>xinfer-cli</code> (Recommended)","text":"<p>The command-line interface is the simplest and most direct way to build your engine. It's a powerful tool for quick experiments and for integrating into build scripts.</p> <p>Assume you have <code>my_model.onnx</code> ready.</p>"},{"location":"guides/building-engines/#basic-fp32-engine","title":"Basic FP32 Engine","text":"<p>This creates a standard, full-precision engine. It's a good starting point for verifying correctness.</p> <pre><code># General Syntax:\n# xinfer-cli --build --onnx &lt;input.onnx&gt; --save_engine &lt;output.engine&gt;\n\nxinfer-cli --build --onnx my_model.onnx --save_engine my_model_fp32.engine\n</code></pre>"},{"location":"guides/building-engines/#fp16-engine-high-performance","title":"FP16 Engine (High Performance)","text":"<p>This is the most common and highly recommended optimization. It enables FP16 precision, which can provide a ~2x speedup on modern GPUs (Turing architecture and newer) by leveraging their Tensor Cores.</p> <pre><code>xinfer-cli --build \\\n    --onnx my_model.onnx \\\n    --save_engine my_model_fp16.engine \\\n    --fp16\n</code></pre>"},{"location":"guides/building-engines/#int8-engine-maximum-performance","title":"INT8 Engine (Maximum Performance)","text":"<p>This provides the highest possible performance (~4x+ speedup) but requires a \"calibration\" step. You must provide a small, representative dataset of images for TensorRT to analyze the model's activation distributions.</p> <p>For more details, see the INT8 Quantization guide.</p> <pre><code># This is a conceptual example. The CLI would need a way to specify a calibrator.\nxinfer-cli --build \\\n    --onnx my_model.onnx \\\n    --save_engine my_model_int8.engine \\\n    --int8 \\\n    --calibration_data /path/to/calibration/images\n</code></pre>"},{"location":"guides/building-engines/#specifying-batch-size","title":"Specifying Batch Size","text":"<p>You can also specify the maximum batch size the engine should be optimized for.</p> <pre><code>xinfer-cli --build \\\n    --onnx my_model.onnx \\\n    --save_engine my_model_fp16_b16.engine \\\n    --fp16 \\\n    --batch 16\n</code></pre>"},{"location":"guides/building-engines/#method-2-the-c-enginebuilder-api","title":"Method 2: The C++ <code>EngineBuilder</code> API","text":"<p>For more advanced use cases, such as building engines programmatically as part of a larger application, you can use the <code>EngineBuilder</code> C++ class directly. This gives you the full power and flexibility of the toolkit.</p>"},{"location":"guides/building-engines/#example-c-build-script","title":"Example C++ Build Script","text":"<p>This C++ program does the exact same thing as the <code>xinfer-cli</code> FP16 example above.</p> <p>File: <code>build_my_engine.cpp</code> <pre><code>#include &lt;xinfer/builders/engine_builder.h&gt;\n#include &lt;iostream&gt;\n#include &lt;stdexcept&gt;\n\nint main() {\n    std::string onnx_path = \"my_model.onnx\";\n    std::string engine_path = \"my_model_fp16_cpp.engine\";\n\n    std::cout &lt;&lt; \"Building FP16 engine from: \" &lt;&lt; onnx_path &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"This may take a few minutes...\\n\";\n\n    try {\n        // 1. Create the builder\n        xinfer::builders::EngineBuilder builder;\n\n        // 2. Configure the build process using the fluent API\n        builder.from_onnx(onnx_path)\n               .with_fp16()\n               .with_max_batch_size(16);\n\n        // 3. Execute the build and save the final engine\n        builder.build_and_save(engine_path);\n\n        std::cout &lt;&lt; \"Engine built successfully and saved to: \" &lt;&lt; engine_path &lt;&lt; std::endl;\n\n    } catch (const std::exception&amp; e) {\n        std::cerr &lt;&lt; \"Error during engine build: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n        return 1;\n    }\n\n    return 0;\n}\n</code></pre></p> <p>To compile and run this, you would link it against the <code>xinfer</code> library, just like the <code>xinfer_example</code> target in the main <code>CMakeLists.txt</code>.</p>"},{"location":"guides/building-engines/#next-steps","title":"Next Steps","text":"<p>Once you have successfully built your <code>.engine</code> file, you are ready for the fun part: running high-speed inference!</p> <ul> <li>See the \ud83d\ude80 Quickstart for a guide on how to load your new engine with the <code>zoo</code> API.</li> <li>See the Core API: Engine documentation to learn how to load and run it manually for custom pipelines.</li> </ul>"},{"location":"guides/custom-pipelines/","title":"How-To Guide: Building Custom Pipelines","text":"<p>The <code>xInfer::zoo</code> provides incredible, one-line solutions for common AI tasks. But what if your task is unique? What if you have a custom model with a non-standard input or a complex, multi-stage post-processing logic?</p> <p>For these scenarios, <code>xInfer</code> provides the low-level Core Toolkit. This guide will show you how to use these powerful building blocks\u2014<code>core</code>, <code>preproc</code>, and <code>postproc</code>\u2014to build a completely custom, high-performance inference pipeline from scratch.</p> <p>This is the \"power user\" path. It gives you maximum control and flexibility.</p>"},{"location":"guides/custom-pipelines/#the-goal-a-custom-multi-model-pipeline","title":"The Goal: A Custom Multi-Model Pipeline","text":"<p>Let's imagine a real-world robotics task that isn't in the <code>zoo</code>: \"Find the largest object in a scene and classify it.\"</p> <p>This requires a two-stage pipeline: 1.  Run an Object Detector: To find all objects and their bounding boxes. 2.  Run an Image Classifier: To classify the content of the largest bounding box found.</p> <p>We will build this pipeline step-by-step using the <code>xInfer</code> Core Toolkit.</p> <p>Prerequisites: - You have already used <code>xinfer-cli</code> to build two separate engine files:   - <code>yolov8n.engine</code>: An object detection model.   - <code>resnet18.engine</code>: An image classification model.</p>"},{"location":"guides/custom-pipelines/#step-1-initialize-the-engines-and-processors","title":"Step 1: Initialize the Engines and Processors","text":"<p>First, we load our pre-built engines and set up the necessary pre-processing modules.</p> <pre><code>#include &lt;xinfer/core/engine.h&gt;\n#include &lt;xinfer/preproc/image_processor.h&gt;\n#include &lt;xinfer/postproc/detection.h&gt;\n#include &lt;xinfer/postproc/yolo_decoder.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;algorithm&gt; // For std::max_element\n\nint main() {\n    try {\n        // --- 1. Load all necessary engines ---\n        std::cout &lt;&lt; \"Loading engines...\\n\";\n        xinfer::core::InferenceEngine detector_engine(\"yolov8n.engine\");\n        xinfer::core::InferenceEngine classifier_engine(\"resnet18.engine\");\n\n        // --- 2. Set up pre-processors for each model ---\n        // The detector uses a 640x640 input with letterboxing\n        xinfer::preproc::ImageProcessor detector_preprocessor(640, 640, true);\n        // The classifier uses a 224x224 input with standard ImageNet normalization\n        xinfer::preproc::ImageProcessor classifier_preprocessor(224, 224, {0.485, 0.456, 0.406}, {0.229, 0.224, 0.225});\n\n        // --- 3. Load input image ---\n        cv::Mat image = cv::imread(\"my_scene.jpg\");\n        if (image.empty()) {\n            throw std::runtime_error(\"Failed to load image.\");\n        }\n</code></pre>"},{"location":"guides/custom-pipelines/#step-2-run-the-first-stage-detection","title":"Step 2: Run the First Stage (Detection)","text":"<p>Now, we execute the first part of our custom pipeline: find all objects.</p> <pre><code>        // --- 4. Run the detection pipeline ---\n        std::cout &lt;&lt; \"Running detection stage...\\n\";\n\n        // 4a. Prepare the input tensor for the detector\n        auto det_input_shape = detector_engine.get_input_shape(0);\n        xinfer::core::Tensor det_input_tensor(det_input_shape, xinfer::core::DataType::kFLOAT);\n        detector_preprocessor.process(image, det_input_tensor);\n\n        // 4b. Run inference\n        auto det_output_tensors = detector_engine.infer({det_input_tensor});\n\n        // 4c. Run the custom post-processing kernels\n        // We use intermediate GPU tensors to avoid slow CPU round-trips\n        const int MAX_BOXES = 1024;\n        xinfer::core::Tensor decoded_boxes({MAX_BOXES, 4}, xinfer::core::DataType::kFLOAT);\n        xinfer::core::Tensor decoded_scores({MAX_BOXES}, xinfer::core::DataType::kFLOAT);\n        xinfer::core::Tensor decoded_classes({MAX_BOXES}, xinfer::core::DataType::kINT32);\n\n        xinfer::postproc::yolo::decode(det_output_tensors, 0.5f, decoded_boxes, decoded_scores, decoded_classes);\n        std::vector&lt;int&gt; nms_indices = xinfer::postproc::detection::nms(decoded_boxes, decoded_scores, 0.5f);\n\n        if (nms_indices.empty()) {\n            std::cout &lt;&lt; \"No objects found.\\n\";\n            return 0;\n        }\n</code></pre>"},{"location":"guides/custom-pipelines/#step-3-connect-the-pipelines-custom-logic","title":"Step 3: Connect the Pipelines (Custom Logic)","text":"<p>This is where the power of the Core API shines. We can now implement our custom logic: find the largest box and prepare it for the next stage.</p> <pre><code>        // --- 5. Custom Logic: Find the largest detected object ---\n        std::cout &lt;&lt; \"Finding largest object...\\n\";\n\n        // Download only the necessary box data to the CPU\n        std::vector&lt;float&gt; h_boxes(decoded_boxes.num_elements());\n        decoded_boxes.copy_to_host(h_boxes.data());\n\n        float max_area = 0.0f;\n        cv::Rect largest_box_roi;\n\n        for (int idx : nms_indices) {\n            float x1 = h_boxes[idx * 4 + 0];\n            float y1 = h_boxes[idx * 4 + 1];\n            float x2 = h_boxes[idx * 4 + 2];\n            float y2 = h_boxes[idx * 4 + 3];\n            float area = (x2 - x1) * (y2 - y1);\n\n            if (area &gt; max_area) {\n                max_area = area;\n                // Scale coordinates back to original image size\n                float scale_x = (float)image.cols / 640.0f;\n                float scale_y = (float)image.rows / 640.0f;\n                largest_box_roi = cv::Rect(x1 * scale_x, y1 * scale_y, (x2-x1) * scale_x, (y2-y1) * scale_y);\n            }\n        }\n</code></pre>"},{"location":"guides/custom-pipelines/#step-4-run-the-second-stage-classification","title":"Step 4: Run the Second Stage (Classification)","text":"<p>Finally, we run the classifier on the cropped image of the largest object.</p> <pre><code>        // --- 6. Run the classification pipeline on the cropped image ---\n        std::cout &lt;&lt; \"Running classification stage on the largest object...\\n\";\n\n        cv::Mat largest_object_patch = image(largest_box_roi);\n\n        // 6a. Prepare the input tensor for the classifier\n        auto cls_input_shape = classifier_engine.get_input_shape(0);\n        xinfer::core::Tensor cls_input_tensor(cls_input_shape, xinfer::core::DataType::kFLOAT);\n        classifier_preprocessor.process(largest_object_patch, cls_input_tensor);\n\n        // 6b. Run inference\n        auto cls_output_tensors = classifier_engine.infer({cls_input_tensor});\n\n        // 6c. Post-process the result on the CPU\n        std::vector&lt;float&gt; logits(cls_output_tensors.num_elements());\n        cls_output_tensors.copy_to_host(logits.data());\n        auto max_it = std::max_element(logits.begin(), logits.end());\n        int class_id = std::distance(logits.begin(), max_it);\n\n        std::cout &lt;&lt; \"\\n--- Custom Pipeline Result ---\\n\";\n        std::cout &lt;&lt; \"The largest object was classified as: Class \" &lt;&lt; class_id &lt;&lt; std::endl;\n\n    } catch (const std::exception&amp; e) {\n        std::cerr &lt;&lt; \"An error occurred in the custom pipeline: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n        return 1;\n    }\n    return 0;\n}\n</code></pre>"},{"location":"guides/custom-pipelines/#conclusion","title":"Conclusion","text":"<p>As you can see, the Core Toolkit provides the ultimate level of control. By composing the low-level <code>InferenceEngine</code>, <code>ImageProcessor</code>, and <code>postproc</code> functions, you can build sophisticated, multi-stage pipelines that are tailored to your exact needs, all while maintaining the hyper-performance of the underlying CUDA and TensorRT components.</p> <p>This is the power of <code>xInfer</code>: it provides a simple, elegant \"easy button\" with the <code>zoo</code>, but it never takes away the expert's ability to open the hood and build their own F1 car from scratch.</p>"},{"location":"guides/int8-quantization/","title":"How-To Guide: INT8 Quantization for Maximum Performance","text":"<p>This guide is for advanced users who want to squeeze every last drop of performance out of their NVIDIA GPU. INT8 quantization is a powerful technique that can provide a 2x or greater speedup on top of an already-optimized FP16 model, resulting in the absolute fastest inference speeds possible.</p> <p><code>xInfer</code> provides a streamlined API for performing INT8 quantization, but it's important to understand the concepts first.</p> <p>What You'll Learn: 1.  What INT8 quantization is and why it's so fast. 2.  The concept of \"calibration\" and why it's necessary. 3.  How to use the <code>xinfer::builders</code> API to create a high-performance INT8 engine.</p>"},{"location":"guides/int8-quantization/#what-is-int8-quantization","title":"What is INT8 Quantization?","text":"<p>By default, neural networks perform their calculations using 32-bit floating-point numbers (<code>FP32</code>). INT8 quantization is a process that converts the model's weights and activations to use 8-bit integers (<code>INT8</code>) instead.</p> <p>Why is this so much faster?</p> <ol> <li>Reduced Memory Bandwidth: An INT8 value takes up 4 times less memory than an FP32 value. This means the GPU can move data from its slow VRAM to its fast compute cores much more quickly. For many models, memory bandwidth is the primary bottleneck, so this provides a huge speedup.</li> <li>Specialized Hardware (Tensor Cores): Modern NVIDIA GPUs (Turing architecture and newer) have dedicated hardware cores called Tensor Cores. These cores are specifically designed to perform integer matrix math at an incredible rate. Running a model in INT8 mode allows TensorRT to take full advantage of this specialized hardware, dramatically increasing the number of operations the GPU can perform per second (TOPS).</li> </ol> <p>The Catch: The Loss of Precision Converting from a 32-bit float to an 8-bit integer is a \"lossy\" conversion. The challenge is to do this conversion in a smart way that minimizes the impact on the model's final accuracy. This is where calibration comes in.</p>"},{"location":"guides/int8-quantization/#the-calibration-process","title":"The Calibration Process","text":"<p>TensorRT uses a process called Post-Training Quantization (PTQ). To figure out the best way to convert the floating-point numbers to integers, it needs to look at the typical range of values that flow through the network.</p> <p>This is what the calibrator does. You provide a small, representative sample of your validation dataset (usually 100-500 images). TensorRT then runs the FP32 version of the model on this sample data and observes the distribution of activation values at each layer.</p> <p>Based on this observation, it calculates the optimal \"scaling factor\" for each layer to map the floating-point range to the <code>[-128, 127]</code> integer range with the minimum possible loss of information.</p>"},{"location":"guides/int8-quantization/#using-xinfer-for-int8-quantization","title":"Using <code>xInfer</code> for INT8 Quantization","text":"<p><code>xInfer</code> makes this complex process simple. You need to provide an implementation of the <code>INT8Calibrator</code> interface. For convenience, <code>xInfer</code> provides a ready-made <code>DataLoaderCalibrator</code> that works directly with an <code>xTorch</code> data loader.</p>"},{"location":"guides/int8-quantization/#example-building-an-int8-classifier-engine","title":"Example: Building an INT8 Classifier Engine","text":"<p>This example shows how to take a trained <code>xTorch</code> ResNet-18 and build a hyper-performant INT8 engine.</p> <p>File: <code>build_int8_engine.cpp</code> <pre><code>#include &lt;xinfer/builders/engine_builder.h&gt;\n#include &lt;xinfer/builders/calibrator.h&gt; // The Calibrator interface\n#include &lt;xtorch/xtorch.h&gt; // We need xTorch for the data loader\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\nint main() {\n    try {\n        std::string onnx_path = \"resnet18.onnx\"; // Assume this was exported from xTorch\n        std::string engine_path = \"resnet18_int8.engine\";\n\n        // --- Step 1: Prepare the Calibration Dataloader ---\n        // We need a small, representative sample of our validation data.\n        auto calibration_dataset = xt::datasets::ImageFolder(\n            \"/path/to/your/calibration_images/\", // A folder with ~500 images\n            xt::transforms::Compose({\n                std::make_shared&lt;xt::transforms::image::Resize&gt;({224, 224}),\n                std::make_shared&lt;xt::transforms::general::Normalize&gt;(\n                    std::vector&lt;float&gt;{0.485, 0.456, 0.406},\n                    std::vector&lt;float&gt;{0.229, 0.224, 0.225}\n                )\n            })\n        );\n        xt::dataloaders::ExtendedDataLoader calib_loader(calibration_dataset, 32, false);\n\n        // --- Step 2: Create the Calibrator Object ---\n        // We wrap our xTorch dataloader with the xInfer calibrator.\n        auto calibrator = std::make_shared&lt;xinfer::builders::DataLoaderCalibrator&gt;(calib_loader);\n\n        // --- Step 3: Configure and Run the Engine Builder ---\n        std::cout &lt;&lt; \"Building INT8 engine... This will take several minutes as it runs calibration.\\n\";\n        xinfer::builders::EngineBuilder builder;\n\n        builder.from_onnx(onnx_path)\n               .with_int8(calibrator) // Pass the calibrator to enable INT8 mode\n               .with_max_batch_size(32);\n\n        builder.build_and_save(engine_path);\n\n        std::cout &lt;&lt; \"INT8 engine built successfully: \" &lt;&lt; engine_path &lt;&lt; std::endl;\n\n    } catch (const std::exception&amp; e) {\n        std::cerr &lt;&lt; \"Error building INT8 engine: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n        return 1;\n    }\n    return 0;\n}\n</code></pre></p>"},{"location":"guides/int8-quantization/#when-to-use-int8","title":"When to Use INT8","text":"<ul> <li>Best For: CNNs and Transformers with standard layers like <code>Conv</code>, <code>Linear</code>, and <code>ReLU</code>.</li> <li>Latency-Critical Applications: If you need the absolute lowest possible latency (e.g., in robotics or real-time video analysis), INT8 is the best choice.</li> <li>High-Throughput Services: If you are running a model in a data center and want to maximize the number of inferences per second per dollar, INT8 is the most cost-effective solution.</li> </ul>"},{"location":"guides/int8-quantization/#when-to-be-cautious","title":"When to Be Cautious","text":"<ul> <li>Accuracy: Always validate the accuracy of your INT8 model against your FP32 baseline. For some models, especially those with very wide and unusual activation ranges, there can be a small drop in accuracy.</li> <li>Exotic Layers: Models with many non-standard or custom layers may not quantize as effectively.</li> </ul> <p>By following this guide, you can leverage the <code>xInfer</code> builders to unlock the full potential of your NVIDIA hardware, creating inference engines that are not just fast, but state-of-the-art in their performance and efficiency.</p>"},{"location":"hub/","title":"The Ignition Hub: Pre-Built Engines, On Demand.","text":"<p>Welcome to the future of C++ AI deployment. The Ignition Hub is a cloud-based repository of pre-built, hyper-optimized TensorRT engine files for the world's most popular open-source models.</p> <p>It is designed to completely eliminate the slowest, most complex, and most error-prone step in the entire inference pipeline: the engine build process.</p>"},{"location":"hub/#the-problem-the-build-barrier","title":"The Problem: The \"Build Barrier\"","text":"<p>Every developer who has used NVIDIA TensorRT knows the pain of the build step:</p> <ul> <li>It's Slow: Building an engine, especially with INT8 calibration, can take many minutes or even hours.</li> <li>It's Heavy: It requires you to have the full, multi-gigabyte CUDA Toolkit, cuDNN, and TensorRT SDKs installed and correctly configured on your machine.</li> <li>It's Brittle: An engine is a compiled binary. An engine built for an RTX 4090 with TensorRT 10 will not work on a Jetson Orin with TensorRT 8.6.</li> </ul> <p>This \"Build Barrier\" makes rapid prototyping, collaboration, and deployment across different hardware targets a massive challenge.</p>"},{"location":"hub/#the-solution-download-dont-build","title":"The Solution: Download, Don't Build.","text":"<p>The Ignition Hub solves this problem by treating the engine build process as a centralized, cloud-native service. We run the slow, complex build process on our massive cloud build farm, so you don't have to.</p> <p>The workflow is transformed:</p> <p>Old Workflow: 1.  Find and download a model's ONNX file. 2.  Install all the heavy SDKs (CUDA, cuDNN, TensorRT). 3.  Write complex C++ build code using the <code>xinfer::builders</code> API. 4.  Wait 10 minutes for the engine to build. 5.  Finally, run your application.</p> <p>The Ignition Hub Workflow: 1.  Find your model on the Hub. 2.  Call a single C++ function: <code>xinfer::hub::download_engine(...)</code>. 3.  Run your application instantly.</p>"},{"location":"hub/#how-it-works","title":"How It Works","text":"<p>The Ignition Hub is a massive, curated catalog of engine files. For every major open-source model (like Llama 3 or YOLOv8), we have pre-built and stored a matrix of engines for every common combination of:</p> <ul> <li>GPU Architecture: From the Jetson Nano (<code>sm_52</code>) to the H100 (<code>sm_90</code>).</li> <li>TensorRT Version: From legacy 8.x versions to the latest 10.x.</li> <li>Precision: <code>FP32</code>, <code>FP16</code>, and <code>INT8</code>.</li> </ul> <p>When you request an engine, our service delivers the one, single, perfectly-optimized binary that is guaranteed to work on your specific hardware and software configuration.</p>"},{"location":"hub/#example-the-magic-of-the-hub-integrated-zoo","title":"Example: The \"Magic\" of the Hub-Integrated <code>zoo</code>","text":"<p>The true power of the Hub is its seamless integration with the <code>xinfer::zoo</code> API. The <code>zoo</code> classes have special constructors that can download models directly from the Hub.</p> <p>This is the future of C++ AI deployment.</p> <pre><code>#include &lt;xinfer/zoo/vision/detector.h&gt;\n#include &lt;xinfer/hub/model_info.h&gt; // For the HardwareTarget struct\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    try {\n        // 1. Define the model we want and the exact hardware we are running on.\n        std::string model_id = \"yolov8n-coco\";\n        xinfer::hub::HardwareTarget my_target = {\n            .gpu_architecture = \"Jetson_Orin_Nano\",\n            .tensorrt_version = \"10.0.1\",\n            .precision = \"INT8\"\n        };\n\n        // 2. Instantiate the detector.\n        //    This one line of code will:\n        //    - Connect to the Ignition Hub.\n        //    - Find the perfect pre-built engine for our exact hardware.\n        //    - Download and cache it locally.\n        //    - Load it into the InferenceEngine.\n        std::cout &lt;&lt; \"Initializing detector from Ignition Hub...\\n\";\n        xinfer::zoo::vision::ObjectDetector detector(model_id, my_target);\n\n        // 3. The detector is now ready to run at maximum performance.\n        std::cout &lt;&lt; \"Detector ready. Running inference...\\n\";\n        cv::Mat image = cv::imread(\"my_image.jpg\");\n        auto detections = detector.predict(image);\n\n        std::cout &lt;&lt; \"Found \" &lt;&lt; detections.size() &lt;&lt; \" objects.\\n\";\n\n    } catch (const std::exception&amp; e) {\n        std::cerr &lt;&lt; \"An error occurred: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n        return 1;\n    }\n    return 0;\n}\n</code></pre>"},{"location":"hub/#get-started","title":"Get Started","text":"<p>Ready to stop building and start inferring?</p> <ul> <li>Usage Guide: Learn how to use the <code>xinfer::hub</code> C++ API.</li> <li>Browse the Hub: (Link to your future web UI)</li> </ul>"},{"location":"hub/usage/","title":"Hub Usage Guide","text":"<p>The <code>xInfer</code> Ignition Hub is designed to be accessed in two primary ways, catering to different levels of control and convenience:</p> <ol> <li>The Core <code>hub</code> API: For developers who want to manually manage the download and loading process.</li> <li>The Integrated <code>zoo</code> API: The recommended, \"magical\" way to use the hub, where the <code>zoo</code> classes handle everything automatically.</li> </ol>"},{"location":"hub/usage/#core-hub-api","title":"Core <code>hub</code> API","text":"<p>This is the low-level API for directly interacting with the Ignition Hub. It gives you precise control over the download process.</p> <p>Header: <code>#include &lt;xinfer/hub/downloader.h&gt;</code></p>"},{"location":"hub/usage/#hubdownload_engine","title":"<code>hub::download_engine</code>","text":"<p>This is the main function for fetching a pre-built engine file from the cloud.</p> <pre><code>#include &lt;xinfer/hub/downloader.h&gt;\n\nstd::string download_engine(\n    const std::string&amp; model_id,\n    const HardwareTarget&amp; target,\n    const std::string&amp; cache_dir = \"./xinfer_cache\",\n    const std::string&amp; hub_url = \"https://api.your-ignition-hub.com\"\n);\n</code></pre> <p>Parameters: - <code>model_id</code> (string): The unique identifier for the model on the hub, e.g., <code>\"yolov8n-coco\"</code>. - <code>target</code> (<code>HardwareTarget</code>): A struct specifying the exact hardware and software configuration you need. - <code>cache_dir</code> (string, optional): The local directory where downloaded engines will be stored. <code>xInfer</code> will automatically use a cached version if it already exists. - <code>hub_url</code> (string, optional): The base URL of the Ignition Hub API.</p> <p>Returns: A <code>std::string</code> containing the local file path to the downloaded (or cached) <code>.engine</code> file.</p>"},{"location":"hub/usage/#example-manually-downloading-an-engine","title":"Example: Manually Downloading an Engine","text":"<pre><code>#include &lt;xinfer/hub/downloader.h&gt;\n#include &lt;xinfer/core/engine.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    try {\n        // 1. Specify the exact engine we need.\n        std::string model_id = \"resnet50-imagenet\";\n        xinfer::hub::HardwareTarget my_gpu_target = {\n            .gpu_architecture = \"RTX_4090\",       // Or \"sm_89\"\n            .tensorrt_version = \"10.1.0\",\n            .precision = \"FP16\"\n        };\n\n        // 2. Download the engine file. This will be fast if it's already cached.\n        std::cout &lt;&lt; \"Downloading engine...\\n\";\n        std::string engine_path = xinfer::hub::download_engine(model_id, my_gpu_target);\n\n        // 3. Now, use the downloaded engine with the core::InferenceEngine.\n        std::cout &lt;&lt; \"Loading engine: \" &lt;&lt; engine_path &lt;&lt; \"\\n\";\n        xinfer::core::InferenceEngine engine(engine_path);\n\n        std::cout &lt;&lt; \"Engine loaded successfully!\\n\";\n        // ... proceed with your custom inference pipeline ...\n\n    } catch (const std::exception&amp; e) {\n        std::cerr &lt;&lt; \"Error: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n        return 1;\n    }\n    return 0;\n}\n</code></pre>"},{"location":"hub/usage/#the-integrated-zoo-api-recommended","title":"The Integrated <code>zoo</code> API (Recommended)","text":"<p>This is the simplest and most powerful way to use the Ignition Hub. The <code>zoo</code> classes have special constructors that take a <code>model_id</code> and handle the download and initialization process automatically.</p> <p>This workflow hides all the complexity of downloading, caching, and loading, providing a true one-line solution.</p>"},{"location":"hub/usage/#example-instantiating-a-classifier-from-the-hub","title":"Example: Instantiating a Classifier from the Hub","text":"<p>This example demonstrates how to get a hyper-performant, cloud-optimized <code>ImageClassifier</code> running with a single line of code.</p> <pre><code>#include &lt;xinfer/zoo/vision/classifier.h&gt;\n#include &lt;xinfer/hub/model_info.h&gt; // The HardwareTarget struct is defined here\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    try {\n        // 1. Define the model we want from the hub and our current hardware.\n        std::string model_id = \"resnet50-imagenet\";\n        xinfer::hub::HardwareTarget my_gpu_target;\n        my_gpu_target.gpu_architecture = \"RTX_4090\";\n        my_gpu_target.tensorrt_version = \"10.1.0\";\n        my_gpu_target.precision = \"FP16\";\n\n        // 2. Instantiate the classifier with the model_id and target.\n        //    This single constructor call does everything:\n        //    - Downloads the correct .engine file from the Ignition Hub.\n        //    - Downloads the associated labels.txt file.\n        //    - Loads the engine and initializes the pre-processor.\n        std::cout &lt;&lt; \"Initializing classifier from Ignition Hub...\\n\";\n        xinfer::zoo::vision::ImageClassifier classifier(model_id, my_gpu_target);\n        std::cout &lt;&lt; \"Classifier ready!\\n\";\n\n        // 3. The classifier is now ready for immediate, high-performance inference.\n        cv::Mat image = cv::imread(\"my_image.jpg\");\n        auto results = classifier.predict(image);\n\n        // ... print results ...\n\n    } catch (const std::exception&amp; e) {\n        std::cerr &lt;&lt; \"An error occurred: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n        return 1;\n    }\n    return 0;\n}\n</code></pre> <p>How it works</p> <p>The <code>zoo</code> constructors that take a <code>model_id</code> are high-level wrappers. Internally, they call <code>hub::download_engine</code> and <code>hub::download_asset</code> to fetch the necessary files, then they call the regular constructor that takes file paths. This provides the magical, \"it just works\" user experience.</p>"},{"location":"hub/usage/#hardware-target-reference","title":"Hardware Target Reference","text":"<p>When specifying your <code>HardwareTarget</code>, you need to use the correct string for your GPU architecture. Here are the most common values.</p> GPU Architecture Name <code>gpu_architecture</code> String Key GPU Examples Hopper <code>\"H100\"</code> or <code>\"sm_90\"</code> H100, H200 Ada Lovelace <code>\"RTX_4090\"</code> or <code>\"sm_89\"</code> RTX 4090, 4080, RTX 6000 Ada Ampere (Embedded) <code>\"Jetson_Orin\"</code> or <code>\"sm_87\"</code> Jetson AGX Orin, Orin Nano Ampere (High-End) <code>\"RTX_3090\"</code> or <code>\"sm_86\"</code> RTX 3090, 3080, A100 Turing <code>\"T4\"</code> or <code>\"sm_75\"</code> RTX 2080 Ti, T4, Jetson Xavier NX <p>You should match the <code>tensorrt_version</code> to the version of the TensorRT libraries installed on your system.</p>"},{"location":"zoo-api/","title":"The <code>xInfer</code> Model Zoo API","text":"<p>Welcome to the <code>xInfer::zoo</code>. This is the high-level, \"batteries-included\" API for <code>xInfer</code>.</p> <p>The <code>zoo</code> is a collection of pre-packaged, hyper-optimized, and incredibly easy-to-use solutions for the world's most common AI tasks.</p>"},{"location":"zoo-api/#the-philosophy-solutions-not-just-tools","title":"The Philosophy: Solutions, Not Just Tools","text":"<p>While the Core Toolkit provides the powerful, low-level \"engine parts\" for building custom pipelines, the <code>zoo</code> provides the finished \"F1 car.\"</p> <p>Each class in the <code>zoo</code> is a complete, end-to-end pipeline that abstracts away all the complexity of pre-processing, inference, and post-processing. The goal is to let you solve a complex problem like real-time object detection or image generation with just two lines of C++ code:</p> <ol> <li>One line to initialize the pipeline from a pre-built engine.</li> <li>One line to predict.</li> </ol> <p>This is the power of the <code>zoo</code>. It gives you the full, state-of-the-art performance of a custom C++/CUDA/TensorRT application with the simplicity of a high-level library.</p>"},{"location":"zoo-api/#key-features-of-all-zoo-classes","title":"Key Features of All <code>zoo</code> Classes","text":"<ul> <li>Performance by Default: Every <code>zoo</code> class is built on top of the hyper-performant <code>xInfer::core::InferenceEngine</code> and uses fused CUDA kernels from the <code>preproc</code> and <code>postproc</code> modules wherever possible.</li> <li>Simple, Task-Oriented API: You don't interact with raw tensors. You provide a <code>cv::Mat</code> image and get back a <code>std::vector&lt;BoundingBox&gt;</code> or a <code>std::string</code>. The API is designed around the final answer, not the intermediate steps.</li> <li>Seamless Hub Integration: Most <code>zoo</code> classes have special constructors that can download a pre-built, perfectly optimized engine for your hardware directly from the Ignition Hub. This provides a \"zero-setup\" user experience.</li> <li>Robust and Production-Ready: These classes are designed to be used directly in your final application. They are efficient, safe, and easy to integrate.</li> </ul>"},{"location":"zoo-api/#the-zoo-catalog-of-solutions","title":"The <code>zoo</code> Catalog of Solutions","text":"<p>The <code>zoo</code> is organized by domain. Explore the available pipelines below to find the solution you need.</p>"},{"location":"zoo-api/#computer-vision","title":"\ud83d\uddbc\ufe0f Computer Vision","text":"<p>Tools for understanding and analyzing visual information from images and video. This is the most mature and comprehensive part of the <code>zoo</code>.</p> <ul> <li>Tasks: Image Classification, Object Detection, Semantic &amp; Instance Segmentation, Pose Estimation, Face Recognition, OCR, and many more.</li> </ul> <p>\u27a1\ufe0f Explore the Vision API</p>"},{"location":"zoo-api/#generative-ai","title":"\u2728 Generative AI","text":"<p>Powerful pipelines for creating novel content, from images and audio to 3D models.</p> <ul> <li>Tasks: Text-to-Image (Diffusion), Image Generation (GANs), Super-Resolution, Style Transfer, Text-to-Speech.</li> </ul> <p>\u27a1\ufe0f Explore the Generative API</p>"},{"location":"zoo-api/#natural-language-processing-nlp","title":"\ud83d\udcdd Natural Language Processing (NLP)","text":"<p>High-throughput, low-latency solutions for understanding and processing human language.</p> <ul> <li>Tasks: Text Classification, Named Entity Recognition (NER), Sentence Embeddings for RAG, Summarization, and Translation.</li> </ul> <p>\u27a1\ufe0f Explore the NLP API</p>"},{"location":"zoo-api/#audio-signal-processing","title":"\ud83c\udfa7 Audio &amp; Signal Processing","text":"<p>Real-time pipelines for analyzing audio signals, from speech to environmental sounds.</p> <ul> <li>Tasks: Speech Recognition, Audio Classification, Speaker Identification, Music Source Separation.</li> </ul> <p>\u27a1\ufe0f Explore the Audio &amp; DSP API</p>"},{"location":"zoo-api/#time-series","title":"\ud83d\udcc8 Time Series","text":"<p>Specialized solutions for forecasting and analyzing sequential data.</p> <ul> <li>Tasks: Forecasting, Anomaly Detection, and Classification.</li> </ul> <p>\u27a1\ufe0f Explore the Time Series API</p>"},{"location":"zoo-api/#3d-spatial-computing","title":"\ud83e\uddca 3D &amp; Spatial Computing","text":"<p>Cutting-edge pipelines for processing 3D data from sensors like LIDAR.</p> <ul> <li>Tasks: 3D Reconstruction (Gaussian Splatting), Point Cloud Detection, Point Cloud Segmentation.</li> </ul> <p>\u27a1\ufe0f Explore the 3D API</p>"},{"location":"zoo-api/#geospatial","title":"\ud83c\udf0d Geospatial","text":"<p>Specialized tools for analyzing satellite and aerial imagery.</p> <ul> <li>Tasks: Building &amp; Road Segmentation, Change Detection, Maritime Object Detection.</li> </ul> <p>\u27a1\ufe0f Explore the Geospatial API</p>"},{"location":"zoo-api/#medical-imaging","title":"\u2695\ufe0f Medical Imaging","text":"<p>High-performance pipelines for medical image analysis.</p> <ul> <li>Tasks: Tumor Detection, Cell Segmentation, Retinal Abnormality Scanning, Artery Analysis.</li> </ul> <p>\u27a1\ufe0f Explore the Medical API</p>"},{"location":"zoo-api/#document-ai","title":"\ud83d\udcc4 Document AI","text":"<p>Pipelines for understanding the structure and content of documents.</p> <ul> <li>Tasks: Table Extraction, Signature Detection, Handwriting Recognition.</li> </ul> <p>\u27a1\ufe0f Explore the Document AI API</p>"},{"location":"zoo-api/#specialized-rl","title":"\ud83d\ude80 Specialized &amp; RL","text":"<p>Hyper-specialized, high-value solutions for specific industries and advanced applications.</p> <ul> <li>Tasks: Reinforcement Learning Policies, Financial (HFT) Models, Physics Simulation, Genomics.</li> </ul> <p>\u27a1\ufe0f Explore the Specialized API</p>"},{"location":"zoo-api/audio/","title":"Zoo API: Audio &amp; Speech","text":"<p>The <code>xinfer::zoo::audio</code> module provides a suite of high-performance pipelines for the most common audio and speech processing tasks.</p> <p>These classes are built on top of <code>xInfer</code>'s custom CUDA pre-processing kernels and hyper-optimized TensorRT engines. They are designed for latency-critical applications like real-time transcription, voice recognition, and audio analysis, where the performance of C++ is a fundamental requirement.</p> <p>All pipelines in this module are powered by the <code>xinfer::preproc::AudioProcessor</code>, which performs the entire waveform-to-spectrogram conversion on the GPU for maximum efficiency.</p>"},{"location":"zoo-api/audio/#classifier","title":"<code>Classifier</code>","text":"<p>Classifies a short audio clip into a predefined set of categories.</p> <p>Header: <code>#include &lt;xinfer/zoo/audio/classifier.h&gt;</code></p>"},{"location":"zoo-api/audio/#use-case-environmental-sound-detection","title":"Use Case: Environmental Sound Detection","text":"<p>An application needs to identify sounds in its environment, such as a \"siren,\" \"dog bark,\" or \"glass breaking,\" for security or contextual awareness.</p> <p><pre><code>#include &lt;xinfer/zoo/audio/classifier.h&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\nint main() {\n    // 1. Configure the audio classifier.\n    xinfer::zoo::audio::ClassifierConfig config;\n    config.engine_path = \"assets/esc50_classifier.engine\";\n    config.labels_path = \"assets/esc50_labels.txt\";\n    // The audio_config should match the model's training parameters.\n    config.audio_config.sample_rate = 16000;\n\n    // 2. Initialize.\n    xinfer::zoo::audio::Classifier classifier(config);\n\n    // 3. Load a raw audio waveform (e.g., from a .wav file).\n    std::vector&lt;float&gt; waveform; // Assume this is loaded with a 2-second audio clip\n\n    // 4. Predict the top 3 sound classes.\n    auto results = classifier.predict(waveform, 3);\n\n    // 5. Print the results.\n    std::cout &lt;&lt; \"Top 3 Audio Classifications:\\n\";\n    for (const auto&amp; result : results) {\n        printf(\" - Label: %-20s, Confidence: %.4f\\n\", result.label.c_str(), result.confidence);\n    }\n}\n</code></pre> Config Struct: <code>ClassifierConfig</code> Input: <code>std::vector&lt;float&gt;</code> audio waveform. Output Struct: <code>AudioClassificationResult</code>.</p>"},{"location":"zoo-api/audio/#speechrecognizer","title":"<code>SpeechRecognizer</code>","text":"<p>Transcribes spoken language from an audio waveform into text.</p> <p>Header: <code>#include &lt;xinfer/zoo/audio/speech_recognizer.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/audio/speech_recognizer.h&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\nint main() {\n    // 1. Configure the speech recognizer.\n    xinfer::zoo::audio::SpeechRecognizerConfig config;\n    config.engine_path = \"assets/whisper_base_en.engine\";\n    config.character_map_path = \"assets/whisper_chars.txt\";\n    config.audio_config.sample_rate = 16000;\n\n    // 2. Initialize.\n    xinfer::zoo::audio::SpeechRecognizer recognizer(config);\n\n    // 3. Load a waveform of someone speaking.\n    std::vector&lt;float&gt; speech_waveform; // Assume this is loaded\n\n    // 4. Get the transcription.\n    auto result = recognizer.predict(speech_waveform);\n\n    // 5. Print the result.\n    std::cout &lt;&lt; \"Transcription: \\\"\" &lt;&lt; result.text &lt;&lt; \"\\\"\\n\";\n    std::cout &lt;&lt; \"Confidence: \" &lt;&lt; result.confidence &lt;&lt; \"\\n\";\n}\n</code></pre> Config Struct: <code>SpeechRecognizerConfig</code> Input: <code>std::vector&lt;float&gt;</code> audio waveform. Output Struct: <code>TranscriptionResult</code>. \"F1 Car\" Technology: This pipeline uses the <code>postproc::ctc_decoder</code> to perform the complex CTC decoding on the GPU, avoiding a massive logits transfer to the CPU.</p>"},{"location":"zoo-api/audio/#speakeridentifier","title":"<code>SpeakerIdentifier</code>","text":"<p>Identifies a person from a sample of their voice, based on a pre-registered database of known speakers.</p> <p>Header: <code>#include &lt;xinfer/zoo/audio/speaker_identifier.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/audio/speaker_identifier.h&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\nint main() {\n    xinfer::zoo::audio::SpeakerIdentifierConfig config;\n    config.engine_path = \"assets/speaker_embedding.engine\";\n\n    xinfer::zoo::audio::SpeakerIdentifier identifier(config);\n\n    // 1. Register known speakers by providing labeled voice samples.\n    std::vector&lt;float&gt; alice_sample; // Load Alice's voice\n    std::vector&lt;float&gt; bob_sample;   // Load Bob's voice\n    identifier.register_speaker(\"Alice\", alice_sample);\n    identifier.register_speaker(\"Bob\", bob_sample);\n    std::cout &lt;&lt; \"Registered speakers: Alice, Bob\\n\";\n\n    // 2. Load an unknown voice sample to identify.\n    std::vector&lt;float&gt; unknown_sample; // Load an unknown voice\n\n    // 3. Identify the speaker.\n    auto result = identifier.identify(unknown_sample);\n\n    // 4. Print the result.\n    std::cout &lt;&lt; \"Identified speaker: \" &lt;&lt; result.speaker_label\n              &lt;&lt; \" (Similarity: \" &lt;&lt; result.similarity_score &lt;&lt; \")\\n\";\n}\n</code></pre> Config Struct: <code>SpeakerIdentifierConfig</code> Methods: - <code>register_speaker(const std::string&amp;, const std::vector&lt;float&gt;&amp;)</code> to enroll speakers. - <code>identify(const std::vector&lt;float&gt;&amp;)</code> to find the best match. - <code>compare(const SpeakerEmbedding&amp;, const SpeakerEmbedding&amp;)</code> to get a similarity score between two embeddings.</p>"},{"location":"zoo-api/audio/#eventdetector","title":"<code>EventDetector</code>","text":"<p>Detects the start and end times of specific sound events in a continuous audio stream.</p> <p>Header: <code>#include &lt;xinfer/zoo/audio/event_detector.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/audio/event_detector.h&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\nint main() {\n    xinfer::zoo::audio::EventDetectorConfig config;\n    config.engine_path = \"assets/audio_event_detector.engine\";\n    config.labels_path = \"assets/event_labels.txt\"; // e.g., \"glass_break\", \"siren\"\n    config.event_threshold = 0.7f;\n\n    xinfer::zoo::audio::EventDetector detector(config);\n\n    std::vector&lt;float&gt; long_audio_stream; // A long recording\n\n    auto events = detector.predict(long_audio_stream);\n\n    std::cout &lt;&lt; \"Detected \" &lt;&lt; events.size() &lt;&lt; \" audio events:\\n\";\n    for (const auto&amp; event : events) {\n        printf(\" - Event: %-15s, Start: %.2fs, End: %.2fs\\n\",\n               event.label.c_str(), event.start_time_seconds, event.end_time_seconds);\n    }\n}\n</code></pre> Config Struct: <code>EventDetectorConfig</code> Input: <code>std::vector&lt;float&gt;</code> audio waveform. Output Struct: <code>AudioEvent</code> (contains label, start/end times, and confidence).</p>"},{"location":"zoo-api/audio/#musicsourceseparator","title":"<code>MusicSourceSeparator</code>","text":"<p>Separates a mixed music track into its constituent sources, such as vocals, drums, bass, and other instruments.</p> <p>Header: <code>#include &lt;xinfer/zoo/audio/music_source_separator.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/audio/music_source_separator.h&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;map&gt;\n\nint main() {\n    xinfer::zoo::audio::MusicSourceSeparatorConfig config;\n    config.engine_path = \"assets/music_separator.engine\";\n    config.source_names = {\"vocals\", \"drums\", \"bass\", \"other\"};\n\n    xinfer::zoo::audio::MusicSourceSeparator separator(config);\n\n    std::vector&lt;float&gt; song_waveform; // Load a full song\n\n    // The result is a map from the source name to its isolated audio waveform.\n    std::map&lt;std::string, xinfer::zoo::audio::AudioWaveform&gt; sources = separator.predict(song_waveform);\n\n    std::cout &lt;&lt; \"Separated music into \" &lt;&lt; sources.size() &lt;&lt; \" sources:\\n\";\n    for (const auto&amp; pair : sources) {\n        std::cout &lt;&lt; \" - Found source: \" &lt;&lt; pair.first &lt;&lt; \" (\" &lt;&lt; pair.second.size() &lt;&lt; \" samples)\\n\";\n        // In a real app, you would save each waveform to a new .wav file.\n    }\n}\n</code></pre> Config Struct: <code>MusicSourceSeparatorConfig</code> Input: <code>std::vector&lt;float&gt;</code> mixed audio waveform. Output: <code>std::map&lt;std::string, AudioWaveform&gt;</code>.</p>"},{"location":"zoo-api/document/","title":"Zoo API: Document AI","text":"<p>The <code>xinfer::zoo::document</code> module provides a suite of high-performance pipelines for understanding the structure and content of complex documents like invoices, forms, and reports.</p> <p>Standard OCR can extract text, but it doesn't understand its meaning or layout. The <code>zoo</code> classes in this module are built on top of advanced, multi-modal models that can parse a document's visual and textual elements to extract structured data. These pipelines are hyper-optimized with <code>xInfer</code> to enable high-throughput document processing.</p>"},{"location":"zoo-api/document/#ocr-optical-character-recognition","title":"<code>OCR</code> (Optical Character Recognition)","text":"<p>A complete, two-stage pipeline that first detects the locations of text in an image and then recognizes the characters within each location.</p> <p>Header: <code>#include &lt;xinfer/zoo/document/ocr.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/document/ocr.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    // 1. Configure the OCR pipeline with two separate engines.\n    xinfer::zoo::document::OCRConfig config;\n    config.detection_engine_path = \"assets/craft_text_detector.engine\";\n    config.recognition_engine_path = \"assets/crnn_text_recognizer.engine\";\n    config.character_set = \"0123456789abcdefghijklmnopqrstuvwxyz\";\n\n    // 2. Initialize.\n    xinfer::zoo::document::OCR ocr_pipeline(config);\n\n    // 3. Process a document image.\n    cv::Mat image = cv::imread(\"assets/invoice.png\");\n    std::vector&lt;xinfer::zoo::document::OCRResult&gt; results = ocr_pipeline.predict(image);\n\n    // 4. Print the extracted text and its location.\n    std::cout &lt;&lt; \"Extracted \" &lt;&lt; results.size() &lt;&lt; \" text regions:\\n\";\n    for (const auto&amp; result : results) {\n        std::cout &lt;&lt; \" - Text: \\\"\" &lt;&lt; result.text &lt;&lt; \"\\\" (Confidence: \" &lt;&lt; result.confidence &lt;&lt; \")\\n\";\n    }\n}\n</code></pre> Config Struct: <code>OCRConfig</code> Input: <code>cv::Mat</code> document image. Output Struct: <code>OCRResult</code> (contains the recognized text, its bounding polygon, and a confidence score).</p>"},{"location":"zoo-api/document/#layoutparser","title":"<code>LayoutParser</code>","text":"<p>Performs document layout analysis. It segments a document page into its constituent structural elements, such as paragraphs, tables, figures, and headers.</p> <p>Header: <code>#include &lt;xinfer/zoo/document/layout_parser.h&gt;</code> (Note: The implementation of this class would use a powerful instance segmentation model.)</p> <p><pre><code>#include &lt;xinfer/zoo/document/layout_parser.h&gt; // Conceptual header\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    xinfer::zoo::document::LayoutParserConfig config;\n    config.engine_path = \"assets/layoutlmv3.engine\"; // Engine for a model like LayoutLMv3\n    config.labels_path = \"assets/layout_labels.txt\"; // e.g., \"paragraph\", \"table\", \"figure\"\n\n    xinfer::zoo::document::LayoutParser parser(config);\n\n    cv::Mat page_image = cv::imread(\"assets/report_page.png\");\n    auto layout_elements = parser.predict(page_image);\n\n    std::cout &lt;&lt; \"Detected \" &lt;&lt; layout_elements.size() &lt;&lt; \" layout elements:\\n\";\n    for (const auto&amp; element : layout_elements) {\n        std::cout &lt;&lt; \" - Found a '\" &lt;&lt; element.label &lt;&lt; \"' at bounding box [ \"\n                  &lt;&lt; element.bounding_box.x &lt;&lt; \", \" &lt;&lt; element.bounding_box.y &lt;&lt; \" ]\\n\";\n    }\n}\n</code></pre> Config Struct: <code>LayoutParserConfig</code> Input: <code>cv::Mat</code> document image. Output Struct: <code>LayoutElement</code> (contains the element's bounding box and its class label).</p>"},{"location":"zoo-api/document/#tableextractor","title":"<code>TableExtractor</code>","text":"<p>A complex pipeline that detects the structure of a table in an image and uses OCR to extract its contents into a structured, machine-readable format.</p> <p>Header: <code>#include &lt;xinfer/zoo/document/table_extractor.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/document/table_extractor.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    xinfer::zoo::document::TableExtractorConfig config;\n    config.structure_engine_path = \"assets/table_structure_model.engine\";\n    config.ocr_config_path = \"assets/ocr_config.json\"; // Path to a config for the internal OCR engine\n\n    xinfer::zoo::document::TableExtractor extractor(config);\n\n    cv::Mat table_image = cv::imread(\"assets/financial_table.png\");\n    xinfer::zoo::document::Table extracted_table = extractor.predict(table_image);\n\n    std::cout &lt;&lt; \"Extracted Table Contents:\\n\";\n    for (const auto&amp; row : extracted_table) {\n        for (const auto&amp; cell : row) {\n            std::cout &lt;&lt; cell &lt;&lt; \"\\t|\\t\";\n        }\n        std::cout &lt;&lt; \"\\n\";\n    }\n}\n</code></pre> Config Struct: <code>TableExtractorConfig</code> Input: <code>cv::Mat</code> image of a table. Output: <code>Table</code> (a <code>std::vector&lt;std::vector&lt;std::string&gt;&gt;</code>). \"F1 Car\" Technology: This class is a powerful orchestrator. It internally uses a TensorRT engine for table structure recognition and the <code>zoo::document::OCR</code> pipeline for text extraction, combining them with C++ logic to reconstruct the final table.</p>"},{"location":"zoo-api/document/#signaturedetector","title":"<code>SignatureDetector</code>","text":"<p>Detects the presence and location of handwritten signatures in a document.</p> <p>Header: <code>#include &lt;xinfer/zoo/document/signature_detector.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/document/signature_detector.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    xinfer::zoo::document::SignatureDetectorConfig config;\n    config.engine_path = \"assets/signature_detector.engine\";\n\n    xinfer::zoo::document::SignatureDetector detector(config);\n\n    cv::Mat contract_page = cv::imread(\"assets/signed_contract.jpg\");\n    auto signatures = detector.predict(contract_page);\n\n    std::cout &lt;&lt; \"Found \" &lt;&lt; signatures.size() &lt;&lt; \" signatures.\\n\";\n    // You could then crop these regions for signature verification.\n}\n</code></pre> Config Struct: <code>SignatureDetectorConfig</code> Input: <code>cv::Mat</code> document image. Output Struct: <code>Signature</code> (contains bounding box and confidence score).</p>"},{"location":"zoo-api/document/#handwritingrecognizer","title":"<code>HandwritingRecognizer</code>","text":"<p>A specialized OCR pipeline designed to transcribe handwritten text from an image patch.</p> <p>Header: <code>#include &lt;xinfer/zoo/document/handwriting_recognizer.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/document/handwriting_recognizer.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    xinfer::zoo::document::HandwritingRecognizerConfig config;\n    config.engine_path = \"assets/handwriting_crnn.engine\";\n    config.character_map_path = \"assets/handwriting_chars.txt\";\n\n    xinfer::zoo::document::HandwritingRecognizer recognizer(config);\n\n    // Assume we've already detected the location of a handwritten line\n    cv::Mat handwritten_line = cv::imread(\"assets/handwritten_note_line.png\");\n    auto result = recognizer.predict(handwritten_line);\n\n    std::cout &lt;&lt; \"Recognized Text: \\\"\" &lt;&lt; result.text &lt;&lt; \"\\\"\\n\";\n    std::cout &lt;&lt; \"Confidence: \" &lt;&lt; result.confidence &lt;&lt; \"\\n\";\n}\n</code></pre> Config Struct: <code>HandwritingRecognizerConfig</code> Input: <code>cv::Mat</code> image of a single line of handwritten text. Output Struct: <code>HandwritingRecognitionResult</code> (contains the transcribed text and a confidence score).</p>"},{"location":"zoo-api/drones/","title":"Zoo API: Autonomous Drones","text":"<p>The <code>xinfer::zoo::drones</code> module provides hyper-optimized, low-latency pipelines for the core perception and control tasks required for autonomous drones.</p> <p>In aerial robotics, the \"perception-to-action\" loop is paramount. Decisions must be made in milliseconds to ensure stable, reactive flight, especially in cluttered, GPS-denied environments. The <code>zoo</code> classes in this module are designed to be the \"brain stem\" of a drone's flight controller, providing the essential AI capabilities with the hard real-time performance that C++ and <code>xInfer</code> guarantee.</p> <p>These pipelines are built to run on power-constrained, embedded NVIDIA Jetson hardware, which is the standard for the drone industry.</p>"},{"location":"zoo-api/drones/#navigationpolicy","title":"<code>NavigationPolicy</code>","text":"<p>Executes a trained Reinforcement Learning (RL) policy for autonomous, vision-based navigation.</p> <p>Header: <code>#include &lt;xinfer/zoo/drones/navigation_policy.h&gt;</code></p>"},{"location":"zoo-api/drones/#use-case-obstacle-avoidance-in-a-cluttered-environment","title":"Use Case: Obstacle Avoidance in a Cluttered Environment","text":"<p>A drone needs to fly through a dense forest or a warehouse to reach a target. It cannot rely on GPS. Instead, it uses a forward-facing depth camera to perceive its surroundings. At every step, an RL policy takes the depth image and the drone's current velocity as input, and outputs the optimal flight commands to avoid obstacles and make progress towards its goal.</p> <p>This entire control loop must run at a very high frequency (e.g., 50-100Hz) to ensure the drone can react to obstacles in time.</p> <p><pre><code>#include &lt;xinfer/zoo/drones/navigation_policy.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\n// This function represents the drone's main, high-frequency control loop.\nvoid flight_control_step(xinfer::zoo::drones::NavigationPolicy&amp; policy, DroneHardware&amp; drone) {\n    // 1. Get the latest sensor readings from the drone's hardware.\n    cv::Mat depth_image = drone.get_depth_camera_frame();\n    std::vector&lt;float&gt; drone_state = drone.get_imu_and_velocity_data();\n\n    // 2. Execute the policy to get the next flight command.\n    //    This is a single, ultra-low-latency call to the TensorRT engine.\n    //    The entire operation should take only a few milliseconds.\n    xinfer::zoo::drones::NavigationAction action = policy.predict(depth_image, drone_state);\n\n    // 3. Send the low-level motor commands to the flight controller.\n    drone.set_motor_commands(action.roll, action.pitch, action.yaw, action.thrust);\n}\n\nint main() {\n    // 1. Configure and initialize the navigation policy during drone startup.\n    //    The engine is pre-built from a policy (typically a CNN+MLP)\n    //    trained in a photorealistic simulator like NVIDIA Isaac Sim.\n    xinfer::zoo::drones::NavigationPolicyConfig config;\n    config.engine_path = \"assets/forest_navigation_policy.engine\";\n\n    xinfer::zoo::drones::NavigationPolicy policy(config);\n    std::cout &lt;&lt; \"Drone navigation policy loaded and ready.\\n\";\n\n    // DroneHardware drone; // A placeholder for the actual drone's hardware interface\n\n    // 2. Run the main flight control loop.\n    // while (drone.is_armed()) {\n    //     flight_control_step(policy, drone);\n    //     // The loop would be timed to run at a fixed frequency (e.g., 100Hz).\n    // }\n\n    return 0;\n}\n</code></pre> Config Struct: <code>NavigationPolicyConfig</code> Input: <code>cv::Mat</code> depth image and a <code>std::vector&lt;float&gt;</code> of the drone's physical state. Output Struct: <code>NavigationAction</code> (contains roll, pitch, yaw, and thrust commands). \"F1 Car\" Technology: The policy's neural network (typically a small CNN to process the image, followed by an MLP) is compiled by <code>xInfer</code>'s <code>builders</code> into a single, fused, INT8-quantized TensorRT engine. This allows the entire perception-to-action pipeline to run with the minimal latency and power consumption that are critical for extending a drone's flight time and ensuring its stability.</p>"},{"location":"zoo-api/dsp/","title":"Zoo API: Digital Signal Processing (DSP)","text":"<p>The <code>xinfer::zoo::dsp</code> module provides a suite of high-performance, GPU-accelerated primitives for common Digital Signal Processing tasks.</p> <p>While many <code>zoo</code> modules are built around neural networks, the <code>dsp</code> module provides fundamental building blocks for processing raw signals like audio waveforms. These classes are designed to be integrated into larger pipelines, such as the pre-processing stage for an audio model or a real-time signal analysis application.</p> <p>Every class in this module is a C++ wrapper around a custom, high-performance CUDA/cuFFT implementation, designed to keep the entire signal processing chain on the GPU.</p>"},{"location":"zoo-api/dsp/#spectrogram","title":"<code>Spectrogram</code>","text":"<p>Calculates the spectrogram of a raw audio waveform. This is the most fundamental feature representation for most audio-based AI tasks.</p> <p>Header: <code>#include &lt;xinfer/zoo/dsp/spectrogram.h&gt;</code></p>"},{"location":"zoo-api/dsp/#use-case-feature-extraction-for-speech-recognition","title":"Use Case: Feature Extraction for Speech Recognition","text":"<p>Before an audio clip can be fed into a speech recognition model, it must be converted into a spectrogram, which represents how the signal's frequency content changes over time. The <code>Spectrogram</code> class performs this entire conversion on the GPU.</p> <p><pre><code>#include &lt;xinfer/zoo/dsp/spectrogram.h&gt;\n#include &lt;opencv2/opencv.hpp&gt; // Used here for easy visualization/saving\n#include &lt;vector&gt;\n\nint main() {\n    // 1. Configure the spectrogram parameters.\n    xinfer::zoo::dsp::SpectrogramConfig config;\n    config.sample_rate = 16000;\n    config.n_fft = 400;       // FFT window size\n    config.hop_length = 160;  // Step size between frames\n\n    // 2. Initialize the spectrogram generator.\n    xinfer::zoo::dsp::Spectrogram spectrogram_generator(config);\n\n    // 3. Load a raw audio waveform.\n    std::vector&lt;float&gt; waveform; // Assume this is loaded from a .wav file\n    // ... load waveform data ...\n\n    // 4. Process the waveform to get the spectrogram.\n    //    The result is a cv::Mat where rows are frequency bins and columns are time frames.\n    cv::Mat spectrogram = spectrogram_generator.process(waveform);\n\n    // 5. Save the spectrogram as an image for visualization.\n    cv::Mat db_spectrogram;\n    cv::normalize(spectrogram, db_spectrogram, 0, 255, cv::NORM_MINMAX, CV_8U);\n    cv::imwrite(\"spectrogram.png\", db_spectrogram);\n}\n</code></pre> Config Struct: <code>SpectrogramConfig</code> Input: <code>std::vector&lt;float&gt;</code> containing the raw audio waveform. Output: <code>cv::Mat</code> (type <code>CV_32F</code>) containing the log-power spectrogram. \"F1 Car\" Technology: This class orchestrates a chain of CUDA kernels and NVIDIA's <code>cuFFT</code> library. It performs framing, windowing, FFT, and power-to-dB conversion entirely on the GPU, avoiding any slow CPU round-trips.</p>"},{"location":"zoo-api/dsp/#signalfilter","title":"<code>SignalFilter</code>","text":"<p>Applies a Finite Impulse Response (FIR) filter to a raw signal. This is a fundamental operation for noise reduction and signal separation.</p> <p>Header: <code>#include &lt;xinfer/zoo/dsp/signal_filter.h&gt;</code></p>"},{"location":"zoo-api/dsp/#use-case-filtering-a-noisy-signal","title":"Use Case: Filtering a Noisy Signal","text":"<p>An engineer needs to apply a low-pass filter to an audio signal to remove high-frequency noise before further processing.</p> <p><pre><code>#include &lt;xinfer/zoo/dsp/signal_filter.h&gt;\n#include &lt;vector&gt;\n\nint main() {\n    // 1. Configure a low-pass filter.\n    xinfer::zoo::dsp::SignalFilterConfig config;\n    config.type = xinfer::zoo::dsp::FilterType::LOW_PASS;\n    config.sample_rate = 44100;\n    config.cutoff_freq1 = 3000.0f; // Cut off frequencies above 3kHz\n    config.filter_length = 129;    // The length of the filter kernel\n\n    // 2. Initialize the filter. This pre-computes the filter kernel on the GPU.\n    xinfer::zoo::dsp::SignalFilter low_pass_filter(config);\n\n    // 3. Load a noisy audio signal.\n    std::vector&lt;float&gt; noisy_waveform;\n    // ... load waveform data ...\n\n    // 4. Process the signal.\n    //    The filtering is done via high-speed convolution in the frequency domain.\n    std::vector&lt;float&gt; filtered_waveform = low_pass_filter.process(noisy_waveform);\n\n    // `filtered_waveform` now contains the clean signal.\n}\n</code></pre> Config Struct: <code>SignalFilterConfig</code> (can specify <code>LOW_PASS</code>, <code>HIGH_PASS</code>, or <code>BAND_PASS</code>). Input: <code>std::vector&lt;float&gt;</code> of the input signal. Output: <code>std::vector&lt;float&gt;</code> of the filtered signal. \"F1 Car\" Technology: This class implements the fast convolution algorithm. It uses <code>cuFFT</code> to transform both the signal and the filter kernel into the frequency domain, performs a single, fast element-wise multiplication, and then uses an inverse FFT to get the final, filtered signal back in the time domain. This is orders of magnitude faster than a direct, time-domain convolution on the CPU for large signals.</p>"},{"location":"zoo-api/gaming/","title":"Zoo API: Game Development","text":"<p>The <code>xinfer::zoo::gaming</code> module provides hyper-optimized solutions for the most demanding computational problems in modern video game development.</p> <p>These tools are designed to be integrated into game engines like Unreal Engine and Unity to enable next-generation AI behaviors, physics simulations, and content creation workflows that are impossible to achieve with standard, general-purpose tools. In an industry where real-time performance is paramount, <code>xInfer</code> provides the critical F1-car components needed to build truly next-generation games.</p>"},{"location":"zoo-api/gaming/#npcbehaviorpolicy","title":"<code>NPCBehaviorPolicy</code>","text":"<p>A hyper-optimized engine for running thousands of individual AI \"brains\" for Non-Player Characters (NPCs) simultaneously.</p> <p>Header: <code>#include &lt;xinfer/zoo/gaming/npc_behavior_policy.h&gt;</code></p>"},{"location":"zoo-api/gaming/#use-case-creating-smart-emergent-ai","title":"Use Case: Creating Smart, Emergent AI","text":"<p>Instead of simple, scripted AI, train complex reinforcement learning policies that allow NPCs to react intelligently to the player and the world. The <code>NPCBehaviorPolicy</code> engine is designed to run these policies for every NPC in a level within a single game frame.</p> <p><pre><code>#include &lt;xinfer/zoo/gaming/npc_behavior_policy.h&gt;\n#include &lt;xinfer/core/tensor.h&gt;\n\n// This function would be called once per game frame from the main engine loop.\nvoid update_all_npc_ai(xinfer::zoo::gaming::NPCBehaviorPolicy&amp; policy, GameWorld&amp; world) {\n    // 1. Gather the current state of all active NPCs into a single batched tensor on the GPU.\n    xinfer::core::Tensor npc_state_batch = world.get_all_npc_states_as_gpu_tensor();\n\n    // 2. Execute the policy for all NPCs in a single, efficient GPU call.\n    //    This is massively faster than running hundreds of individual model calls.\n    xinfer::core::Tensor npc_action_batch = policy.predict_batch(npc_state_batch);\n\n    // 3. Apply the resulting actions back to each NPC in the game world.\n    world.set_all_npc_actions_from_gpu_tensor(npc_action_batch);\n}\n\nint main() {\n    // Initialize the policy engine once during level load.\n    xinfer::zoo::gaming::NPCBehaviorPolicyConfig config;\n    config.engine_path = \"assets/npc_behavior_policy.engine\";\n\n    xinfer::zoo::gaming::NPCBehaviorPolicy policy(config);\n\n    // while (game_is_running) {\n    //     update_all_npc_ai(policy, current_world);\n    // }\n}\n</code></pre> Config Struct: <code>NPCBehaviorPolicyConfig</code> Input: A batched <code>core::Tensor</code> where each row is the state of one NPC. Output: A batched <code>core::Tensor</code> where each row is the action for one NPC. \"F1 Car\" Technology: The key is batched inference. This engine is optimized to run hundreds or thousands of small MLP forward passes in a single GPU launch, avoiding the massive overhead of individual inference calls from a game engine script.</p>"},{"location":"zoo-api/gaming/#fluidsimulator-from-zoospecial","title":"<code>FluidSimulator</code> (from <code>zoo::special</code>)","text":"<p>A real-time, GPU-accelerated fluid dynamics solver for interactive water, smoke, and fire.</p> <p>Header: <code>#include &lt;xinfer/zoo/special/physics.h&gt;</code></p>"},{"location":"zoo-api/gaming/#use-case-dynamic-interactive-environments","title":"Use Case: Dynamic, Interactive Environments","text":"<p>Create game levels with truly dynamic water that reacts to characters, or realistic smoke and fire that fills rooms and curls around obstacles. This moves beyond static, pre-baked animations.</p> <p><pre><code>#include &lt;xinfer/zoo/special/physics.h&gt;\n#include &lt;xinfer/core/tensor.h&gt;\n\nint main() {\n    // Initialize the simulator once.\n    xinfer::zoo::special::FluidSimulatorConfig config;\n    config.resolution_x = 256;\n    config.resolution_y = 256;\n    xinfer::zoo::special::FluidSimulator simulator(config);\n\n    // Create GPU tensors to hold the fluid state.\n    xinfer::core::Tensor velocity_field, density_field;\n\n    // In the game's physics update loop (the \"tick\").\n    // while (game_is_running) {\n    //     // Add forces/density to the simulation (e.g., from a player walking through water).\n    //     add_player_forces_to_gpu_tensor(velocity_field);\n\n    //     // This single call runs a chain of custom CUDA kernels.\n    //     simulator.step(velocity_field, density_field);\n\n    //     // Render the resulting density_field as smoke or water.\n    //     render_gpu_tensor(density_field);\n    // }\n}\n</code></pre> \"F1 Car\" Technology: This is not a neural network. It is a from-scratch CUDA implementation of a physics solver (like Smoothed-Particle Hydrodynamics). It is orders of magnitude faster than a CPU-based physics engine and more specialized than the engine's built-in rigid body physics.</p>"},{"location":"zoo-api/gaming/#lightbaker-conceptual-tool","title":"<code>LightBaker</code> (Conceptual Tool)","text":"<p>A hyper-optimized tool for pre-calculating complex global illumination and lighting, reducing multi-hour bakes to minutes.</p>"},{"location":"zoo-api/gaming/#use-case-accelerating-artist-iteration","title":"Use Case: Accelerating Artist Iteration","text":"<p>A lighting artist needs to see the results of their changes quickly. Waiting overnight for a light bake is a major workflow bottleneck. This tool provides near-instant feedback.</p> <p><pre><code>// This would be a command-line tool or a plugin button in the editor.\n#include &lt;xinfer/tools/light_baker.h&gt; // Conceptual header\n#include &lt;iostream&gt;\n\nint main(int argc, char** argv) {\n    // 1. Configure the light baker.\n    xinfer::zoo::tools::LightBakerConfig config;\n    config.scene_file = argv; // e.g., \"my_level.gltf\"\n    config.output_path = \"my_level_lightmaps/\";\n    config.quality = \"High\";\n    config.num_bounces = 8;\n\n    // 2. Initialize and run the bake.\n    xinfer::zoo::tools::LightBaker baker(config);\n\n    std::cout &lt;&lt; \"Starting light bake...\\n\";\n    baker.bake();\n    std::cout &lt;&lt; \"Light bake complete. Results saved to \" &lt;&lt; config.output_path &lt;&lt; std.endl;\n\n    return 0;\n}\n</code></pre> \"F1 Car\" Technology: A custom CUDA path tracing engine, written from scratch. It is faster than a game engine's built-in baker because it is specialized only for baking static lightmaps and ignores all the complexity of real-time rendering.</p>"},{"location":"zoo-api/generative/","title":"Zoo API: Generative AI","text":"<p>The <code>xinfer::zoo::generative</code> module provides high-level pipelines for a wide range of creative and generative AI tasks.</p> <p>These classes are built on top of <code>xInfer</code>'s hyper-optimized engines for state-of-the-art generative models like Stable Diffusion, GANs, and more. The <code>zoo</code> API abstracts away the complexity of these models, from multi-stage pipelines to iterative sampling loops, providing simple, powerful tools to bring your creative ideas to life in C++.</p>"},{"location":"zoo-api/generative/#diffusionpipeline","title":"<code>DiffusionPipeline</code>","text":"<p>Generates high-quality images from text prompts using a Stable Diffusion-style model.</p> <p>Header: <code>#include &lt;xinfer/zoo/generative/diffusion_pipeline.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/generative/diffusion_pipeline.h&gt;\n#include &lt;xinfer/utils/image_utils.h&gt; // For saving the final tensor\n#include &lt;iostream&gt;\n\nint main() {\n    // 1. Configure the pipeline.\n    //    The engine would be a pre-built U-Net from a Stable Diffusion model.\n    xinfer::zoo::generative::DiffusionPipelineConfig config;\n    config.unet_engine_path = \"assets/stable_diffusion_unet.engine\";\n    config.num_timesteps = 50; // Number of denoising steps\n\n    // 2. Initialize.\n    xinfer::zoo::generative::DiffusionPipeline pipeline(config);\n\n    // 3. Generate an image.\n    //    The complex, 50-step iterative loop is handled internally in high-performance C++.\n    std::cout &lt;&lt; \"Generating image with diffusion model...\\n\";\n    // A full implementation would also take a text prompt that is processed by a CLIP text encoder.\n    xinfer::core::Tensor image_tensor = pipeline.generate(1);\n\n    // 4. Save the result.\n    xinfer::utils::save_tensor_as_image(image_tensor, \"diffusion_output.png\");\n    std::cout &lt;&lt; \"Image saved to diffusion_output.png\\n\";\n}\n</code></pre> Config Struct: <code>DiffusionPipelineConfig</code> Input: <code>batch_size</code> (and optionally, text prompt embeddings). Output: <code>xinfer::core::Tensor</code> containing the generated image. \"F1 Car\" Technology: The entire iterative denoising loop is a compiled C++ <code>for</code> loop, and each step uses a custom, fused CUDA kernel from <code>postproc::diffusion_sampler</code>, providing a massive speedup over a Python-based loop.</p>"},{"location":"zoo-api/generative/#dcgan","title":"<code>DCGAN</code>","text":"<p>Generates images from a random latent vector using a Generative Adversarial Network.</p> <p>Header: <code>#include &lt;xinfer/zoo/generative/dcgan.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/generative/dcgan.h&gt;\n#include &lt;xinfer/utils/image_utils.h&gt;\n\nint main() {\n    xinfer::zoo::generative::DCGAN_Generator generator(\"assets/dcgan_generator.engine\");\n\n    std::cout &lt;&lt; \"Generating image with DCGAN...\\n\";\n    xinfer::core::Tensor image_tensor = generator.generate(1);\n\n    xinfer::utils::save_tensor_as_image(image_tensor, \"dcgan_output.png\");\n    std::cout &lt;&lt; \"Image saved to dcgan_output.png\\n\";\n}\n</code></pre> Input: <code>batch_size</code>. Output: <code>xinfer::core::Tensor</code> containing the generated image.</p>"},{"location":"zoo-api/generative/#styletransfer","title":"<code>StyleTransfer</code>","text":"<p>Applies the artistic style of one image to the content of another.</p> <p>Header: <code>#include &lt;xinfer/zoo/generative/style_transfer.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/generative/style_transfer.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n\nint main() {\n    // The engine is pre-built from a model trained on a specific style (e.g., \"Starry Night\").\n    xinfer::zoo::generative::StyleTransferConfig config;\n    config.engine_path = \"assets/starry_night_style.engine\";\n\n    xinfer::zoo::generative::StyleTransfer stylizer(config);\n\n    cv::Mat content_image = cv::imread(\"assets/my_photo.jpg\");\n    cv::Mat styled_image = stylizer.predict(content_image);\n\n    cv::imwrite(\"styled_output.jpg\", styled_image);\n    std::cout &lt;&lt; \"Saved styled image to styled_output.jpg\\n\";\n}\n</code></pre> Config Struct: <code>StyleTransferConfig</code> Input: <code>cv::Mat</code> content image. Output: <code>cv::Mat</code> stylized image.</p>"},{"location":"zoo-api/generative/#superresolution","title":"<code>SuperResolution</code>","text":"<p>Upscales a low-resolution image to a high-resolution version, adding realistic detail.</p> <p>Header: <code>#include &lt;xinfer/zoo/generative/super_resolution.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/generative/super_resolution.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n\nint main() {\n    xinfer::zoo::generative::SuperResolutionConfig config;\n    config.engine_path = \"assets/esrgan_x4.engine\";\n    config.upscale_factor = 4;\n\n    xinfer::zoo::generative::SuperResolution upscaler(config);\n\n    cv::Mat low_res_image = cv::imread(\"assets/low_res.png\");\n    cv::Mat high_res_image = upscaler.predict(low_res_image);\n\n    cv::imwrite(\"super_resolution_output.png\", high_res_image);\n}\n</code></pre> Config Struct: <code>SuperResolutionConfig</code> Input: <code>cv::Mat</code> low-resolution image. Output: <code>cv::Mat</code> high-resolution image.</p>"},{"location":"zoo-api/generative/#and-more","title":"And More...","text":"<p>This module provides many more specialized generative pipelines.</p> <ul> <li><code>Inpainter</code>: Fills in a masked region of an image with plausible content.</li> <li><code>Outpainter</code>: Extends the boundaries of an image with generated content.</li> <li><code>Colorizer</code>: Adds realistic color to a black-and-white image.</li> <li><code>TextToSpeech</code>: Converts a string of text into a spoken audio waveform.</li> <li><code>ImageToVideo</code>: Generates a short video clip from a starting image.</li> <li><code>VideoFrameInterpolation</code>: Creates smooth, slow-motion video by generating intermediate frames.</li> </ul> <p>Each of these would have its own section with a code example, just like the ones above.</p>"},{"location":"zoo-api/geospatial/","title":"Zoo API: Geospatial","text":"<p>The <code>xinfer::zoo::geospatial</code> module provides a suite of high-performance pipelines for analyzing geospatial imagery from satellites, airplanes, and drones.</p> <p>Processing massive, multi-channel satellite images is a significant computational challenge. The <code>zoo</code> classes in this module are built on <code>xInfer</code>'s hyper-optimized C++/TensorRT core to enable rapid, large-scale analysis that is often impractical with standard Python-based frameworks.</p>"},{"location":"zoo-api/geospatial/#buildingsegmenter","title":"<code>BuildingSegmenter</code>","text":"<p>Performs semantic segmentation on satellite imagery to extract the footprints of buildings.</p> <p>Header: <code>#include &lt;xinfer/zoo/geospatial/building_segmenter.h&gt;</code></p>"},{"location":"zoo-api/geospatial/#use-case-urban-planning-and-insurance-risk-assessment","title":"Use Case: Urban Planning and Insurance Risk Assessment","text":"<p>An urban planner needs to create a map of all buildings in a city, or an insurance company needs to assess the number of properties in a high-risk flood zone.</p> <p><pre><code>#include &lt;xinfer/zoo/geospatial/building_segmenter.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    // 1. Configure the building segmenter.\n    xinfer::zoo::geospatial::BuildingSegmenterConfig config;\n    config.engine_path = \"assets/building_segmenter.engine\";\n    config.probability_threshold = 0.5f;\n\n    // 2. Initialize.\n    xinfer::zoo::geospatial::BuildingSegmenter segmenter(config);\n\n    // 3. Load a large satellite or aerial image.\n    cv::Mat satellite_image = cv::imread(\"assets/city_orthomosaic.tif\");\n\n    // 4. Predict the binary mask of all buildings.\n    cv::Mat building_mask = segmenter.predict_mask(satellite_image);\n\n    // 5. (Optional) Convert the mask to vector polygons for use in GIS software.\n    std::vector&lt;xinfer::zoo::geospatial::BuildingPolygon&gt; polygons = segmenter.predict_polygons(satellite_image);\n\n    std::cout &lt;&lt; \"Found \" &lt;&lt; polygons.size() &lt;&lt; \" building footprints.\\n\";\n    cv::imwrite(\"building_footprints.png\", building_mask);\n}\n</code></pre> Config Struct: <code>BuildingSegmenterConfig</code> Methods: - <code>predict_mask(const cv::Mat&amp;)</code> returns a <code>cv::Mat</code> binary mask. - <code>predict_polygons(const cv::Mat&amp;)</code> returns a <code>std::vector&lt;BuildingPolygon&gt;</code>.   \"F1 Car\" Technology: This class internally tiles the large input image, runs a high-performance U-Net engine on each tile, and stitches the results back together. The polygonization step uses efficient OpenCV algorithms.</p>"},{"location":"zoo-api/geospatial/#roadextractor","title":"<code>RoadExtractor</code>","text":"<p>Performs semantic segmentation to extract the road network from satellite imagery.</p> <p>Header: <code>#include &lt;xinfer/zoo/geospatial/road_extractor.h&gt;</code></p>"},{"location":"zoo-api/geospatial/#use-case-logistics-and-infrastructure-mapping","title":"Use Case: Logistics and Infrastructure Mapping","text":"<p>A logistics company wants to create an up-to-date road map for a remote area, or a government agency needs to assess road infrastructure.</p> <p><pre><code>#include &lt;xinfer/zoo/geospatial/road_extractor.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    xinfer::zoo::geospatial::RoadExtractorConfig config;\n    config.engine_path = \"assets/road_extractor.engine\";\n\n    xinfer::zoo::geospatial::RoadExtractor extractor(config);\n\n    cv::Mat satellite_image = cv::imread(\"assets/rural_area.tif\");\n    cv::Mat road_mask = extractor.predict_mask(satellite_image);\n\n    // The mask can be further processed into a graph-based road network.\n    std::cout &lt;&lt; \"Road network mask generated.\\n\";\n    cv::imwrite(\"road_network.png\", road_mask);\n}\n</code></pre> Config Struct: <code>RoadExtractorConfig</code> Methods: - <code>predict_mask(const cv::Mat&amp;)</code> returns a <code>cv::Mat</code> binary mask. - <code>predict_segments(const cv::Mat&amp;)</code> returns a <code>std::vector&lt;RoadSegment&gt;</code> (contours).</p>"},{"location":"zoo-api/geospatial/#maritimedetector","title":"<code>MaritimeDetector</code>","text":"<p>Detects and classifies maritime objects (ships, boats) in satellite or aerial imagery.</p> <p>Header: <code>#include &lt;xinfer/zoo/geospatial/maritime_detector.h&gt;</code></p>"},{"location":"zoo-api/geospatial/#use-case-port-security-and-maritime-surveillance","title":"Use Case: Port Security and Maritime Surveillance","text":"<p>A coast guard or port authority needs to monitor maritime traffic and identify specific vessels in a large area of open water or a crowded port.</p> <p><pre><code>#include &lt;xinfer/zoo/geospatial/maritime_detector.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    xinfer::zoo::geospatial::MaritimeDetectorConfig config;\n    config.engine_path = \"assets/ship_detector.engine\";\n    config.labels_path = \"assets/ship_classes.txt\"; // e.g., \"Cargo Ship\", \"Tanker\", \"Fishing Vessel\"\n\n    xinfer::zoo::geospatial::MaritimeDetector detector(config);\n\n    cv::Mat coastal_image = cv::imread(\"assets/port_image.jpg\");\n    auto detections = detector.predict(coastal_image);\n\n    std::cout &lt;&lt; \"Detected \" &lt;&lt; detections.size() &lt;&lt; \" maritime objects:\\n\";\n    for (const auto&amp; obj : detections) {\n        // Draw the rotated bounding box\n        cv::polylines(coastal_image, obj.contour, true, {0, 255, 0}, 2);\n        std::cout &lt;&lt; \" - \" &lt;&lt; obj.label &lt;&lt; \" (Confidence: \" &lt;&lt; obj.confidence &lt;&lt; \")\\n\";\n    }\n    cv::imwrite(\"maritime_detections.jpg\", coastal_image);\n}\n</code></pre> Config Struct: <code>MaritimeDetectorConfig</code> Input: <code>cv::Mat</code> satellite/aerial image. Output Struct: <code>DetectedObject</code> (contains class, confidence, and a contour for rotated bounding boxes).</p>"},{"location":"zoo-api/geospatial/#disasterassessor","title":"<code>DisasterAssessor</code>","text":"<p>Compares pre- and post-disaster satellite images to automatically map and assess the extent of damage.</p> <p>Header: <code>#include &lt;xinfer/zoo/geospatial/disaster_assessor.h&gt;</code></p>"},{"location":"zoo-api/geospatial/#use-case-emergency-response-and-insurance-assessment","title":"Use Case: Emergency Response and Insurance Assessment","text":"<p>After a hurricane or wildfire, emergency response teams and insurance companies need to rapidly determine which buildings have been damaged or destroyed.</p> <p><pre><code>#include &lt;xinfer/zoo/geospatial/disaster_assessor.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    xinfer::zoo::geospatial::DisasterAssessorConfig config;\n    config.engine_path = \"assets/damage_assessor.engine\";\n\n    xinfer::zoo::geospatial::DisasterAssessor assessor(config);\n\n    cv::Mat image_before = cv::imread(\"assets/town_before_hurricane.png\");\n    cv::Mat image_after = cv::imread(\"assets/town_after_hurricane.png\");\n\n    // The model takes both images as input and identifies the changes.\n    cv::Mat damage_mask = assessor.predict(image_before, image_after);\n\n    std::cout &lt;&lt; \"Damage assessment mask generated.\\n\";\n    cv::imwrite(\"damage_mask.png\", damage_mask);\n}\n</code></pre> Config Struct: <code>DisasterAssessorConfig</code> Input: Two <code>cv::Mat</code> images (pre- and post-disaster). Output: A <code>cv::Mat</code> binary mask where white pixels indicate damaged areas.</p>"},{"location":"zoo-api/geospatial/#cropmonitor","title":"<code>CropMonitor</code>","text":"<p>Analyzes multi-spectral satellite imagery to assess crop health and calculate common agricultural indices.</p> <p>Header: <code>#include &lt;xinfer/zoo/geospatial/crop_monitor.h&gt;</code></p>"},{"location":"zoo-api/geospatial/#use-case-precision-agriculture","title":"Use Case: Precision Agriculture","text":"<p>A large farm or agricultural cooperative needs to monitor the health of thousands of acres of crops to apply fertilizer and water more efficiently.</p> <p><pre><code>#include &lt;xinfer/zoo/geospatial/crop_monitor.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    // In a real app, this would be a multi-channel GeoTIFF file\n    cv::Mat multispectral_image; // Assume this is loaded with B, G, R, NIR channels\n\n    // You can perform classic analysis without a model:\n    cv::Mat ndvi_map = xinfer::zoo::geospatial::CropMonitor::calculate_ndvi(multispectral_image);\n    cv::imwrite(\"ndvi_map.png\", ndvi_map);\n    std::cout &lt;&lt; \"NDVI map calculated and saved.\\n\";\n\n    // Or use a trained AI model for a more advanced health prediction:\n    xinfer::zoo::geospatial::CropMonitorConfig config;\n    config.engine_path = \"assets/crop_health_model.engine\";\n    xinfer::zoo::geospatial::CropMonitor monitor(config);\n    cv::Mat health_map = monitor.predict_health_map(multispectral_image);\n    cv::imwrite(\"health_map.png\", health_map);\n    std::cout &lt;&lt; \"AI-based health map generated and saved.\\n\";\n}\n</code></pre> Config Struct: <code>CropMonitorConfig</code> Methods: - <code>predict_health_map(const cv::Mat&amp;)</code> returns a <code>cv::Mat</code> health score map. - <code>calculate_ndvi(const cv::Mat&amp;)</code> is a static utility function that returns a <code>cv::Mat</code> NDVI map.</p>"},{"location":"zoo-api/hft/","title":"Zoo API: High-Frequency Trading (HFT)","text":"<p>The <code>xinfer::zoo::hft</code> module provides an ultra-low-latency, \"zero-overhead\" solution for executing AI-driven trading strategies.</p> <p>This is the most specialized module in the <code>xInfer</code> ecosystem. It is designed for a single purpose: to provide the fastest possible path from market data to a trading decision. In the world of HFT, performance is not measured in milliseconds; it is measured in microseconds and nanoseconds.</p> <p>This module is not a general-purpose trading framework. It is a set of hyper-optimized inference engines designed to be integrated into a sophisticated, co-located C++ trading system.</p> <p>Extreme Performance, Expert Use</p> <p>The classes in this module make critical trade-offs for speed. They bypass many standard software abstractions and assume they are being run in a dedicated, real-time environment. They are intended for expert users with a deep understanding of low-latency systems.</p>"},{"location":"zoo-api/hft/#orderexecutionpolicy","title":"<code>OrderExecutionPolicy</code>","text":"<p>Executes a trained Reinforcement Learning (RL) policy for optimal trade execution.</p> <p>Header: <code>#include &lt;xinfer/zoo/hft/order_execution_policy.h&gt;</code></p>"},{"location":"zoo-api/hft/#use-case-minimizing-market-impact","title":"Use Case: Minimizing Market Impact","text":"<p>A major challenge in algorithmic trading is executing a large order (e.g., selling 1 million shares of a stock) without causing the price to move against you. An RL agent can be trained in a market simulator to learn an optimal \"scheduling\" policy\u2014breaking the large order into many small \"child\" orders and placing them over time to minimize impact.</p> <p>The <code>OrderExecutionPolicy</code> is the hyper-optimized engine for running this learned policy in a live market.</p> <p><pre><code>#include &lt;xinfer/zoo/hft/order_execution_policy.h&gt;\n#include &lt;xinfer/core/tensor.h&gt;\n#include &lt;iostream&gt;\n\n// This function would be in the critical path of a C++ trading application's event loop.\n// It would be triggered by a new market data update.\nvoid on_market_tick(xinfer::zoo::hft::OrderExecutionPolicy&amp; policy, TradingSystem&amp; system) {\n    // 1. Get the current market state directly into a pre-allocated GPU tensor.\n    //    In a real HFT system, this would use kernel-bypass networking to get\n    //    data to the GPU with the lowest possible latency.\n    xinfer::core::Tensor market_state_tensor = system.get_current_market_state_gpu();\n\n    // 2. Execute the policy. This is a single, deterministic, microsecond-scale call.\n    xinfer::zoo::hft::OrderExecutionAction action = policy.predict(market_state_tensor);\n\n    // 3. Act on the decision immediately.\n    if (action.action == xinfer::zoo::hft::OrderActionType::PLACE_SELL) {\n        system.execute_sell_order(action.volume, action.price_level);\n    }\n}\n\nint main() {\n    // 1. Configure and load the pre-compiled policy engine during application startup.\n    xinfer::zoo::hft::OrderExecutionPolicyConfig config;\n    config.engine_path = \"assets/vwap_execution_policy.engine\";\n\n    xinfer::zoo::hft::OrderExecutionPolicy policy(config);\n    std::cout &lt;&lt; \"HFT Order Execution Policy loaded and ready.\\n\";\n\n    // TradingSystem system;\n    // system.connect_to_exchange();\n\n    // 2. Run in a tight, pinned-thread event loop.\n    // while (system.is_market_open()) {\n    //     if (system.has_new_market_data()) {\n    //         on_market_tick(policy, system);\n    //     }\n    // }\n\n    return 0;\n}\n</code></pre> Config Struct: <code>OrderExecutionPolicyConfig</code> Input: <code>xinfer::core::Tensor</code> representing the current state of the limit order book and other market features. Output Struct: <code>OrderExecutionAction</code> (an enum for BUY/SELL/HOLD, plus volume and price level). \"F1 Car\" Technology: This class wraps a TensorRT engine that has been built from a very small, simple MLP. The key is that the entire <code>xInfer</code> runtime has minimal overhead, allowing it to be called in a hard real-time loop where every nanosecond of jitter matters.</p>"},{"location":"zoo-api/hft/#marketdataparser","title":"<code>MarketDataParser</code>","text":"<p>This is not a model, but a hyper-performant utility. It's a custom CUDA kernel for parsing raw, high-frequency financial data feeds.</p> <p>Header: <code>#include &lt;xinfer/zoo/special/hft.h&gt;</code> (Note: For simplicity, this could live in the same <code>hft.h</code> header)</p>"},{"location":"zoo-api/hft/#use-case-bypassing-the-cpu-for-market-data","title":"Use Case: Bypassing the CPU for Market Data","text":"<p>The first bottleneck in any HFT system is parsing the stream of raw binary data from the exchange. Doing this on the CPU is slow. This <code>MarketDataParser</code> is designed to work with network cards that support GPU Direct, allowing network packets to be streamed directly into GPU memory.</p> <p><pre><code>#include &lt;xinfer/zoo/special/hft.h&gt;\n#include &lt;xinfer/core/tensor.h&gt;\n\nint main() {\n    // 1. Initialize the parser. This loads the custom CUDA kernel.\n    xinfer::zoo::hft::MarketDataParser parser;\n\n    // 2. In the network receive loop:\n    //    - A raw network buffer is received directly into GPU memory.\n    void* d_raw_packet_buffer = get_packet_from_network_card_gpu();\n    size_t packet_size = 1024; // in bytes\n\n    //    - A GPU tensor is pre-allocated to hold the structured order book.\n    xinfer::core::Tensor market_state_tensor({1, 20, 2}, xinfer::core::DataType::kFLOAT); // e.g. 20 levels of bids/asks\n\n    // 3. Launch the CUDA kernel to parse the raw data and update the state tensor.\n    //    This is a single, fast kernel launch.\n    parser.parse_and_update(d_raw_packet_buffer, packet_size, market_state_tensor);\n\n    // `market_state_tensor` is now ready to be fed into a policy engine.\n\n    return 0;\n}\n</code></pre> \"F1 Car\" Technology: This is the ultimate \"F1 car\" component. It is a from-scratch CUDA kernel that implements a parser for a specific financial protocol (like FIX/FAST or a proprietary exchange protocol). By doing this on the GPU, you completely eliminate the CPU from the critical path, which is a massive competitive advantage.</p>"},{"location":"zoo-api/medical/","title":"Zoo API: Medical Imaging","text":"<p>The <code>xinfer::zoo::medical</code> module provides a suite of high-performance, specialized pipelines for medical image analysis.</p> <p>Developing AI for healthcare requires the highest standards of performance, reliability, and precision. The classes in this module are designed to be integrated into clinical research, diagnostic workflows, and medical devices. They are built on top of <code>xInfer</code>'s hyper-optimized C++/TensorRT core to provide the low-latency, high-throughput processing that is essential for modern medical applications.</p> <p>For Research Use Only</p> <p>The <code>zoo::medical</code> pipelines are powerful tools for research and development. They are not certified as medical devices. Any clinical or diagnostic application built with <code>xInfer</code> must undergo its own separate and rigorous regulatory approval process (e.g., FDA, CE).</p>"},{"location":"zoo-api/medical/#tumordetector","title":"<code>TumorDetector</code>","text":"<p>Performs 3D object detection on medical scans (like CT or MRI) to identify and localize potential tumors.</p> <p>Header: <code>#include &lt;xinfer/zoo/medical/tumor_detector.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/medical/tumor_detector.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\nint main() {\n    // 1. Configure the 3D tumor detector.\n    xinfer::zoo::medical::TumorDetectorConfig config;\n    config.engine_path = \"assets/lung_nodule_detector_3d.engine\";\n    config.labels_path = \"assets/nodule_types.txt\";\n\n    // 2. Initialize.\n    xinfer::zoo::medical::TumorDetector detector(config);\n\n    // 3. Load a series of 2D CT scan slices that form a 3D volume.\n    std::vector&lt;cv::Mat&gt; ct_scan_slices;\n    // ... load slices from DICOM files ...\n\n    // 4. Predict 3D bounding boxes for all potential tumors.\n    std::vector&lt;xinfer::zoo::medical::Tumor&gt; tumors = detector.predict(ct_scan_slices);\n\n    // 5. Print results.\n    std::cout &lt;&lt; \"Found \" &lt;&lt; tumors.size() &lt;&lt; \" potential tumors:\\n\";\n    for (const auto&amp; tumor : tumors) {\n        std::cout &lt;&lt; \" - Type: \" &lt;&lt; tumor.label &lt;&lt; \", Confidence: \" &lt;&lt; tumor.confidence &lt;&lt; \"\\n\";\n    }\n}\n</code></pre> Config Struct: <code>TumorDetectorConfig</code> Input: <code>std::vector&lt;cv::Mat&gt;</code> representing the 3D scan volume. Output Struct: <code>Tumor</code> (contains 3D bounding box, class, and confidence). \"F1 Car\" Technology: This pipeline is built to run a 3D CNN (like a 3D U-Net) and would use <code>xInfer</code>'s planned 3D NMS post-processing kernel for maximum performance.</p>"},{"location":"zoo-api/medical/#cellsegmenter","title":"<code>CellSegmenter</code>","text":"<p>Performs instance segmentation on microscope images to identify and count individual cells.</p> <p>Header: <code>#include &lt;xinfer/zoo/medical/cell_segmenter.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/medical/cell_segmenter.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    // 1. Configure the cell segmenter.\n    xinfer::zoo::medical::CellSegmenterConfig config;\n    config.engine_path = \"assets/cell_unet.engine\";\n    config.probability_threshold = 0.5f;\n\n    // 2. Initialize.\n    xinfer::zoo::medical::CellSegmenter segmenter(config);\n\n    // 3. Process a microscope image.\n    cv::Mat microscope_image = cv::imread(\"assets/blood_smear.png\");\n    auto result = segmenter.predict(microscope_image);\n\n    // 4. Print the result and save the instance mask.\n    std::cout &lt;&lt; \"Detected \" &lt;&lt; result.cell_count &lt;&lt; \" individual cells.\\n\";\n    // result.instance_mask is a CV_32S Mat where each cell has a unique integer ID.\n    // (This can be colorized for visualization).\n    cv::imwrite(\"cell_instance_mask.png\", result.instance_mask);\n}\n</code></pre> Config Struct: <code>CellSegmenterConfig</code> Input: <code>cv::Mat</code> microscope image. Output Struct: <code>CellSegmentationResult</code> (contains an instance mask, cell count, and contours).</p>"},{"location":"zoo-api/medical/#retinascanner","title":"<code>RetinaScanner</code>","text":"<p>Analyzes retinal fundus images to detect and grade signs of diabetic retinopathy.</p> <p>Header: <code>#include &lt;xinfer/zoo/medical/retina_scanner.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/medical/retina_scanner.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    xinfer::zoo::medical::RetinaScannerConfig config;\n    config.engine_path = \"assets/retinopathy_classifier.engine\";\n\n    xinfer::zoo::medical::RetinaScanner scanner(config);\n\n    cv::Mat fundus_image = cv::imread(\"assets/retina_scan.jpg\");\n    auto result = scanner.predict(fundus_image);\n\n    std::cout &lt;&lt; \"Retina Scan Analysis:\\n\";\n    std::cout &lt;&lt; \" - Diagnosis: \" &lt;&lt; result.diagnosis\n              &lt;&lt; \" (Grade \" &lt;&lt; result.severity_grade &lt;&lt; \")\\n\"\n              &lt;&lt; \" - Confidence: \" &lt;&lt; result.confidence &lt;&lt; \"\\n\";\n\n    // The heatmap can be overlaid on the original image to show areas the AI focused on.\n    cv::imwrite(\"lesion_heatmap.png\", result.lesion_heatmap);\n}\n</code></pre> Config Struct: <code>RetinaScannerConfig</code> Input: <code>cv::Mat</code> fundus image. Output Struct: <code>RetinaScanResult</code> (contains diagnosis, severity grade, confidence, and a <code>cv::Mat</code> heatmap).</p>"},{"location":"zoo-api/medical/#ultrasoundguide","title":"<code>UltrasoundGuide</code>","text":"<p>Performs real-time segmentation on ultrasound video feeds to assist medical professionals.</p> <p>Header: <code>#include &lt;xinfer/zoo/medical/ultrasound_guide.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/medical/ultrasound_guide.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n\nint main() {\n    xinfer::zoo::medical::UltrasoundGuideConfig config;\n    config.engine_path = \"assets/ultrasound_nerve_segmenter.engine\";\n\n    xinfer::zoo::medical::UltrasoundGuide guide(config);\n\n    cv::VideoCapture cap(0); // Open a live camera/ultrasound feed\n    cv::Mat frame;\n    while (cap.read(frame)) {\n        // Run the segmentation pipeline in real-time\n        auto result = guide.predict(frame);\n\n        // Overlay the segmentation mask on the live feed for guidance\n        cv::addWeighted(frame, 1.0, result.segmentation_mask, 0.4, 0.0, frame);\n        cv::imshow(\"Ultrasound Guidance\", frame);\n        if (cv::waitKey(1) == 27) break; // Exit on ESC\n    }\n}\n</code></pre> Config Struct: <code>UltrasoundGuideConfig</code> Input: <code>cv::Mat</code> ultrasound video frame. Output Struct: <code>UltrasoundGuideResult</code> (contains a binary segmentation mask and contours).</p>"},{"location":"zoo-api/medical/#pathologyassistant","title":"<code>PathologyAssistant</code>","text":"<p>Analyzes gigapixel whole-slide images to detect and quantify mitotic activity, assisting pathologists in cancer grading.</p> <p>Header: <code>#include &lt;xinfer/zoo/medical/pathology_assistant.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/medical/pathology_assistant.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    xinfer::zoo::medical::PathologyAssistantConfig config;\n    config.engine_path = \"assets/mitosis_detector.engine\";\n    config.batch_size = 16; // Process 16 tiles at a time for high throughput\n\n    xinfer::zoo::medical::PathologyAssistant assistant(config);\n\n    // In a real app, this would use a library like OpenSlide to read a gigapixel .svs file\n    cv::Mat whole_slide_image = cv::imread(\"assets/pathology_slide_large.tif\");\n\n    auto result = assistant.predict(whole_slide_image);\n\n    std::cout &lt;&lt; \"Pathology Slide Analysis:\\n\";\n    std::cout &lt;&lt; \" - Overall Mitotic Score: \" &lt;&lt; result.overall_mitotic_score &lt;&lt; std::endl;\n\n    cv::imwrite(\"mitotic_heatmap.png\", result.mitotic_heatmap);\n}\n</code></pre> Config Struct: <code>PathologyAssistantConfig</code> Input: <code>cv::Mat</code> representing a whole-slide image. Output Struct: <code>PathologyResult</code> (contains a summary score and a <code>cv::Mat</code> heatmap). \"F1 Car\" Technology: This class internally handles the complex logic of tiling a massive image, running inference on batches of tiles, and stitching the results back together into a final heatmap.</p>"},{"location":"zoo-api/nlp/","title":"Zoo API: Natural Language Processing (NLP)","text":"<p>The <code>xinfer::zoo::nlp</code> module provides a suite of high-performance pipelines for common Natural Language Processing tasks.</p> <p>These classes are built on top of hyper-optimized TensorRT engines for state-of-the-art Transformer and sequence models. They are designed to bring high-throughput, low-latency language understanding to your native C++ applications, enabling tasks from real-time sentiment analysis to complex document processing.</p> <p>A core component for these pipelines is a tokenizer, which converts raw text into integer IDs that the models can understand. <code>xInfer</code> assumes you will manage tokenization using a library like SentencePiece or Hugging Face Tokenizers C++, and the <code>zoo</code> classes will take these token IDs as input.</p>"},{"location":"zoo-api/nlp/#classifier","title":"<code>Classifier</code>","text":"<p>Performs text classification. Given a piece of text, it assigns it to a pre-defined category (e.g., sentiment, topic, intent).</p> <p>Header: <code>#include &lt;xinfer/zoo/nlp/classifier.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/nlp/classifier.h&gt;\n#include &lt;iostream&gt;\n#include &lt;string&gt;\n#include &lt;vector&gt;\n\nint main() {\n    // 1. Configure the text classifier.\n    //    The engine would be a pre-built BERT or DistilBERT model.\n    xinfer::zoo::nlp::ClassifierConfig config;\n    config.engine_path = \"assets/sentiment_bert.engine\";\n    config.labels_path = \"assets/sentiment_labels.txt\"; // e.g., \"negative\", \"positive\"\n    config.vocab_path = \"assets/bert_vocab.txt\"; // For the internal tokenizer\n\n    // 2. Initialize.\n    xinfer::zoo::nlp::Classifier classifier(config);\n\n    // 3. Predict the sentiment of a sentence.\n    std::string text = \"xInfer is an incredibly fast and easy-to-use library!\";\n    auto results = classifier.predict(text, 2); // Get top 2 results\n\n    // 4. Print the results.\n    std::cout &lt;&lt; \"Sentiment analysis for: \\\"\" &lt;&lt; text &lt;&lt; \"\\\"\\n\";\n    for (const auto&amp; result : results) {\n        printf(\" - Label: %-10s, Confidence: %.4f\\n\", result.label.c_str(), result.confidence);\n    }\n}```\n**Config Struct:** `ClassifierConfig`\n**Input:** `std::string` of text.\n**Output Struct:** `TextClassificationResult` (contains class ID, label, and confidence).\n\n---\n\n## `Embedder`\n\nConverts a piece of text into a fixed-size, high-dimensional vector (an \"embedding\") that captures its semantic meaning. This is the backbone of modern semantic search and RAG systems.\n\n**Header:** `#include &lt;xinfer/zoo/nlp/embedder.h&gt;`\n\n```cpp\n#include &lt;xinfer/zoo/nlp/embedder.h&gt;\n#include &lt;iostream&gt;\n#include &lt;string&gt;\n#include &lt;vector&gt;\n\nint main() {\n    // 1. Configure the embedder.\n    //    The engine would be a pre-built Sentence-BERT model.\n    xinfer::zoo::nlp::EmbedderConfig config;\n    config.engine_path = \"assets/sentence_bert.engine\";\n    config.vocab_path = \"assets/bert_vocab.txt\";\n\n    // 2. Initialize.\n    xinfer::zoo::nlp::Embedder embedder(config);\n\n    // 3. Create embeddings for a list of sentences.\n    std::vector&lt;std::string&gt; texts = {\n        \"The cat sat on the mat.\",\n        \"A feline was resting on the rug.\"\n    };\n    std::vector&lt;xinfer::zoo::nlp::TextEmbedding&gt; embeddings = embedder.predict_batch(texts);\n\n    // 4. Compare the embeddings using cosine similarity.\n    float similarity = xinfer::zoo::nlp::Embedder::compare(embeddings, embeddings);\n\n    std::cout &lt;&lt; \"Semantic similarity between the two sentences: \" &lt;&lt; similarity &lt;&lt; std::endl;\n}\n</code></pre> Config Struct: <code>EmbedderConfig</code> Input: <code>std::string</code> or <code>std::vector&lt;std::string&gt;</code>. Output: <code>TextEmbedding</code> (a <code>std::vector&lt;float&gt;</code>).</p>"},{"location":"zoo-api/nlp/#ner-named-entity-recognition","title":"<code>NER</code> (Named Entity Recognition)","text":"<p>Scans a piece of text and extracts named entities like people, organizations, and locations.</p> <p>Header: <code>#include &lt;xinfer/zoo/nlp/ner.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/nlp/ner.h&gt;\n#include &lt;iostream&gt;\n#include &lt;string&gt;\n\nint main() {\n    // 1. Configure the NER pipeline.\n    xinfer::zoo::nlp::NERConfig config;\n    config.engine_path = \"assets/ner_bert.engine\";\n    config.labels_path = \"assets/ner_labels.txt\"; // e.g., \"B-PER\", \"I-PER\", \"B-ORG\"\n    config.vocab_path = \"assets/bert_vocab.txt\";\n\n    // 2. Initialize.\n    xinfer::zoo::nlp::NER ner_pipeline(config);\n\n    // 3. Predict.\n    std::string text = \"Apple Inc. was founded by Steve Jobs in Cupertino.\";\n    auto entities = ner_pipeline.predict(text);\n\n    // 4. Print the extracted entities.\n    std::cout &lt;&lt; \"Found \" &lt;&lt; entities.size() &lt;&lt; \" entities:\\n\";\n    for (const auto&amp; entity : entities) {\n        std::cout &lt;&lt; \" - Text: \\\"\" &lt;&lt; entity.text &lt;&lt; \"\\\", Label: \" &lt;&lt; entity.label &lt;&lt; \"\\n\";\n    }\n}\n</code></pre> Config Struct: <code>NERConfig</code> Input: <code>std::string</code>. Output Struct: <code>NamedEntity</code> (contains the text, label, score, and position).</p>"},{"location":"zoo-api/nlp/#questionanswering","title":"<code>QuestionAnswering</code>","text":"<p>Finds the answer to a question within a given context paragraph.</p> <p>Header: <code>#include &lt;xinfer/zoo/nlp/question_answering.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/nlp/question_answering.h&gt;\n#include &lt;iostream&gt;\n#include &lt;string&gt;\n\nint main() {\n    xinfer::zoo::nlp::QAConfig config;\n    config.engine_path = \"assets/qa_bert.engine\";\n    config.vocab_path = \"assets/bert_vocab.txt\";\n\n    xinfer::zoo::nlp::QuestionAnswering qa_pipeline(config);\n\n    std::string context = \"xInfer is a C++ library designed for high-performance inference. It uses NVIDIA TensorRT to optimize models.\";\n    std::string question = \"What technology does xInfer use?\";\n\n    auto result = qa_pipeline.predict(question, context);\n\n    std::cout &lt;&lt; \"Question: \" &lt;&lt; question &lt;&lt; \"\\n\";\n    std::cout &lt;&lt; \"Answer: \" &lt;&lt; result.answer &lt;&lt; \" (Score: \" &lt;&lt; result.score &lt;&lt; \")\\n\";\n}\n</code></pre> Config Struct: <code>QAConfig</code> Input: <code>std::string</code> for question and <code>std::string</code> for context. Output Struct: <code>QAResult</code> (contains the answer text, score, and position).</p>"},{"location":"zoo-api/nlp/#textgenerator-codegenerator","title":"<code>TextGenerator</code> / <code>CodeGenerator</code>","text":"<p>Provides an interface for running generative Large Language Models (LLMs) for text or code completion.</p> <p>Header: <code>#include &lt;xinfer/zoo/nlp/text_generator.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/nlp/text_generator.h&gt;\n#include &lt;iostream&gt;\n#include &lt;string&gt;\n\nint main() {\n    xinfer::zoo::nlp::TextGeneratorConfig config;\n    config.engine_path = \"assets/llama3_8b.engine\";\n    config.vocab_path = \"assets/llama_vocab.json\";\n    config.max_new_tokens = 100;\n\n    xinfer::zoo::nlp::TextGenerator generator(config);\n\n    std::string prompt = \"xInfer is a C++ library that enables \";\n\n    std::cout &lt;&lt; \"Prompt: \" &lt;&lt; prompt;\n    // The streaming function calls the lambda for each new piece of text generated.\n    generator.predict_stream(prompt, [](const std::string&amp; token_str) {\n        std::cout &lt;&lt; token_str &lt;&lt; std::flush;\n    });\n    std::cout &lt;&lt; std::endl;\n}\n</code></pre> Config Struct: <code>TextGeneratorConfig</code> / <code>CodeGeneratorConfig</code> Methods: <code>.predict()</code> for a single string, and <code>.predict_stream()</code> for real-time, token-by-token streaming.</p>"},{"location":"zoo-api/retail/","title":"Zoo API: Retail","text":"<p>The <code>xinfer::zoo::retail</code> module provides a suite of high-performance pipelines specifically designed for the unique challenges of the retail industry.</p> <p>In retail, efficiency is everything. From optimizing the supply chain to understanding customer behavior in real-time, the ability to process vast amounts of data quickly and cost-effectively is a critical competitive advantage. The <code>zoo</code> classes in this module are end-to-end solutions that leverage <code>xInfer</code>'s hyper-optimized engines to automate core retail operations.</p>"},{"location":"zoo-api/retail/#shelfauditor","title":"<code>ShelfAuditor</code>","text":"<p>Provides a complete solution for automated shelf monitoring. It uses a high-performance object detection model to audit product availability and placement on store shelves.</p> <p>Header: <code>#include &lt;xinfer/zoo/retail/shelf_auditor.h&gt;</code></p>"},{"location":"zoo-api/retail/#use-case-real-time-out-of-stock-detection","title":"Use Case: Real-Time Out-of-Stock Detection","text":"<p>A store manager or a robot can capture an image of an aisle. The <code>ShelfAuditor</code> instantly processes this image to identify which products are missing or misplaced, creating an actionable alert for store associates.</p> <p><pre><code>#include &lt;xinfer/zoo/retail/shelf_auditor.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    // 1. Configure the auditor.\n    //    The engine would be a YOLO model trained on specific product SKUs.\n    xinfer::zoo::retail::ShelfAuditorConfig config;\n    config.engine_path = \"assets/product_detector.engine\";\n    config.labels_path = \"assets/product_skus.txt\";\n    config.confidence_threshold = 0.7f;\n\n    // 2. Initialize.\n    xinfer::zoo::retail::ShelfAuditor auditor(config);\n\n    // 3. Process an image of a store shelf.\n    cv::Mat shelf_image = cv::imread(\"assets/aisle_4_snapshot.jpg\");\n    std::vector&lt;xinfer::zoo::retail::ShelfItem&gt; results = auditor.audit(shelf_image);\n\n    // 4. Print the inventory count.\n    std::cout &lt;&lt; \"Shelf Audit Results:\\n\";\n    for (const auto&amp; item : results) {\n        std::cout &lt;&lt; \" - Item: \" &lt;&lt; item.label\n                  &lt;&lt; \" (ID: \" &lt;&lt; item.class_id &lt;&lt; \")\"\n                  &lt;&lt; \", Count: \" &lt;&lt; item.count &lt;&lt; \"\\n\";\n    }\n    // This data can be compared against the store's inventory database to find discrepancies.\n}\n</code></pre> Config Struct: <code>ShelfAuditorConfig</code> Input: <code>cv::Mat</code> image of a store shelf. Output Struct: <code>ShelfItem</code> (contains product ID, label, on-shelf count, and locations). \"F1 Car\" Technology: This class is a specialized wrapper around the <code>zoo::vision::ObjectDetector</code>, using its high-throughput capabilities to count hundreds of items in a single frame.</p>"},{"location":"zoo-api/retail/#customeranalyzer","title":"<code>CustomerAnalyzer</code>","text":"<p>Performs real-time, anonymous tracking of customers in a physical store to generate analytics on traffic patterns and behavior.</p> <p>Header: <code>#include &lt;xinfer/zoo/retail/customer_analyzer.h&gt;</code></p>"},{"location":"zoo-api/retail/#use-case-store-layout-optimization","title":"Use Case: Store Layout Optimization","text":"<p>A store manager wants to understand how customers move through the store to optimize product placement and identify bottlenecks. The <code>CustomerAnalyzer</code> processes video feeds to generate a continuous stream of tracking data and a long-term traffic heatmap.</p> <p><pre><code>#include &lt;xinfer/zoo/retail/customer_analyzer.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n\nint main() {\n    // 1. Configure the analyzer.\n    //    This uses two engines: one for person detection and one for pose estimation.\n    xinfer::zoo::retail::CustomerAnalyzerConfig config;\n    config.detection_engine_path = \"assets/person_detector.engine\";\n    config.pose_engine_path = \"assets/pose_estimator.engine\";\n\n    // 2. Initialize.\n    xinfer::zoo::retail::CustomerAnalyzer analyzer(config);\n\n    // 3. Process frames from a store's security camera in a loop.\n    cv::VideoCapture cap(\"assets/store_footage.mp4\");\n    cv::Mat frame;\n    while (cap.read(frame)) {\n        // This call updates the internal state of all tracked customers.\n        std::vector&lt;xinfer::zoo::retail::TrackedCustomer&gt; tracked_customers = analyzer.track(frame);\n\n        // (In a real app, you would use this tracking data for further analysis)\n    }\n\n    // 4. After processing, generate a long-term traffic heatmap.\n    cv::Mat heatmap = analyzer.generate_heatmap();\n    cv::imwrite(\"traffic_heatmap.png\", heatmap);\n    std::cout &lt;&lt; \"Saved customer traffic heatmap to traffic_heatmap.png\\n\";\n}\n</code></pre> Config Struct: <code>CustomerAnalyzerConfig</code> Input: <code>cv::Mat</code> video frame. Output Struct: <code>TrackedCustomer</code> (contains a unique track ID and bounding box). Method: <code>generate_heatmap()</code> returns a <code>cv::Mat</code> visualization.</p>"},{"location":"zoo-api/retail/#demandforecaster","title":"<code>DemandForecaster</code>","text":"<p>A specialized version of the time-series forecaster, tailored for predicting retail product demand.</p> <p>Header: <code>#include &lt;xinfer/zoo/retail/demand_forecaster.h&gt;</code></p>"},{"location":"zoo-api/retail/#use-case-automated-inventory-replenishment","title":"Use Case: Automated Inventory Replenishment","text":"<p>A retail chain needs to automatically forecast the demand for thousands of products to optimize their inventory and prevent \"out of stock\" situations.</p> <p><pre><code>#include &lt;xinfer/zoo/retail/demand_forecaster.h&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\nint main() {\n    // 1. Configure the forecaster.\n    xinfer::zoo::retail::DemandForecasterConfig config;\n    config.engine_path = \"assets/sales_forecasting_model.engine\";\n    config.input_sequence_length = 90;  // Use 90 days of sales history\n    config.output_sequence_length = 14; // Predict demand for the next 14 days\n\n    // 2. Initialize.\n    xinfer::zoo::retail::DemandForecaster forecaster(config);\n\n    // 3. Provide the historical sales data for a single product.\n    std::vector&lt;float&gt; historical_sales(90);\n    // ... load the last 90 days of sales data from a database ...\n\n    // 4. Predict the future demand.\n    std::vector&lt;float&gt; predicted_demand = forecaster.predict(historical_sales);\n\n    std::cout &lt;&lt; \"Predicted demand for the next 14 days:\\n\";\n    for (size_t i = 0; i &lt; predicted_demand.size(); ++i) {\n        std::cout &lt;&lt; \" - Day \" &lt;&lt; i+1 &lt;&lt; \": \" &lt;&lt; (int)predicted_demand[i] &lt;&lt; \" units\\n\";\n    }\n}\n</code></pre> Config Struct: <code>DemandForecasterConfig</code> Input: <code>std::vector&lt;float&gt;</code> of historical sales data. Output: <code>std::vector&lt;float&gt;</code> of predicted future sales. \"F1 Car\" Technology: This class is a specialized wrapper around the <code>zoo::timeseries::Forecaster</code>, which can use a hyper-optimized Mamba or Transformer engine to capture complex seasonalities and trends in the sales data.</p>"},{"location":"zoo-api/rl/","title":"Zoo API: Reinforcement Learning","text":"<p>The <code>xinfer::zoo::rl</code> module provides the core, high-performance building block for deploying agents trained with Reinforcement Learning (RL).</p> <p>In RL, the inference step\u2014running the policy network to decide on an action\u2014is often in the \"hot loop\" of a real-time system. The latency of this single operation can determine the success or failure of the entire application, whether it's a robot, a trading bot, or a game AI.</p> <p>The <code>xInfer</code> RL zoo is designed around a single, powerful, and generic class: the <code>Policy</code>.</p>"},{"location":"zoo-api/rl/#core-component-rlpolicy","title":"Core Component: <code>rl::Policy</code>","text":"<p>Header: <code>#include &lt;xinfer/zoo/rl/policy.h&gt;</code></p> <p>The <code>Policy</code> class is a hyper-optimized, low-latency engine for executing a trained RL agent's decision-making network. It is a generic wrapper that can run any policy that takes a state tensor as input and produces an action tensor as output.</p> <p>The real \"F1 car\" magic comes from the TensorRT engine it loads. You would train your agent in a Python framework (like Stable Baselines3 or CleanRL), export the final actor/policy network to ONNX, and then use the <code>xinfer-cli</code> to build a hyper-optimized, INT8- or FP16-quantized engine. The <code>Policy</code> class then runs this engine with minimal overhead.</p>"},{"location":"zoo-api/rl/#core-api","title":"Core API","text":"<pre><code>#include &lt;xinfer/zoo/rl/policy.h&gt;\n\n// Configuration\nstruct PolicyConfig {\n    std::string engine_path;\n};\n\nclass Policy {\npublic:\n    explicit Policy(const PolicyConfig&amp; config);\n\n    // Get an action for a single state\n    core::Tensor predict(const core::Tensor&amp; state);\n\n    // Get actions for a batch of states (highly efficient)\n    core::Tensor predict_batch(const core::Tensor&amp; state_batch);\n};\n</code></pre>"},{"location":"zoo-api/rl/#domain-specific-applications","title":"Domain-Specific Applications","text":"<p>While the <code>rl::Policy</code> is generic, it serves as the core engine for many specialized, real-world applications. The following <code>zoo</code> classes are powerful examples of how to use the <code>Policy</code> engine to solve domain-specific problems.</p>"},{"location":"zoo-api/rl/#industrial-robotics-roboticsassemblypolicy","title":"Industrial Robotics: <code>robotics::AssemblyPolicy</code>","text":"<p>Header: <code>#include &lt;xinfer/zoo/robotics/assembly_policy.h&gt;</code></p> <p>Use Case: Controls a robot arm to perform complex, vision-based assembly tasks. The policy takes a camera image and the robot's joint angles as input and outputs motor commands.</p> <pre><code>// This function would be in the robot's 100Hz control loop\nvoid execute_robot_step(xinfer::zoo::robotics::AssemblyPolicy&amp; policy) {\n    // 1. Get current state from sensors\n    cv::Mat camera_image = get_camera_frame();\n    std::vector&lt;float&gt; joint_states = get_joint_angles();\n\n    // 2. Execute the policy to get the next action in milliseconds\n    std::vector&lt;float&gt; next_action = policy.predict(camera_image, joint_states);\n\n    // 3. Send action to motor controllers\n    send_motor_commands(next_action);\n}\n</code></pre>"},{"location":"zoo-api/rl/#autonomous-drones-dronesnavigationpolicy","title":"Autonomous Drones: <code>drones::NavigationPolicy</code>","text":"<p>Header: <code>#include &lt;xinfer/zoo/drones/navigation_policy.h&gt;</code></p> <p>Use Case: Enables agile, GPS-denied flight in cluttered environments. The policy takes a depth image and the drone's current state (velocity, orientation) as input and outputs flight control commands (roll, pitch, yaw, thrust).</p> <pre><code>// This function runs inside the drone's flight controller\nvoid navigate_step(xinfer::zoo::drones::NavigationPolicy&amp; policy) {\n    // 1. Get state from sensors\n    cv::Mat depth_image = get_depth_camera_frame();\n    std::vector&lt;float&gt; drone_state = get_imu_data();\n\n    // 2. Execute the policy to get flight commands\n    xinfer::zoo::drones::NavigationAction action = policy.predict(depth_image, drone_state);\n\n    // 3. Send commands to the motors\n    set_motor_outputs(action.roll, action.pitch, action.yaw, action.thrust);\n}\n</code></pre>"},{"location":"zoo-api/rl/#high-frequency-trading-hftorderexecutionpolicy","title":"High-Frequency Trading: <code>hft::OrderExecutionPolicy</code>","text":"<p>Header: <code>#include &lt;xinfer/zoo/hft/order_execution_policy.h&gt;</code></p> <p>Use Case: Manages the execution of a large financial order to minimize market impact. The policy takes the current state of the limit order book as input and decides whether to place a small buy/sell order in the next microsecond.</p> <pre><code>// This function is in the hot path of a trading application's event loop\nvoid on_market_data_update(xinfer::zoo::hft::OrderExecutionPolicy&amp; policy) {\n    // 1. Get the current market state as a GPU tensor\n    xinfer::core::Tensor market_state_tensor = get_order_book_tensor();\n\n    // 2. Execute the policy with microsecond latency\n    xinfer::zoo::hft::OrderExecutionAction action = policy.predict(market_state_tensor);\n\n    // 3. Execute the trade\n    if (action.action == OrderActionType::PLACE_BUY) {\n        execute_buy_order(action.volume, action.price_level);\n    }\n}\n</code></pre>"},{"location":"zoo-api/rl/#game-development-gamingnpc_behaviorpolicy","title":"Game Development: <code>gaming::NPC_BehaviorPolicy</code>","text":"<p>Header: `#include  <p>Use Case: Creates intelligent, non-scripted AI for hundreds of game characters. The policy takes a batch of states for all NPCs in a level and outputs a batch of actions.</p> <pre><code>// This function runs once per game frame\nvoid update_all_npc_ai(xinfer::zoo::gaming::NPCBehaviorPolicy&amp; policy, World&amp; world) {\n    // 1. Gather the states of all active NPCs into a single batched tensor\n    xinfer::core::Tensor npc_state_batch = world.get_all_npc_states();\n\n    // 2. Execute the policy for all NPCs in a single, efficient GPU call\n    xinfer::core::Tensor npc_action_batch = policy.predict_batch(npc_state_batch);\n\n    // 3. Apply the actions to each NPC in the world\n    world.set_all_npc_actions(npc_action_batch);\n}\n</code></pre>"},{"location":"zoo-api/robotics/","title":"Zoo API: Robotics","text":"<p>The <code>xinfer::zoo::robotics</code> module provides a suite of hyper-optimized, low-latency pipelines for common robotics tasks.</p> <p>In robotics, performance is not optional. The \"perception-to-action\" loop\u2014the time it takes for a robot to see the world, understand it, and react\u2014must often be measured in milliseconds. The <code>zoo</code> classes in this module are designed from the ground up for these hard real-time constraints, providing the core building blocks for intelligent robotic systems.</p> <p>These pipelines are built to run efficiently on power-constrained embedded hardware, such as the NVIDIA Jetson platform.</p>"},{"location":"zoo-api/robotics/#assemblypolicy","title":"<code>AssemblyPolicy</code>","text":"<p>Executes a trained Reinforcement Learning (RL) policy for complex, vision-based manipulation tasks.</p> <p>Header: <code>#include &lt;xinfer/zoo/robotics/assembly_policy.h&gt;</code></p>"},{"location":"zoo-api/robotics/#use-case-ai-driven-robotic-assembly","title":"Use Case: AI-Driven Robotic Assembly","text":"<p>This class is designed for tasks like peg-in-hole insertion, wire routing, or component assembly, where a robot must learn from both visual and physical feedback. The <code>AssemblyPolicy</code> acts as the robot's \"brain,\" taking in sensor data and outputting low-level motor commands.</p> <p><pre><code>#include &lt;xinfer/zoo/robotics/assembly_policy.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\n// This function would be in the robot's main control loop, running at high frequency (e.g., 100Hz)\nvoid execute_robot_step(xinfer::zoo::robotics::AssemblyPolicy&amp; policy) {\n    // 1. Get current state from the robot's sensors.\n    cv::Mat camera_image; // Image from the robot's wrist-mounted camera\n    std::vector&lt;float&gt; joint_states; // Current angles of the robot's joints\n    // ... update camera_image and joint_states from the robot's hardware interface ...\n\n    // 2. Execute the policy to get the next action.\n    //    This is a single, ultra-low-latency call to a fused C++/TensorRT pipeline.\n    std::vector&lt;float&gt; next_action = policy.predict(camera_image, joint_states);\n\n    // 3. Send the action to the robot's motor controllers.\n    // ... send motor commands based on next_action ...\n}\n\nint main() {\n    // 1. Configure and initialize the assembly policy.\n    //    This loads two pre-built engines: one for vision, one for the policy MLP.\n    xinfer::zoo::robotics::AssemblyPolicyConfig config;\n    config.vision_encoder_engine_path = \"assets/robot_vision_encoder.engine\";\n    config.policy_engine_path = \"assets/peg_insertion_policy.engine\";\n\n    xinfer::zoo::robotics::AssemblyPolicy policy(config);\n    std::cout &lt;&lt; \"Assembly policy loaded and ready.\\n\";\n\n    // 2. Run the robot's control loop.\n    // while (task_is_active) {\n    //     execute_robot_step(policy);\n    // }\n\n    return 0;\n}\n</code></pre> Config Struct: <code>AssemblyPolicyConfig</code> Input: <code>cv::Mat</code> from a camera and a <code>std::vector&lt;float&gt;</code> of the robot's physical state. Output: <code>std::vector&lt;float&gt;</code> representing the next motor commands (e.g., joint velocities or end-effector deltas). \"F1 Car\" Technology: This class orchestrates multiple, hyper-optimized TensorRT engines (a vision encoder and a policy MLP) within a hard real-time C++ application, ensuring a consistent, low-latency perception-to-action loop.</p>"},{"location":"zoo-api/robotics/#graspplanner","title":"<code>GraspPlanner</code>","text":"<p>Performs 6D pose estimation on an object to determine a stable grasp for a robotic hand.</p> <p>Header: <code>#include &lt;xinfer/zoo/robotics/grasp_planner.h&gt;</code> (Note: This is a conceptual name for a 6D Pose Estimator specialized for robotics)</p>"},{"location":"zoo-api/robotics/#use-case-bin-picking","title":"Use Case: Bin Picking","text":"<p>A robot needs to pick a specific object out of a bin of jumbled, randomly oriented parts. It must accurately determine the object's 3D position and orientation to grasp it successfully.</p> <p><pre><code>#include &lt;xinfer/zoo/robotics/grasp_planner.h&gt; // Conceptual header\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    // 1. Initialize the grasp planner.\n    //    The engine would be a specialized model trained on 3D data.\n    xinfer::zoo::robotics::GraspPlannerConfig config;\n    config.engine_path = \"assets/6d_pose_estimator.engine\";\n\n    xinfer::zoo::robotics::GraspPlanner planner(config);\n\n    // 2. Get an RGB-D image from a 3D camera.\n    cv::Mat color_image; // ... from sensor\n    cv::Mat depth_image; // ... from sensor\n\n    // 3. Predict the 6D pose of the target object.\n    auto grasp_poses = planner.predict(color_image, depth_image);\n\n    // 4. Execute the grasp with the robot arm.\n    if (!grasp_poses.empty()) {\n        std::cout &lt;&lt; \"Found a graspable object. Executing pick.\\n\";\n        // ... robot control logic to move to the grasp_poses ...\n    } else {\n        std::cout &lt;&lt; \"No graspable object found.\\n\";\n    }\n\n    return 0;\n}\n</code></pre> Config Struct: <code>GraspPlannerConfig</code> Input: <code>cv::Mat</code> for color and depth data. Output: A list of possible 6D grasp poses (position + quaternion rotation). \"F1 Car\" Technology: This pipeline would use custom CUDA kernels for point cloud processing (e.g., from the depth image) and a fused TensorRT engine for the final pose regression.</p>"},{"location":"zoo-api/special/","title":"Zoo API: Specialized &amp; High-Value Models","text":"<p>The <code>xinfer::zoo::special</code> module is the home for our most advanced, domain-specific, and performance-critical solutions.</p> <p>While other <code>zoo</code> modules provide optimized pipelines for common AI tasks, the classes in this module often solve problems that are computationally infeasible without a from-scratch, hyper-optimized C++/CUDA implementation. These are the \"F1 car\" engines for industries where every microsecond and every floating-point operation counts.</p> <p>This module is for experts who are pushing the boundaries of what is possible with computational science and AI.</p>"},{"location":"zoo-api/special/#hftmodel-high-frequency-trading","title":"<code>HFTModel</code> (High-Frequency Trading)","text":"<p>Provides a minimal-overhead, ultra-low-latency engine for executing a financial trading model.</p> <p>Header: <code>#include &lt;xinfer/zoo/special/hft.h&gt;</code></p>"},{"location":"zoo-api/special/#use-case-microsecond-scale-market-prediction","title":"Use Case: Microsecond-Scale Market Prediction","text":"<p>In high-frequency trading, the entire \"perception-to-action\" loop\u2014from receiving a market data packet to sending an order\u2014must happen in a few microseconds. The model inference is a critical part of this loop. The <code>HFTModel</code> is designed to be called from a low-level C++ trading application where performance is the only metric that matters.</p> <p><pre><code>#include &lt;xinfer/zoo/special/hft.h&gt;\n#include &lt;xinfer/core/tensor.h&gt;\n#include &lt;iostream&gt;\n\n// This function would be in the hot path of a trading application\nvoid on_market_data_update(xinfer::zoo::special::HFTModel&amp; model) {\n    // 1. Receive market data (e.g., limit order book state)\n    //    and place it directly into a pre-allocated GPU tensor.\n    xinfer::core::Tensor market_state_tensor;\n    // ... logic to populate the tensor from a network feed ...\n\n    // 2. Execute the model to get a trading signal.\n    //    This is a single, hyper-optimized TensorRT call.\n    xinfer::zoo::special::TradingSignal signal = model.predict(market_state_tensor);\n\n    // 3. Act on the signal immediately.\n    if (signal.action == xinfer::zoo::special::TradingAction::BUY) {\n        // ... execute buy order ...\n    }\n}\n\nint main() {\n    // 1. Configure and load the pre-compiled HFT policy engine.\n    xinfer::zoo::special::HFTConfig config;\n    config.engine_path = \"assets/market_alpha_model.engine\";\n\n    xinfer::zoo::special::HFTModel model(config);\n\n    // 2. Run in a tight loop.\n    // while (market_is_open) {\n    //     on_market_data_update(model);\n    // }\n    return 0;\n}\n</code></pre> Config Struct: <code>HFTConfig</code> Input: <code>xinfer::core::Tensor</code> representing the current market state. Output Struct: <code>TradingSignal</code> (an enum for BUY/SELL/HOLD and a confidence score).</p>"},{"location":"zoo-api/special/#fluidsimulator-physics-simulation","title":"<code>FluidSimulator</code> (Physics Simulation)","text":"<p>Provides a real-time, GPU-accelerated fluid dynamics solver. This is not a neural network; it is a direct CUDA implementation of a physics model.</p> <p>Header: <code>#include &lt;xinfer/zoo/special/physics.h&gt;</code></p>"},{"location":"zoo-api/special/#use-case-interactive-vfx-and-engineering","title":"Use Case: Interactive VFX and Engineering","text":"<p>This class allows for real-time simulation of smoke, fire, or water for visual effects, or for interactive computational fluid dynamics (CFD) in engineering design.</p> <p><pre><code>#include &lt;xinfer/zoo/special/physics.h&gt;\n#include &lt;xinfer/core/tensor.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    // 1. Configure the simulator grid and fluid properties.\n    xinfer::zoo::special::FluidSimulatorConfig config;\n    config.resolution_x = 512;\n    config.resolution_y = 512;\n    config.viscosity = 0.01f;\n\n    // 2. Initialize the simulator.\n    xinfer::zoo::special::FluidSimulator simulator(config);\n\n    // 3. Create GPU tensors to hold the state of the simulation.\n    xinfer::core::Tensor velocity_field({1, 2, 512, 512}, xinfer::core::DataType::kFLOAT);\n    xinfer::core::Tensor density_field({1, 1, 512, 512}, xinfer::core::DataType::kFLOAT);\n    // ... initialize fields with some starting state ...\n\n    // 4. Run the simulation loop.\n    std::cout &lt;&lt; \"Running fluid simulation for 100 steps...\\n\";\n    for (int i = 0; i &lt; 100; ++i) {\n        // Add a force or density source (e.g., from user input)\n        // ...\n\n        // This single call executes a chain of custom CUDA kernels (advect, diffuse, project).\n        simulator.step(velocity_field, density_field);\n\n        // (In a real app, you would render the density_field here)\n    }\n    std::cout &lt;&lt; \"Simulation complete.\\n\";\n    return 0;\n}\n</code></pre> Config Struct: <code>FluidSimulatorConfig</code> Input/Output: <code>xinfer::core::Tensor</code> objects representing the fluid's state, which are modified in-place. \"F1 Car\" Technology: This class is a wrapper around a set of from-scratch, hyper-optimized CUDA kernels for solving the Navier-Stokes equations.</p>"},{"location":"zoo-api/special/#variantcaller-genomics","title":"<code>VariantCaller</code> (Genomics)","text":"<p>Provides an ultra-fast engine for genomic analysis, designed to work with next-generation sequencing data.</p> <p>Header: <code>#include &lt;xinfer/zoo/special/genomics.h&gt;</code></p>"},{"location":"zoo-api/special/#use-case-accelerating-personalized-medicine","title":"Use Case: Accelerating Personalized Medicine","text":"<p>A researcher wants to analyze a patient's entire DNA sequence to find mutations linked to a disease. This requires a model that can process a sequence of billions of characters.</p> <p><pre><code>#include &lt;xinfer/zoo/special/genomics.h&gt;\n#include &lt;iostream&gt;\n#include &lt;string&gt;\n\nint main() {\n    // 1. Configure the variant caller.\n    //    The engine would be a hyper-optimized Mamba or long-convolution model.\n    xinfer::zoo::special::VariantCallerConfig config;\n    config.engine_path = \"assets/genomic_foundation_model.engine\";\n    config.vocab_path = \"assets/dna_vocab.json\";\n\n    // 2. Initialize.\n    xinfer::zoo::special::VariantCaller caller(config);\n\n    // 3. Load a long DNA sequence (e.g., an entire chromosome).\n    std::string dna_sequence = \"GATTACA...\"; // This would be millions of characters long\n\n    // 4. Run the prediction.\n    //    This is only possible because the underlying Mamba engine can handle\n    //    the massive sequence length with linear, not quadratic, complexity.\n    std::cout &lt;&lt; \"Analyzing DNA sequence for variants...\\n\";\n    std::vector&lt;xinfer::zoo::special::GenomicVariant&gt; variants = caller.predict(dna_sequence);\n\n    std::cout &lt;&lt; \"Found \" &lt;&lt; variants.size() &lt;&lt; \" potential variants.\\n\";\n    return 0;\n}\n</code></pre> Config Struct: <code>VariantCallerConfig</code> Input: A <code>std::string</code> containing a very long DNA sequence. Output Struct: <code>GenomicVariant</code> (contains the position, reference base, and alternate base). \"F1 Car\" Technology: This class is built on top of a hyper-optimized Mamba engine, likely using a custom CUDA kernel for the selective scan operation. This is the only architecture that can feasibly handle the massive sequence lengths found in genomics.</p>"},{"location":"zoo-api/telecom/","title":"Zoo API: Telecommunications","text":"<p>The <code>xinfer::zoo::telecom</code> module provides hyper-optimized, low-latency solutions specifically designed for the telecommunications industry.</p> <p>Modern networks, particularly 5G and beyond, are incredibly complex, dynamic systems. Managing them efficiently and reliably requires AI that can make decisions in real-time. This is a domain where the performance of C++ and the optimization of <code>xInfer</code> are not just beneficial\u2014they are a fundamental requirement.</p> <p>The classes in this module are designed to be integrated directly into the control plane of network infrastructure, providing a level of autonomous optimization that is impossible with slower, non-specialized frameworks.</p>"},{"location":"zoo-api/telecom/#networkcontrolpolicy","title":"<code>NetworkControlPolicy</code>","text":"<p>Executes a trained Reinforcement Learning (RL) policy to dynamically control network parameters.</p> <p>This is a powerful tool for automating complex network management tasks like resource allocation, beamforming, and traffic shaping.</p> <p>Header: <code>#include &lt;xinfer/zoo/telecom/network_control_policy.h&gt;</code></p>"},{"location":"zoo-api/telecom/#use-case-real-time-radio-access-network-ran-optimization","title":"Use Case: Real-Time Radio Access Network (RAN) Optimization","text":"<p>Imagine an RL agent trained to manage a 5G cell tower. At every timestep, it analyzes the current state of the network (e.g., user load, signal interference, data demand) and chooses the optimal configuration for hundreds of parameters. This requires a policy that can run in milliseconds.</p> <p><pre><code>#include &lt;xinfer/zoo/telecom/network_control_policy.h&gt;\n#include &lt;xinfer/core/tensor.h&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\n// This function would be part of the network's control loop, running continuously\nvoid manage_network_slice(xinfer::zoo::telecom::NetworkControlPolicy&amp; policy) {\n    // 1. Get the current network state from telemetry data.\n    //    This data is compiled into a single state vector.\n    std::vector&lt;float&gt; current_state_vector;\n    // ... collect real-time data from network monitoring tools ...\n\n    // 2. Create a GPU tensor from the state vector.\n    auto input_shape = {1, current_state_vector.size()}; // Batch size 1\n    xinfer::core::Tensor state_tensor(input_shape, xinfer::core::DataType::kFLOAT);\n    state_tensor.copy_from_host(current_state_vector.data());\n\n    // 3. Execute the policy to get the optimal actions.\n    //    This is a single, low-latency call to the TensorRT engine.\n    xinfer::core::Tensor action_tensor = policy.predict(state_tensor);\n\n    // 4. Decode the action tensor and apply the new configuration.\n    std::vector&lt;float&gt; new_config_params(action_tensor.num_elements());\n    action_tensor.copy_to_host(new_config_params.data());\n\n    // ... send new_config_params to the network hardware controllers ...\n    std::cout &lt;&lt; \"New network configuration applied.\\n\";\n}\n\nint main() {\n    // 1. Configure and initialize the policy engine.\n    //    The engine is pre-built from a model trained in a simulator.\n    xinfer::zoo::telecom::NetworkControlPolicyConfig config;\n    config.engine_path = \"assets/ran_optimization_policy.engine\";\n\n    xinfer::zoo::telecom::NetworkControlPolicy policy(config);\n\n    // 2. Run the control loop.\n    // while (true) {\n    //     manage_network_slice(policy);\n    // }\n\n    return 0;\n}\n</code></pre> Config Struct: <code>NetworkControlPolicyConfig</code> Input: <code>xinfer::core::Tensor</code> representing the current state of the network. Output: <code>xinfer::core::Tensor</code> representing the new configuration parameters (the \"actions\"). \"F1 Car\" Technology: This class is a wrapper around the <code>zoo::rl::Policy</code> engine, which provides a hyper-optimized, low-latency implementation of the MLP or small Transformer that typically constitutes an RL policy. This allows the control loop to run at the millisecond scale required for real-time network management.</p>"},{"location":"zoo-api/threed/","title":"Zoo API: 3D &amp; Spatial Computing","text":"<p>The <code>xinfer::zoo::threed</code> module provides high-level pipelines for processing and understanding 3D data, primarily from sensors like LIDAR or through multi-view reconstruction.</p> <p>These classes are built on top of <code>xInfer</code>'s most advanced \"F1 car\" technologies. They abstract away extremely complex, non-standard GPU operations, allowing you to integrate state-of-the-art 3D AI into your C++ applications.</p>"},{"location":"zoo-api/threed/#pointclouddetector","title":"<code>PointCloudDetector</code>","text":"<p>Performs 3D object detection directly on point cloud data. This is a fundamental task for autonomous driving and robotics.</p> <p>Header: <code>#include &lt;xinfer/zoo/threed/pointcloud_detector.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/threed/pointcloud_detector.h&gt;\n#include &lt;xinfer/core/tensor.h&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\nint main() {\n    // 1. Configure the 3D detector.\n    xinfer::zoo::threed::PointCloudDetectorConfig config;\n    config.engine_path = \"assets/pointpillar.engine\";\n    config.labels_path = \"assets/kitti_labels.txt\"; // e.g., \"Car\", \"Pedestrian\", \"Cyclist\"\n\n    // 2. Initialize.\n    xinfer::zoo::threed::PointCloudDetector detector(config);\n\n    // 3. Load LIDAR point cloud data and upload to a GPU tensor.\n    //    (In a real app, this would come from a LIDAR sensor driver)\n    std::vector&lt;float&gt; points; // Vector of [x, y, z, intensity] floats\n    // ... load points from a .bin file ...\n    xinfer::core::Tensor point_cloud({1, points.size() / 4, 4}, xinfer::core::DataType::kFLOAT);\n    point_cloud.copy_from_host(points.data());\n\n    // 4. Predict 3D bounding boxes.\n    std::vector&lt;xinfer::zoo::threed::BoundingBox3D&gt; detections = detector.predict(point_cloud);\n\n    // 5. Print results.\n    std::cout &lt;&lt; \"Detected \" &lt;&lt; detections.size() &lt;&lt; \" objects:\\n\";\n    for (const auto&amp; box : detections) {\n        std::cout &lt;&lt; \" - Label: \" &lt;&lt; box.label &lt;&lt; \", Confidence: \" &lt;&lt; box.confidence &lt;&lt; \"\\n\";\n    }\n}\n</code></pre> Config Struct: <code>PointCloudDetectorConfig</code> Input: <code>xinfer::core::Tensor</code> containing the point cloud data. Output Struct: <code>BoundingBox3D</code> (contains 3D center, dimensions, yaw, label, etc.).</p>"},{"location":"zoo-api/threed/#pointcloudsegmenter","title":"<code>PointCloudSegmenter</code>","text":"<p>Performs semantic segmentation on a point cloud, assigning a class label to every single point.</p> <p>Header: <code>#include &lt;xinfer/zoo/threed/pointcloud_segmenter.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/threed/pointcloud_segmenter.h&gt;\n#include &lt;xinfer/core/tensor.h&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\nint main() {\n    // 1. Configure the 3D segmenter.\n    xinfer::zoo::threed::PointCloudSegmenterConfig config;\n    config.engine_path = \"assets/randlanet.engine\";\n    config.labels_path = \"assets/semantic_kitti_labels.txt\"; // e.g., \"road\", \"building\", \"vegetation\"\n\n    // 2. Initialize.\n    xinfer::zoo::threed::PointCloudSegmenter segmenter(config);\n\n    // 3. Load LIDAR point cloud data.\n    std::vector&lt;float&gt; points; // Vector of [x, y, z, intensity] floats\n    // ... load points ...\n    xinfer::core::Tensor point_cloud({1, points.size() / 4, 4}, xinfer::core::DataType::kFLOAT);\n    point_cloud.copy_to_host(points.data());\n\n    // 4. Predict per-point labels.\n    std::vector&lt;int&gt; point_labels = segmenter.predict(point_cloud);\n\n    // 5. Print results.\n    std::cout &lt;&lt; \"Segmentation complete. Got \" &lt;&lt; point_labels.size() &lt;&lt; \" labels.\\n\";\n    // You would now use these labels to colorize the point cloud for visualization.\n}\n</code></pre> Config Struct: <code>PointCloudSegmenterConfig</code> Input: <code>xinfer::core::Tensor</code> containing the point cloud data. Output: <code>std::vector&lt;int&gt;</code> of class IDs, one for each input point.</p>"},{"location":"zoo-api/threed/#reconstructor","title":"<code>Reconstructor</code>","text":"<p>Reconstructs a 3D scene from a collection of 2D images, using a state-of-the-art neural rendering pipeline like 3D Gaussian Splatting.</p> <p>Header: <code>#include &lt;xinfer/zoo/threed/reconstructor.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/threed/reconstructor.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\nint main() {\n    // 1. Configure the reconstructor.\n    xinfer::zoo::threed::ReconstructorConfig config;\n    config.num_iterations = 15000; // More iterations = higher quality\n\n    // 2. Initialize. This sets up the custom CUDA pipeline.\n    xinfer::zoo::threed::Reconstructor reconstructor(config);\n\n    // 3. Load a set of images and their corresponding camera poses.\n    std::vector&lt;cv::Mat&gt; images;\n    std::vector&lt;cv::Mat&gt; poses;\n    // ... load images and camera poses from disk ...\n\n    // 4. Run the reconstruction and meshing process.\n    //    This is a heavy operation that trains the 3D representation and extracts a mesh.\n    std::cout &lt;&lt; \"Starting 3D reconstruction...\\n\";\n    xinfer::zoo::threed::Mesh3D result_mesh = reconstructor.predict(images, poses);\n\n    // 5. Save the resulting mesh to a file.\n    // (You would write the result_mesh.vertices and faces to an .obj file here)\n    std::cout &lt;&lt; \"Reconstruction complete. Mesh has \" &lt;&lt; result_mesh.vertices.size() / 3 &lt;&lt; \" vertices.\\n\";\n}\n</code></pre> Config Struct: <code>ReconstructorConfig</code> Input: <code>std::vector&lt;cv::Mat&gt;</code> of images and a <code>std::vector&lt;cv::Mat&gt;</code> of camera poses. Output Struct: <code>Mesh3D</code> (contains vertices, faces, and vertex colors). \"F1 Car\" Technology: This class is a wrapper around a full, from-scratch custom CUDA pipeline for training and meshing a neural representation like Gaussian Splatting.</p>"},{"location":"zoo-api/threed/#slamaccelerator","title":"<code>SLAMAccelerator</code>","text":"<p>Provides hyper-optimized components to accelerate a Visual SLAM (Simultaneous Localization and Mapping) pipeline.</p> <p>Header: <code>#include &lt;xinfer/zoo/threed/slam_accelerator.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/threed/slam_accelerator.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    // 1. Configure the SLAM accelerator.\n    //    The engine would be a learned feature extractor like SuperPoint.\n    xinfer::zoo::threed::SLAMAcceleratorConfig config;\n    config.feature_engine_path = \"assets/superpoint.engine\";\n\n    // 2. Initialize.\n    xinfer::zoo::threed::SLAMAccelerator accelerator(config);\n\n    // 3. In your main SLAM loop, use the accelerator for feature extraction.\n    cv::Mat video_frame; // from camera\n    xinfer::zoo::threed::SLAMFeatureResult features = accelerator.extract_features(video_frame);\n\n    std::cout &lt;&lt; \"Extracted \" &lt;&lt; features.keypoints.size() &lt;&lt; \" keypoints.\\n\";\n    // You would then pass these features to the tracking and mapping parts of your SLAM system.\n}\n</code></pre> Config Struct: <code>SLAMAcceleratorConfig</code> Input: <code>cv::Mat</code> video frame. Output Struct: <code>SLAMFeatureResult</code> (contains <code>cv::KeyPoint</code>s and <code>cv::Mat</code> descriptors).</p>"},{"location":"zoo-api/timeseries/","title":"Zoo API: Time-Series","text":"<p>The <code>xinfer::zoo::timeseries</code> module provides high-level, optimized pipelines for analyzing and predicting sequential data.</p> <p>These classes are designed to handle common tasks in domains like finance, IoT (Internet of Things), and predictive maintenance. They abstract away the complexity of sequence model inference, allowing you to get answers from your time-series data with simple, clean C++ code.</p>"},{"location":"zoo-api/timeseries/#forecaster","title":"<code>Forecaster</code>","text":"<p>Performs time-series forecasting. Given a sequence of historical data, it predicts a sequence of future values.</p> <p>Header: <code>#include &lt;xinfer/zoo/timeseries/forecaster.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/timeseries/forecaster.h&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\nint main() {\n    // 1. Configure the forecaster.\n    //    The engine would be pre-built from a model like N-BEATS or a Transformer.\n    xinfer::zoo::timeseries::ForecasterConfig config;\n    config.engine_path = \"assets/sales_forecaster.engine\";\n    config.input_sequence_length = 90;  // Model was trained on 90 days of history\n    config.output_sequence_length = 14; // Model predicts the next 14 days\n\n    // 2. Initialize the forecaster.\n    xinfer::zoo::timeseries::Forecaster forecaster(config);\n\n    // 3. Provide a vector of historical data.\n    //    (In a real app, this would come from a database or sensor feed)\n    std::vector&lt;float&gt; historical_sales(90); // Must match input_sequence_length\n    // ... fill historical_sales with data ...\n\n    // 4. Predict the future values in a single line.\n    std::vector&lt;float&gt; forecast = forecaster.predict(historical_sales);\n\n    // 5. Print the results.\n    std::cout &lt;&lt; \"Predicted sales for the next 14 days:\\n\";\n    for (size_t i = 0; i &lt; forecast.size(); ++i) {\n        std::cout &lt;&lt; \"Day \" &lt;&lt; i + 1 &lt;&lt; \": \" &lt;&lt; forecast[i] &lt;&lt; std::endl;\n    }\n}\n</code></pre> Config Struct: <code>ForecasterConfig</code> Input: <code>std::vector&lt;float&gt;</code> of historical data. Output: <code>std::vector&lt;float&gt;</code> of predicted future values.</p>"},{"location":"zoo-api/timeseries/#anomalydetector","title":"<code>AnomalyDetector</code>","text":"<p>Analyzes a window of time-series data to detect anomalous patterns or events. This is ideal for predictive maintenance and monitoring.</p> <p>Header: <code>#include &lt;xinfer/zoo/timeseries/anomaly_detector.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/timeseries/anomaly_detector.h&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\nint main() {\n    // 1. Configure the anomaly detector.\n    //    The engine is typically a reconstruction model like an Autoencoder or LSTM.\n    xinfer::zoo::timeseries::AnomalyDetectorConfig config;\n    config.engine_path = \"assets/machine_vibration_anomaly.engine\";\n    config.sequence_length = 128; // The model analyzes windows of 128 sensor readings\n    config.anomaly_threshold = 0.85f;\n\n    // 2. Initialize.\n    xinfer::zoo::timeseries::AnomalyDetector detector(config);\n\n    // 3. Provide a window of recent sensor data.\n    std::vector&lt;float&gt; sensor_data_window(128);\n    // ... fill sensor_data_window with recent readings ...\n\n    // 4. Predict.\n    xinfer::zoo::timeseries::AnomalyResult result = detector.predict(sensor_data_window);\n\n    // 5. Check the result.\n    if (result.is_anomaly) {\n        std::cout &lt;&lt; \"ANOMALY DETECTED!\\n\";\n        std::cout &lt;&lt; \"Anomaly Score: \" &lt;&lt; result.anomaly_score\n                  &lt;&lt; \" (Threshold: \" &lt;&lt; config.anomaly_threshold &lt;&lt; \")\\n\";\n    } else {\n        std::cout &lt;&lt; \"System normal. Anomaly Score: \" &lt;&lt; result.anomaly_score &lt;&lt; std::endl;\n    }\n}```\n**Config Struct:** `AnomalyDetectorConfig`\n**Input:** `std::vector&lt;float&gt;` of sequential data.\n**Output Struct:** `AnomalyResult` (contains a boolean flag, the anomaly score, and the reconstruction error per time step).\n\n---\n\n## `Classifier`\n\nPerforms classification on a segment of time-series data. This can be used for tasks like identifying the state of a machine or classifying heartbeats from an ECG signal.\n\n**Header:** `#include &lt;xinfer/zoo/timeseries/classifier.h&gt;`\n\n```cpp\n#include &lt;xinfer/zoo/timeseries/classifier.h&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\nint main() {\n    // 1. Configure the time-series classifier.\n    xinfer::zoo::timeseries::ClassifierConfig config;\n    config.engine_path = \"assets/ecg_arrhythmia.engine\";\n    config.labels_path = \"assets/ecg_labels.txt\"; // e.g., \"Normal Beat\", \"Arrhythmia\"\n    config.sequence_length = 256; // Model analyzes windows of 256 readings\n\n    // 2. Initialize.\n    xinfer::zoo::timeseries::Classifier classifier(config);\n\n    // 3. Provide a segment of time-series data.\n    std::vector&lt;float&gt; ecg_segment(256);\n    // ... fill ecg_segment with data from an ECG sensor ...\n\n    // 4. Predict.\n    xinfer::zoo::timeseries::TimeSeriesClassificationResult result = classifier.predict(ecg_segment);\n\n    // 5. Print the result.\n    std::cout &lt;&lt; \"ECG Segment Classification:\\n\";\n    std::cout &lt;&lt; \" - Label: \" &lt;&lt; result.label\n              &lt;&lt; \", Confidence: \" &lt;&lt; result.confidence &lt;&lt; std::endl;\n}\n</code></pre> Config Struct: <code>ClassifierConfig</code> Input: <code>std::vector&lt;float&gt;</code> of sequential data. Output Struct: <code>TimeSeriesClassificationResult</code> (contains class ID, label, and confidence score).</p>"},{"location":"zoo-api/vision/","title":"Zoo API: Computer Vision","text":"<p>The <code>xinfer::zoo::vision</code> module provides a comprehensive suite of high-level, hyper-optimized pipelines for the most common computer vision tasks.</p> <p>Each class in this module is an end-to-end solution that handles all the complexity of pre-processing, TensorRT inference, and GPU-accelerated post-processing, giving you the final, human-readable answer with a single <code>.predict()</code> call.</p>"},{"location":"zoo-api/vision/#imageclassifier","title":"<code>ImageClassifier</code>","text":"<p>Performs image classification, identifying the primary subject of an image.</p> <p>Header: <code>#include &lt;xinfer/zoo/vision/classifier.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/vision/classifier.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    // 1. Configure the classifier\n    xinfer::zoo::vision::ClassifierConfig config;\n    config.engine_path = \"assets/resnet50.engine\";\n    config.labels_path = \"assets/imagenet_labels.txt\";\n\n    // 2. Initialize\n    xinfer::zoo::vision::ImageClassifier classifier(config);\n\n    // 3. Predict\n    cv::Mat image = cv::imread(\"assets/dog.jpg\");\n    auto results = classifier.predict(image, 3); // Get top 3 results\n\n    // 4. Print results\n    std::cout &lt;&lt; \"Top 3 Predictions:\\n\";\n    for (const auto&amp; result : results) {\n        printf(\" - Label: %s, Confidence: %.4f\\n\", result.label.c_str(), result.confidence);\n    }\n}\n</code></pre> Config Struct: <code>ClassifierConfig</code> Output Struct: <code>ClassificationResult</code></p>"},{"location":"zoo-api/vision/#objectdetector","title":"<code>ObjectDetector</code>","text":"<p>Detects and localizes multiple objects within an image.</p> <p>Header: <code>#include &lt;xinfer/zoo/vision/detector.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/vision/detector.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    // 1. Configure the detector\n    xinfer::zoo::vision::DetectorConfig config;\n    config.engine_path = \"assets/yolov8n.engine\";\n    config.labels_path = \"assets/coco.names\";\n    config.confidence_threshold = 0.5f;\n\n    // 2. Initialize\n    xinfer::zoo::vision::ObjectDetector detector(config);\n\n    // 3. Predict\n    cv::Mat image = cv::imread(\"assets/street.jpg\");\n    auto detections = detector.predict(image);\n\n    // 4. Draw results\n    for (const auto&amp; box : detections) {\n        cv::rectangle(image, { (int)box.x1, (int)box.y1 }, { (int)box.x2, (int)box.y2 }, {0, 255, 0}, 2);\n    }\n    cv::imwrite(\"detections_output.jpg\", image);\n    std::cout &lt;&lt; \"Saved annotated image to detections_output.jpg\\n\";\n}\n</code></pre> Config Struct: <code>DetectorConfig</code> Output Struct: <code>BoundingBox</code></p>"},{"location":"zoo-api/vision/#segmenter","title":"<code>Segmenter</code>","text":"<p>Performs semantic segmentation, assigning a class label to every pixel in an image.</p> <p>Header: <code>#include &lt;xinfer/zoo/vision/segmenter.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/vision/segmenter.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n\nint main() {\n    // 1. Configure the segmenter\n    xinfer::zoo::vision::SegmenterConfig config;\n    config.engine_path = \"assets/segformer.engine\";\n\n    // 2. Initialize\n    xinfer::zoo::vision::Segmenter segmenter(config);\n\n    // 3. Predict\n    cv::Mat image = cv::imread(\"assets/cityscape.jpg\");\n    cv::Mat class_mask = segmenter.predict(image); // Returns a CV_8UC1 mask\n\n    // 4. Visualize the result\n    cv::Mat color_mask;\n    // (Here you would apply a colormap for visualization)\n    cv::imwrite(\"segmentation_output.png\", class_mask);\n}\n</code></pre> Config Struct: <code>SegmenterConfig</code> Output: <code>cv::Mat</code> (single-channel, 8-bit integer mask)</p>"},{"location":"zoo-api/vision/#instancesegmenter","title":"<code>InstanceSegmenter</code>","text":"<p>Performs instance segmentation, detecting individual object instances and providing a per-pixel mask for each one.</p> <p>Header: <code>#include &lt;xinfer/zoo/vision/instance_segmenter.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/vision/instance_segmenter.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n\nint main() {\n    // 1. Configure the instance segmenter\n    xinfer::zoo::vision::InstanceSegmenterConfig config;\n    config.engine_path = \"assets/mask_rcnn.engine\";\n    config.labels_path = \"assets/coco.names\";\n\n    // 2. Initialize\n    xinfer::zoo::vision::InstanceSegmenter segmenter(config);\n\n    // 3. Predict\n    cv::Mat image = cv::imread(\"assets/people.jpg\");\n    auto results = segmenter.predict(image);\n\n    // 4. Draw results\n    for (const auto&amp; instance : results) {\n        // (Draw the instance.mask and instance.bounding_box on the image)\n    }\n    cv::imwrite(\"instance_segmentation_output.jpg\", image);\n}\n</code></pre> Config Struct: <code>InstanceSegmenterConfig</code> Output Struct: <code>InstanceSegmentationResult</code> (contains box, mask, label, etc.)</p>"},{"location":"zoo-api/vision/#poseestimator","title":"<code>PoseEstimator</code>","text":"<p>Estimates the 2D keypoints of a human pose.</p> <p>Header: <code>#include &lt;xinfer/zoo/vision/pose_estimator.h&gt;</code></p> <p><pre><code>#include &lt;xinfer/zoo/vision/pose_estimator.h&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n\nint main() {\n    xinfer::zoo::vision::PoseEstimatorConfig config;\n    config.engine_path = \"assets/rtmpose.engine\";\n\n    xinfer::zoo::vision::PoseEstimator estimator(config);\n\n    cv::Mat image = cv::imread(\"assets/person_running.jpg\");\n    auto poses = estimator.predict(image);\n\n    // Draw the keypoints for the first detected person\n    if (!poses.empty()) {\n        for (const auto&amp; keypoint : poses) {\n            if (keypoint.confidence &gt; 0.5f) {\n                cv::circle(image, { (int)keypoint.x, (int)keypoint.y }, 3, {0, 0, 255}, -1);\n            }\n        }\n    }\n    cv::imwrite(\"pose_output.jpg\", image);\n}\n</code></pre> Config Struct: <code>PoseEstimatorConfig</code> Output: <code>std::vector&lt;Pose&gt;</code> where <code>Pose</code> is a <code>std::vector&lt;Keypoint&gt;</code></p>"},{"location":"zoo-api/vision/#and-more","title":"And More...","text":"<p>This module provides many more specialized pipelines, each with a simple, consistent API.</p> <ul> <li><code>DepthEstimator</code>: Predicts a dense depth map from a single RGB image.</li> <li><code>FaceDetector</code>: A lightweight and fast detector specifically for faces.</li> <li><code>FaceRecognizer</code>: Generates a 512-d feature embedding for a face, used for identification.</li> <li><code>HandTracker</code>: Detects and tracks hands and their keypoints in real-time.</li> <li><code>OCR</code>: A full, two-stage pipeline for detecting and recognizing text.</li> <li><code>ImageDeblur</code>: Sharpens blurry images using a generative model.</li> <li><code>LowLightEnhancer</code>: Brightens and denoises dark or nighttime images.</li> <li><code>SmokeFlameDetector</code>: A specialized detector for industrial safety and wildfire monitoring.</li> </ul> <p>Each of these would have its own section with a code example, just like the ones above.</p>"}]}