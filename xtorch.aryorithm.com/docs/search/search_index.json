{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#xtorch-the-batteries-included-c-library-for-pytorch","title":"xTorch: The Batteries-Included C++ Library for PyTorch","text":"<p>Bridging the usability gap in PyTorch\u2019s C++ API with high-level abstractions for building, training, and deploying models entirely in C++.</p> <p>GitHub Repository Get Started View Examples</p> <p>Library Under Development</p> <p>xTorch is currently under active development. The API may change. For stability, please use an official release version for production workloads.</p>"},{"location":"#the-motivation-a-first-class-c-deep-learning-experience","title":"The Motivation: A First-Class C++ Deep Learning Experience","text":"<p>PyTorch's C++ library, LibTorch, provides a powerful and performant core for deep learning. However, after 2019, its focus shifted primarily to a deployment-only role, leaving a significant gap for developers who wanted or needed to conduct end-to-end model development directly in C++. High-level utilities, data loaders with augmentations, and a rich model zoo\u2014features that make the Python API so productive\u2014were either missing or deprecated.</p> <p>xTorch was created to fill this gap.</p> <p>It extends LibTorch with the high-level, \"batteries-included\" abstractions that C++ developers have been missing. By building a thin, intuitive layer on top of LibTorch\u2019s robust core, xTorch restores ease-of-use without sacrificing the raw performance of C++. Our goal is to empower C++ developers with a productive experience on par with PyTorch in Python, enabling them to build, train, and deploy models with minimal boilerplate and maximum efficiency.</p>"},{"location":"#key-features","title":"Key Features","text":"Feature Description :material-layers-triple: High-Level Abstractions Simplified model classes, pre-built architectures (<code>ResNet</code>, <code>DCGAN</code>), and intuitive APIs. :material-run-fast: Simplified <code>Trainer</code> Loop A powerful, callback-driven training loop that handles optimization, metrics, logging, and checkpointing. :material-database-search: Enhanced Data Handling Built-in datasets (<code>ImageFolder</code>, <code>MNIST</code>), powerful <code>DataLoader</code>, and a rich library of data transforms. :material-rocket-launch: Seamless Serialization Easily save, load, and export models to TorchScript for production-ready inference pipelines. :material-chart-line: Uncompromised Performance Eliminate Python overhead. Achieve significant speedups over standard PyTorch workflows. :material-wrench: Extensive Toolkit A massive collection of optimizers, loss functions, normalizations, and regularization techniques."},{"location":"#a-quick-look","title":"A Quick Look","text":"<p>See how xTorch transforms a verbose C++ training task into a few lines of clean, expressive code.</p> <pre><code>#include &lt;xtorch/xtorch.h&gt;\n\nint main() {\n    // 1. Load data with transforms\n    auto dataset = xt::datasets::MNIST(\"./data\", xt::datasets::DataMode::TRAIN,\n        std::make_unique&lt;xt::transforms::Compose&gt;(\n            std::make_shared&lt;xt::transforms::image::Resize&gt;(std::vector&lt;int64_t&gt;{32, 32}),\n            std::make_shared&lt;xt::transforms::general::Normalize&gt;(0.5, 0.5)\n        )\n    );\n    xt::dataloaders::ExtendedDataLoader data_loader(dataset, 64, true);\n\n    // 2. Define model and optimizer\n    xt::models::LeNet5 model(10);\n    torch::optim::Adam optimizer(model.parameters(), 1e-3);\n\n    // 3. Configure and run the trainer\n    xt::Trainer trainer;\n    trainer.set_max_epochs(10)\n           .set_optimizer(optimizer)\n           .set_loss_fn(torch::nll_loss)\n           .add_callback(std::make_shared&lt;xt::LoggingCallback&gt;());\n\n    trainer.fit(model, data_loader, nullptr, torch::kCPU);\n\n    return 0;\n}\n</code></pre>"},{"location":"#who-is-this-for","title":"Who is this for?","text":"<ul> <li>C++ Developers who want to leverage a PyTorch-like ML framework without leaving their primary ecosystem.</li> <li>Performance Engineers needing to eliminate Python bottlenecks for data-intensive training or inference workloads.</li> <li>Researchers &amp; Students in HPC, robotics, or embedded systems where pure C++ deployment is a necessity.</li> <li>Educators looking for a tool to teach performance-aware machine learning concepts in C++.</li> </ul> <p>Ready to dive in? Check out the Installation Guide to set up your environment.</p>"},{"location":"api/","title":"API Reference","text":"<p>Welcome to the xTorch API reference. This section provides detailed documentation on all the public namespaces, classes, and functions that make up the xTorch library. It is designed to be a technical resource for understanding the specific components available for building your deep learning models.</p>"},{"location":"api/#relationship-with-libtorch","title":"Relationship with LibTorch","text":"<p>xTorch is built as a high-level extension of LibTorch (the PyTorch C++ API). Its goal is to provide usability and convenience, not to replace the core engine.</p> <p>While this reference covers all components added by xTorch, it does not duplicate the documentation for the underlying PyTorch C++ library. For fundamental operations involving <code>torch::Tensor</code>, <code>torch::autograd</code>, standard optimizers, and the base <code>torch::nn</code> modules (like <code>torch::nn::Linear</code>, <code>torch::nn::Conv2d</code>, <code>torch::nn::ReLU</code>), please refer to the official PyTorch C++ documentation.</p> <p>A good mental model is: - LibTorch provides the core building blocks (tensors, layers, autograd). - xTorch provides the complete toolkit to assemble those blocks into production-ready workflows (trainers, data loaders, pre-built models, transforms).</p>"},{"location":"api/#library-modules","title":"Library Modules","text":"<p>The xTorch API is organized into a series of modules, each corresponding to a specific part of the machine learning workflow. Use the links below to navigate to the detailed documentation for each component.</p> <ul> <li> <p>Activations: A comprehensive collection of modern and experimental activation functions to introduce non-linearity into your models.</p> </li> <li> <p>DataLoaders: High-performance, easy-to-use utilities for batching, shuffling, and parallel loading of datasets.</p> </li> <li> <p>Datasets: A collection of built-in dataset handlers for various domains (vision, NLP, audio) and general-purpose classes like <code>ImageFolder</code> for loading custom data.</p> </li> <li> <p>Dropouts: An extensive library of advanced dropout techniques for model regularization, going far beyond standard dropout.</p> </li> <li> <p>Losses: A collection of specialized loss functions for tasks like metric learning, object detection, and robust training.</p> </li> <li> <p>Models: A model zoo containing pre-built, ready-to-use implementations of popular and state-of-the-art architectures.</p> </li> <li> <p>Normalizations: Implementations of various normalization layers beyond standard <code>BatchNorm</code>, such as <code>LayerNorm</code>, <code>InstanceNorm</code>, and more experimental variants.</p> </li> <li> <p>Optimizations: Advanced and recent optimization algorithms to complement the standard optimizers provided by LibTorch.</p> </li> <li> <p>Regularizations: A collection of explicit regularization techniques that can be applied during training.</p> </li> <li> <p>Trainers: The core training engine, featuring the <code>Trainer</code> class and a <code>Callback</code> system to abstract and manage the training loop.</p> </li> <li> <p>Transforms: A rich library of data preprocessing and augmentation functions for images, audio, text, and other data modalities.</p> </li> <li> <p>Utilities: A set of helper functions and tools for common tasks like logging, device management, and filesystem operations.</p> </li> </ul>"},{"location":"api/activations/","title":"Activations","text":"<p>Activation functions are a critical component of neural networks, introducing non-linear properties that allow the network to learn complex patterns.</p> <p>xTorch provides access to all standard activation functions from LibTorch and dramatically expands this collection with dozens of modern and experimental alternatives.</p>"},{"location":"api/activations/#standard-libtorch-activations","title":"Standard LibTorch Activations","text":"<p>All standard PyTorch activation functions are available directly through LibTorch. You can use them either as modules (e.g., <code>torch::nn::ReLU</code>) or as functional calls (e.g., <code>torch::nn::functional::relu</code>).</p> <p>Common Examples: - <code>torch::nn::ReLU</code> / <code>torch::relu</code> - <code>torch::nn::Sigmoid</code> / <code>torch::sigmoid</code> - <code>torch::nn::Tanh</code> / <code>torch::tanh</code> - <code>torch::nn::Softmax</code> / <code>torch::softmax</code> - <code>torch::nn::LeakyReLU</code> / <code>torch::leaky_relu</code> - <code>torch::nn::GELU</code></p> <p>For a complete list and usage details, please refer to the official PyTorch C++ documentation.</p>"},{"location":"api/activations/#xtorch-extended-activations","title":"xTorch Extended Activations","text":"<p>In addition to the standard functions, xTorch includes a massive library of activation functions proposed in various research papers. This allows for easy experimentation with cutting-edge or specialized non-linearities without needing to implement them from scratch.</p>"},{"location":"api/activations/#usage","title":"Usage","text":"<p>All xTorch activations are implemented as <code>torch::nn::Module</code>s and can be found under the <code>xt::activations</code> namespace. They can be instantiated and used just like any standard <code>torch::nn</code> module.</p> <pre><code>#include &lt;xtorch/xtorch.h&gt;\n\n// Define a simple model\nstruct MyModel : torch::nn::Module {\n    torch::nn::Linear fc1{nullptr}, fc2{nullptr};\n    // Instantiate an xTorch activation function\n    xt::activations::Mish mish;\n\n    MyModel() {\n        fc1 = register_module(\"fc1\", torch::nn::Linear(784, 128));\n        // The activation is registered automatically by instantiation\n        fc2 = register_module(\"fc2\", torch::nn::Linear(128, 10));\n    }\n\n    torch::Tensor forward(torch::Tensor x) {\n        x = fc1(x);\n        // Apply the activation\n        x = mish(x);\n        x = fc2(x);\n        return x;\n    }\n};\n</code></pre>"},{"location":"api/activations/#available-activations","title":"Available Activations","text":"<p>Below is a comprehensive list of the activation functions available in the <code>xt::activations</code> module.</p> <p>Note</p> <p>Some activations may have constructor arguments for initialization parameters (e.g., <code>alpha</code> or <code>beta</code>). Refer to the corresponding header file in <code>&lt;xtorch/activations/&gt;</code> for specific details.</p> <code>AGLU</code> <code>AhaF</code> <code>AMLines</code> <code>Andhra</code> <code>Aria</code> <code>ASAF</code> <code>ASU</code> <code>CoLU</code> <code>CosLU</code> <code>CReLU</code> <code>DeLU</code> <code>DRA</code> <code>ELiSH</code> <code>ESwish</code> <code>EvoNormS0</code> <code>FEM</code> <code>GCLU</code> <code>GCU</code> <code>GEGLU</code> <code>GoLU</code> <code>Gumbel</code> <code>HardELiSH</code> <code>HardSwish</code> <code>HeLU</code> <code>Hermite</code> <code>KAF</code> <code>KAN</code> <code>LEAF</code> <code>LinComb</code> <code>Marcsinh</code> <code>MarginReLU</code> <code>Maxout</code> <code>Mish</code> <code>ModReLU</code> <code>NailOr</code> <code>NFN</code> <code>Nipuna</code> <code>NLSig</code> <code>NormLinComb</code> <code>PAU</code> <code>Phish</code> <code>PMish</code> <code>Poly</code> <code>Rational</code> <code>ReGLU</code> <code>ReLUN</code> <code>RReLU</code> <code>ScaledSoftSign</code> <code>SERF</code> <code>SeRLU</code> <code>ShiftedSoftplus</code> <code>ShiLU</code> <code>SiLU</code> <code>SIREN</code> <code>Smish</code> <code>SmoothStep</code> <code>Splash</code> <code>SquaredReLU</code> <code>SRelu</code> <code>StarReLU</code> <code>SwiGELU</code> <code>Swish</code> <code>TAAF</code> <code>TanhExp</code>"},{"location":"api/dataloaders/","title":"DataLoaders","text":"<p>A <code>DataLoader</code> is a crucial utility that wraps a <code>Dataset</code> and provides an iterable over it. Its primary responsibilities are to handle batching, shuffling, and multi-process data loading, ensuring that the GPU is fed with data efficiently without becoming a bottleneck.</p> <p>While LibTorch provides a basic data loading API (<code>torch::data::DataLoader</code>), it can be complex to use and lacks some of the convenient features found in Python's <code>torch.utils.data.DataLoader</code>.</p> <p>xTorch simplifies and enhances this process with its own high-performance implementations.</p>"},{"location":"api/dataloaders/#xtdataloadersextendeddataloader","title":"<code>xt::dataloaders::ExtendedDataLoader</code>","text":"<p>The <code>ExtendedDataLoader</code> is the primary, high-level data loader in xTorch. It is designed to be both easy to use and highly performant, mirroring the functionality and simplicity of its Python counterpart.</p> <p>It abstracts away the complexity of parallel data fetching and provides a simple <code>for</code> loop interface for iterating over batches of data.</p>"},{"location":"api/dataloaders/#key-features","title":"Key Features","text":"<ul> <li>Simple API: Requires minimal setup and configuration.</li> <li>Multi-Process Data Loading: Uses multiple worker processes to load data in parallel, preventing CPU bottlenecks.</li> <li>Automatic Batching: Combines individual data samples into batches.</li> <li>Optional Shuffling: Can automatically shuffle the data at the beginning of each epoch.</li> <li>Prefetching: Pre-fetches batches in the background to keep the GPU saturated.</li> </ul>"},{"location":"api/dataloaders/#usage","title":"Usage","text":"<p>The <code>ExtendedDataLoader</code> is typically initialized with a dataset object and configuration options. It can then be used in a range-based <code>for</code> loop to retrieve data batches.</p> <pre><code>#include &lt;xtorch/xtorch.h&gt;\n\nint main() {\n    // 1. Assume 'dataset' is an initialized xt::datasets::Dataset object\n    auto dataset = xt::datasets::MNIST(\"./data\", xt::datasets::DataMode::TRAIN);\n\n    // 2. Instantiate the ExtendedDataLoader\n    xt::dataloaders::ExtendedDataLoader data_loader(\n        dataset,\n        /*batch_size=*/64,\n        /*shuffle=*/true,\n        /*num_workers=*/4,\n        /*prefetch_factor=*/2\n    );\n\n    // 3. Iterate over the data loader to get batches\n    torch::Device device(torch::kCUDA);\n    int batch_count = 0;\n    for (auto&amp; batch : data_loader) {\n        // Each 'batch' is a pair of (data, target) tensors\n        torch::Tensor data = batch.first.to(device);\n        torch::Tensor target = batch.second.to(device);\n\n        if (batch_count == 0) {\n            std::cout &lt;&lt; \"Batch Data Shape: \" &lt;&lt; data.sizes() &lt;&lt; std::endl;\n            std::cout &lt;&lt; \"Batch Target Shape: \" &lt;&lt; target.sizes() &lt;&lt; std::endl;\n        }\n        batch_count++;\n    }\n    std::cout &lt;&lt; \"Total batches: \" &lt;&lt; batch_count &lt;&lt; std::endl;\n}\n</code></pre>"},{"location":"api/dataloaders/#constructor-parameters","title":"Constructor Parameters","text":"<p>The <code>ExtendedDataLoader</code> is configured through its constructor:</p> <p><code>ExtendedDataLoader(Dataset&amp; dataset, size_t batch_size, bool shuffle = false, int num_workers = 0, int prefetch_factor = 2)</code></p> Parameter Type Description <code>dataset</code> <code>xt::datasets::Dataset&amp;</code> The dataset from which to load the data. <code>batch_size</code> <code>size_t</code> The number of samples per batch. <code>shuffle</code> <code>bool</code> If <code>true</code>, the data is reshuffled at every epoch. Defaults to <code>false</code>. <code>num_workers</code> <code>int</code> The number of subprocesses to use for data loading. <code>0</code> means that the data will be loaded in the main process. Defaults to <code>0</code>. <code>prefetch_factor</code> <code>int</code> The number of batches to prefetch in advance for each worker. This helps hide data loading latency. Defaults to <code>2</code>."},{"location":"api/dataloaders/#integration-with-xttrainer","title":"Integration with <code>xt::Trainer</code>","text":"<p>The <code>ExtendedDataLoader</code> is designed to work seamlessly with the <code>xt::Trainer</code>. You simply pass your initialized data loader instances to the <code>trainer.fit()</code> method.</p> <pre><code>// Assume model, optimizer, train_loader, and val_loader are initialized\nxt::Trainer trainer;\ntrainer.set_max_epochs(10)\n       .set_optimizer(optimizer)\n       .set_loss_fn(torch::nll_loss);\n\n// The trainer will automatically iterate over the data loaders\ntrainer.fit(model, train_loader, &amp;val_loader, device);\n</code></pre> <p>For most use cases, the <code>xt::dataloaders::ExtendedDataLoader</code> is the recommended and only data loader you will need.</p>"},{"location":"api/dropouts/","title":"Dropouts","text":"<p>Dropout is a powerful regularization technique used to prevent overfitting in neural networks. During training, it randomly sets a fraction of input units to zero at each update, which helps the model learn more robust features that are not overly dependent on any single neuron.</p>"},{"location":"api/dropouts/#standard-libtorch-dropouts","title":"Standard LibTorch Dropouts","text":"<p>LibTorch provides the most common dropout implementations, which are fully supported and can be used in any xTorch model.</p> <p>Standard Modules: - <code>torch::nn::Dropout</code>: Randomly zeroes entire elements. - <code>torch::nn::Dropout2d</code>: Randomly zeroes entire channels of a 2D feature map. - <code>torch::nn::Dropout3d</code>: Randomly zeroes entire channels of a 3D feature map.</p> <p>For detailed usage, please see the official PyTorch C++ API documentation.</p>"},{"location":"api/dropouts/#xtorch-extended-dropouts","title":"xTorch Extended Dropouts","text":"<p>Experimenting with different regularization strategies is key to achieving state-of-the-art performance. To facilitate this, xTorch provides a comprehensive library of advanced and specialized dropout variants proposed in recent research.</p> <p>These implementations allow you to easily swap out standard dropout with more sophisticated techniques like <code>DropBlock</code>, <code>ScheduledDropPath</code>, or <code>VariationalDropout</code>.</p>"},{"location":"api/dropouts/#usage","title":"Usage","text":"<p>All xTorch dropout modules are located in the <code>xt::dropouts</code> namespace. They are implemented as <code>torch::nn::Module</code>s and can be integrated into your models just like <code>torch::nn::Dropout</code>.</p> <p>Most dropout layers require a dropout probability <code>p</code> during construction.</p> <pre><code>#include &lt;xtorch/xtorch.h&gt;\n\n// Define a model with an advanced dropout variant\nstruct MyModel : torch::nn::Module {\n    torch::nn::Linear fc1{nullptr}, fc2{nullptr};\n    // Instantiate DropBlock with a probability of 0.5 and block size of 7\n    xt::dropouts::DropBlock drop_block;\n\n    MyModel()\n        : drop_block(xt::dropouts::DropBlockOptions(0.5).block_size(7))\n    {\n        fc1 = register_module(\"fc1\", torch::nn::Linear(784, 128));\n        // Register the dropout module\n        register_module(\"drop_block\", drop_block);\n        fc2 = register_module(\"fc2\", torch::nn::Linear(128, 10));\n    }\n\n    torch::Tensor forward(torch::Tensor x) {\n        x = torch::relu(fc1(x));\n        // Apply dropout during training\n        x = drop_block(x);\n        x = fc2(x);\n        return x;\n    }\n};\n\nint main() {\n    MyModel model;\n    // Set the model to training mode to enable dropout\n    model.train();\n    auto input = torch::randn({32, 784});\n    auto output = model.forward(input);\n    std::cout &lt;&lt; \"Output shape: \" &lt;&lt; output.sizes() &lt;&lt; std::endl;\n}\n</code></pre> <p>Training vs. Evaluation Mode</p> <p>Dropout is only active during training. Always remember to call <code>model.eval()</code> before running inference to automatically disable all dropout layers in the model.</p>"},{"location":"api/dropouts/#available-dropouts","title":"Available Dropouts","text":"<p>Below is the list of specialized dropout implementations available in the <code>xt::dropouts</code> module.</p> <code>AdaptiveDropout</code> <code>AttentionDropout</code> <code>AutoDropout</code> <code>BandDropout</code> <code>CheckerboardDropout</code> <code>ConcreteDropout</code> <code>CurriculumDropout</code> <code>DropBlock</code> <code>DropConnect</code> <code>DropPath</code> <code>DropPathway</code> <code>EarlyDropout</code> <code>EmbeddingDropout</code> <code>FraternalDropout</code> <code>GaussianDropout</code> <code>GradDrop</code> <code>LayerDrop</code> <code>MonteCarloDropout</code> <code>RecurrentDropout</code> <code>RNNDrop</code> <code>ScheduledDropPath</code> <code>SensorDropout</code> <code>ShakeDrop</code> <code>SpatialDropout</code> <code>SpectralDropout</code> <code>TargetedDropout</code> <code>TemporalDropout</code> <code>VariationalDropout</code> <code>VariationalGaussianDropout</code> <code>ZoneOut</code> <p>Constructor Options</p> <p>Many dropout variants have unique parameters (e.g., <code>block_size</code> for <code>DropBlock</code>). Refer to the corresponding header file in <code>&lt;xtorch/dropouts/&gt;</code> for the specific <code>Options</code> struct and available settings.</p>"},{"location":"api/losses/","title":"Loss Functions","text":"<p>A loss function (or criterion) is a fundamental component of training a neural network. It calculates a single value that measures how well the model's prediction matches the target label. The goal of training is to minimize this value.</p>"},{"location":"api/losses/#standard-libtorch-losses","title":"Standard LibTorch Losses","text":"<p>LibTorch provides a comprehensive set of standard loss functions that are sufficient for most common tasks. These can be used directly within any xTorch project.</p> <p>Common Examples: - <code>torch::nn::MSELoss</code>: Mean Squared Error, for regression. - <code>torch::nn::L1Loss</code>: Mean Absolute Error, for regression. - <code>torch::nn::CrossEntropyLoss</code>: For multi-class classification. - <code>torch::nn::BCELoss</code>: Binary Cross Entropy, for binary classification. - <code>torch::nll_loss</code>: Negative Log Likelihood Loss, often used with <code>LogSoftmax</code>.</p> <p>For a complete list and usage instructions, please refer to the official PyTorch C++ API documentation.</p>"},{"location":"api/losses/#xtorch-extended-losses","title":"xTorch Extended Losses","text":"<p>For more advanced or specialized tasks\u2014such as object detection, metric learning, or dealing with class imbalance\u2014standard loss functions may not be optimal. To address this, xTorch provides a rich library of modern and specialized loss functions from the research literature.</p> <p>These implementations are ready to use, allowing you to easily experiment with advanced training objectives.</p>"},{"location":"api/losses/#usage","title":"Usage","text":"<p>All xTorch loss functions are implemented as <code>torch::nn::Module</code>s and are located in the <code>xt::losses</code> namespace. They can be instantiated and used just like any standard <code>torch::nn</code> module.</p> <p>They integrate seamlessly with the <code>xt::Trainer</code>.</p> <pre><code>#include &lt;xtorch/xtorch.hh&gt;\n\nint main() {\n    // Assume we have model outputs and targets\n    auto output = torch::randn({16, 10}); // Batch of 16, 10 classes\n    auto target = torch::randint(0, 10, {16}); // 16 target labels\n\n    // --- Standalone Usage ---\n    // Instantiate a loss function with its options\n    xt::losses::FocalLoss focal_loss(xt::losses::FocalLossOptions().gamma(2.0));\n    // Calculate the loss\n    torch::Tensor loss = focal_loss(output, target);\n    std::cout &lt;&lt; \"Calculated Focal Loss: \" &lt;&lt; loss.item&lt;float&gt;() &lt;&lt; std::endl;\n\n\n    // --- Integration with xt::Trainer ---\n    xt::Trainer trainer;\n    // The trainer can accept any callable, including our instantiated module\n    trainer.set_loss_fn(focal_loss);\n\n    // Now, when trainer.fit() is called, it will use the FocalLoss internally.\n    // trainer.fit(model, data_loader, nullptr, device);\n}\n</code></pre>"},{"location":"api/losses/#available-loss-functions","title":"Available Loss Functions","text":"<p>Below is the comprehensive list of specialized loss functions available in the <code>xt::losses</code> module.</p> <code>AdaptiveLoss</code> <code>ArcFaceLoss</code> <code>BalancedL1Loss</code> <code>CycleConsistencyLoss</code> <code>DHELLoss</code> <code>DiceBCELoss</code> <code>DiceLoss</code> <code>DSAMLoss</code> <code>DualSoftmaxLoss</code> <code>DynamicSmoothL1Loss</code> <code>EarlyExitingLoss</code> <code>ElasticFace</code> <code>FlipLoss</code> <code>FocalLoss</code> <code>GANHingeLoss</code> <code>GANLeastSquaresLoss</code> <code>GeneralizedFocalLoss</code> <code>GHMCLoss</code> <code>GHMRLoss</code> <code>HappierLoss</code> <code>HBMLoss</code> <code>InfoNCELoss</code> <code>LovaszSoftmaxLoss</code> <code>MetricMixupLoss</code> <code>MultiLoss</code> <code>NTXentLoss</code> <code>ObjectAwareLoss</code> <code>PIOULoss</code> <code>ProxyAnchorLoss</code> <code>RankBasedLoss</code> <code>SeesawLoss</code> <code>SelfAdjustingSmoothL1Loss</code> <code>SupervisedContrastiveLoss</code> <code>TripletEntropyLoss</code> <code>TripletLoss</code> <code>UnsupervisedFeatureLoss</code> <code>UPITLoss</code> <code>VarifocalLoss</code> <code>WGANGPLoss</code> <code>ZLPRLoss</code> <p>Constructor Options</p> <p>Many of these loss functions have tunable hyperparameters (like <code>gamma</code> in <code>FocalLoss</code> or <code>margin</code> in <code>TripletLoss</code>). These are configured via an <code>Options</code> struct passed to the constructor. Please refer to the specific header file in <code>&lt;xtorch/losses/&gt;</code> for details on the available settings.</p>"},{"location":"api/normalizations/","title":"Normalization Layers","text":"<p>Normalization layers are a crucial component in modern deep neural networks. They help stabilize the learning process, reduce internal covariate shift, and often lead to faster convergence and better generalization.</p>"},{"location":"api/normalizations/#standard-libtorch-normalizations","title":"Standard LibTorch Normalizations","text":"<p>LibTorch provides a solid foundation with the most widely-used normalization layers. These are the go-to choices for many standard architectures.</p> <p>Common Examples: - <code>torch::nn::BatchNorm1d</code>, <code>torch::nn::BatchNorm2d</code>, <code>torch::nn::BatchNorm3d</code> - <code>torch::nn::LayerNorm</code> - <code>torch::nn::InstanceNorm1d</code>, <code>torch::nn::InstanceNorm2d</code>, <code>torch::nn::InstanceNorm3d</code> - <code>torch::nn::GroupNorm</code> - <code>torch::nn::LocalResponseNorm</code></p> <p>For detailed usage of these standard layers, please refer to the official PyTorch C++ API documentation.</p>"},{"location":"api/normalizations/#xtorch-extended-normalizations","title":"xTorch Extended Normalizations","text":"<p>To enable research and development with cutting-edge architectures, xTorch provides an extensive collection of advanced and specialized normalization techniques proposed in the literature.</p> <p>These implementations allow you to move beyond <code>BatchNorm</code> and experiment with alternatives like <code>EvoNorms</code>, <code>FilterResponseNormalization</code>, or <code>WeightStandardization</code> with ease.</p>"},{"location":"api/normalizations/#usage","title":"Usage","text":"<p>All xTorch normalization layers are located in the <code>xt::normalizations</code> namespace. They are implemented as <code>torch::nn::Module</code>s and can be integrated directly into your model definitions, just like any standard <code>torch::nn</code> module.</p> <pre><code>#include &lt;xtorch/xtorch.hh&gt;\n\n// A simple convolutional block using Filter Response Normalization\nstruct ConvBlock : torch::nn::Module {\n    torch::nn::Conv2d conv{nullptr};\n    // Instantiate the FRN layer for 64 channels\n    xt::normalizations::FilterResponseNormalization frn;\n    xt::activations::Mish mish;\n\n    ConvBlock(int in_channels, int out_channels)\n        : frn(xt::normalizations::FilterResponseNormalizationOptions(out_channels))\n    {\n        conv = register_module(\"conv\", torch::nn::Conv2d(\n            torch::nn::Conv2dOptions(in_channels, out_channels, 3).padding(1)\n        ));\n        // Register the normalization layer\n        register_module(\"frn\", frn);\n    }\n\n    torch::Tensor forward(torch::Tensor x) {\n        x = conv(x);\n        // Apply normalization, then activation\n        x = frn(x);\n        x = mish(x);\n        return x;\n    }\n};\n</code></pre> <p>Training vs. Evaluation Mode</p> <p>Like <code>BatchNorm</code>, many normalization layers have different behaviors during training (e.g., updating running statistics) and evaluation. Always remember to call <code>model.train()</code> or <code>model.eval()</code> to switch the model to the appropriate mode.</p>"},{"location":"api/normalizations/#available-normalization-layers","title":"Available Normalization Layers","text":"<p>Below is the list of normalization techniques available in the <code>xt::normalizations</code> module.</p> <code>ActivationNormalization</code> <code>AdaptiveInstanceNormalization</code> <code>AttentiveNormalization</code> <code>BatchChannelNormalization</code> <code>CincFlow</code> <code>ConditionalBatchNormalization</code> <code>ConditionalInstanceNormalization</code> <code>CosineNormalization</code> <code>CrossNorm</code> <code>DecorrelatedBatchNormalization</code> <code>EvoNorms</code> <code>FilterResponseNormalization</code> <code>GradientNormalization</code> <code>InPlaceABN</code> <code>InstanceLevelMetaNormalization</code> <code>LayerScale</code> <code>LocalContrastNormalization</code> <code>MixtureNormalization</code> <code>ModeNormalization</code> <code>MPNNormalization</code> <code>OnlineNormalization</code> <code>PixelNormalization</code> <code>PowerNormalization</code> <code>ReZero</code> <code>SABN</code> <code>SelfNorm</code> <code>SPADE</code> <code>SparseSwitchableNormalization</code> <code>SpectralNormalization</code> <code>SRN</code> <code>SwitchableNormalization</code> <code>SyncBN</code> <code>VirtualBatchNormalization</code> <code>WeightDemodulation</code> <code>WeightNormalization</code> <code>WeightStandardization</code> <p>Constructor Options</p> <p>Many of these layers require specific arguments during construction, such as the number of channels or features. These are configured via an <code>Options</code> struct passed to the constructor. Please refer to the specific header file in <code>&lt;xtorch/normalizations/&gt;</code> for details on the available settings for each layer.</p>"},{"location":"api/optimizations/","title":"Optimizers","text":"<p>An optimizer is an algorithm that adapts the neural network's attributes, such as weights and learning rates, to minimize the loss function. The choice of optimizer can have a significant impact on training speed and final model performance.</p>"},{"location":"api/optimizations/#standard-libtorch-optimizers","title":"Standard LibTorch Optimizers","text":"<p>LibTorch provides a robust set of the most common and well-established optimization algorithms, which are suitable for a wide range of tasks.</p> <p>Common Examples: - <code>torch::optim::SGD</code> - <code>torch::optim::Adam</code> - <code>torch::optim::RMSprop</code> - <code>torch::optim::Adagrad</code></p> <p>These optimizers are used by passing the model's parameters and an <code>Options</code> struct to their constructor. For a complete guide, please refer to the official PyTorch C++ optimizer documentation.</p>"},{"location":"api/optimizations/#xtorch-extended-optimizers","title":"xTorch Extended Optimizers","text":"<p>The field of optimization is an active area of research, with new and improved algorithms being published regularly. To empower developers and researchers to leverage these advancements, xTorch includes a massive library of modern and specialized optimizers.</p> <p>This allows you to easily replace <code>Adam</code> with variants like <code>RAdam</code>, <code>AdamW</code>, <code>AdaBelief</code>, or <code>LAMB</code> to see if they improve your model's convergence or generalization.</p>"},{"location":"api/optimizations/#usage","title":"Usage","text":"<p>All xTorch optimizers are located in the <code>xt::optimizations</code> namespace. They are designed to be a drop-in replacement for standard <code>torch::optim</code> optimizers. You construct them in the same way: by providing the model's parameters and an options struct.</p> <p>They integrate perfectly with the <code>xt::Trainer</code>.</p> <pre><code>#include &lt;xtorch/xtorch.hh&gt;\n\nint main() {\n    // 1. Assume 'model' is an initialized torch::nn::Module\n    xt::models::LeNet5 model(10);\n    model.to(torch::kCPU);\n\n    // 2. Instantiate an xTorch optimizer\n    // It takes the model parameters and an Options struct, just like a standard optimizer.\n    xt::optimizations::RAdam optimizer(\n        model.parameters(),\n        xt::optimizations::RAdamOptions(1e-3) // Learning rate of 0.001\n    );\n\n    // 3. Integrate with the xt::Trainer\n    xt::Trainer trainer;\n    trainer.set_max_epochs(10)\n           .set_optimizer(optimizer) // Pass the xTorch optimizer to the trainer\n           .set_loss_fn(torch::nll_loss);\n\n    // The trainer will now use RAdam to update the model's weights.\n    // trainer.fit(model, data_loader, nullptr, torch::kCPU);\n\n    std::cout &lt;&lt; \"Trainer configured with RAdam optimizer.\" &lt;&lt; std::endl;\n}\n</code></pre>"},{"location":"api/optimizations/#available-optimizers","title":"Available Optimizers","text":"<p>Below is the comprehensive list of optimizers available in the <code>xt::optimizations</code> module.</p> <code>OneBitAdam</code> <code>OneBitLamb</code> <code>AdaBelief</code> <code>AdaBound</code> <code>Adafactor</code> <code>AdaFisher</code> <code>AdaHessian</code> <code>AdaMax</code> <code>AdamMini</code> <code>AdaMod</code> <code>AdamW</code> <code>AdaShift</code> <code>AdaSmooth</code> <code>AdaSqrt</code> <code>ADOPT</code> <code>AggMo</code> <code>AMSBound</code> <code>AMSGrad</code> <code>AO</code> <code>Apollo</code> <code>Atmo</code> <code>DeepEnsembles</code> <code>DemonAdam</code> <code>DemonCM</code> <code>Demon</code> <code>DFA</code> <code>DiagAdaFisher</code> <code>DistributedShampoo</code> <code>DSPT</code> <code>ECO</code> <code>FA</code> <code>FASFA</code> <code>FATA</code> <code>ForwardGradient</code> <code>GCANS</code> <code>GradientCheckpointing</code> <code>GradientSparsification</code> <code>Gravity</code> <code>HGS</code> <code>Info</code> <code>KP</code> <code>LAMB</code> <code>LARS</code> <code>LocalSGD</code> <code>Lookahead</code> <code>MadGrad</code> <code>MAS</code> <code>MPSO</code> <code>Nadam</code> <code>NTASGD</code> <code>PLO</code> <code>PO</code> <code>PowerPropagation</code> <code>PowerSGD</code> <code>QHAdam</code> <code>QHM</code> <code>RAdam</code> <code>SLamb</code> <code>SM3</code> <code>SMA</code> <code>SRMM</code> <code>StochasticWeightAveraging</code> <code>YellowFin</code> <p>Constructor Options</p> <p>Each optimizer has its own set of hyperparameters (e.g., <code>lr</code>, <code>betas</code>, <code>eps</code>, <code>weight_decay</code>). These are configured via a dedicated <code>Options</code> struct passed to the constructor. Please refer to the specific header file in <code>&lt;xtorch/optimizations/&gt;</code> for details on the available settings for each optimizer.</p>"},{"location":"api/regularizations/","title":"Regularization Techniques","text":"<p>Regularization refers to a collection of techniques designed to prevent a model from overfitting the training data. By adding a penalty for model complexity, regularization helps the model generalize better to unseen data.</p>"},{"location":"api/regularizations/#standard-regularization-in-libtorch","title":"Standard Regularization in LibTorch","text":"<p>The most common forms of regularization are readily available when using LibTorch and are standard practice in deep learning.</p> <ol> <li> <p>Weight Decay (L2 Regularization): This is the most common technique. It is not a separate module but rather an option built directly into optimizers. You can enable it by setting the <code>weight_decay</code> parameter in the optimizer's options.</p> <pre><code>// Enable weight decay in the Adam optimizer\ntorch::optim::Adam optimizer(\n    model.parameters(),\n    torch::optim::AdamOptions(1e-3).weight_decay(1e-4) // L2 penalty\n);\n</code></pre> </li> <li> <p>Dropout: This technique randomly zeroes out activations during training. It is a powerful regularizer implemented as a set of modules. See the dedicated Dropouts page for a comprehensive list of variants.</p> </li> </ol>"},{"location":"api/regularizations/#xtorch-extended-regularization-techniques","title":"xTorch Extended Regularization Techniques","text":"<p>Beyond weight decay and dropout, there is a wide range of explicit regularization methods that can be applied to activations, weights, or the loss function itself. xTorch provides a rich collection of these techniques, allowing for advanced experimentation.</p>"},{"location":"api/regularizations/#usage","title":"Usage","text":"<p>Most xTorch regularization techniques are implemented as <code>torch::nn::Module</code>s and are located in the <code>xt::regulariztions</code> namespace. They can be applied in your model's forward pass or used to wrap your loss function.</p> <p>A common example is <code>LabelSmoothing</code>, which helps prevent the model from becoming overconfident in its predictions.</p> <pre><code>#include &lt;xtorch/xtorch.hh&gt;\n\nint main() {\n    // Assume we have model outputs and targets\n    auto logits = torch::randn({16, 10}); // Raw outputs (logits)\n    auto targets = torch::randint(0, 10, {16});\n\n    // 1. Define a standard loss function\n    torch::nn::CrossEntropyLoss cross_entropy_loss;\n\n    // 2. Instantiate the LabelSmoothing regularizer\n    // Epsilon is the smoothing factor.\n    xt::regulariztions::LabelSmoothing label_smoother(\n        xt::regulariztions::LabelSmoothingOptions(0.1) // 10% smoothing\n    );\n\n    // 3. Apply label smoothing to the loss calculation\n    // This typically involves combining the smoother's output with the standard loss.\n    // The exact usage may vary, so always check the header.\n    // For LabelSmoothing, it acts as a loss itself.\n    torch::Tensor smoothed_loss = label_smoother(logits, targets);\n\n    std::cout &lt;&lt; \"Standard Cross Entropy Loss: \"\n              &lt;&lt; cross_entropy_loss(logits, targets).item&lt;float&gt;() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Loss with Label Smoothing: \"\n              &lt;&lt; smoothed_loss.item&lt;float&gt;() &lt;&lt; std::endl;\n\n    // 4. In a Trainer, you would set this as your loss function\n    xt::Trainer trainer;\n    trainer.set_loss_fn(label_smoother);\n    // trainer.fit(...);\n}\n</code></pre>"},{"location":"api/regularizations/#available-regularization-techniques","title":"Available Regularization Techniques","text":"<p>Below is the list of regularization modules available in the <code>xt::regulariztions</code> module.</p> <code>ActivationRegularization</code> <code>ALS</code> <code>AuxiliaryBatchNormalization</code> <code>BatchNuclearNormMaximization</code> <code>DiscriminativeRegularization</code> <code>EntropyRegularization</code> <code>EuclideanNormRegularization</code> <code>Fierce</code> <code>GANFeatureMatching</code> <code>GMVAE</code> <code>LabelSmoothing</code> <code>LayerScale</code> <code>LCC</code> <code>LVR</code> <code>ManifoldMixup</code> <code>OffDiagonalOrthogonalRegularization</code> <code>OrthogonalRegularization</code> <code>PathLengthRegularization</code> <code>PGM</code> <code>R1Regularization</code> <code>Rome</code> <code>SCN</code> <code>ShakeShakeRegularization</code> <code>SRN</code> <code>StochasticDepth</code> <code>STTP</code> <code>SVDParameterization</code> <code>TargetPolicySmoothing</code> <code>TemporalActivationRegularization</code> <code>WeightsReset</code> <p>Implementation Details</p> <p>The implementation of regularization techniques can vary greatly. Some act like loss functions, others are modules to be applied in the <code>forward</code> pass, and some might be implemented as training callbacks. Always refer to the specific header file in <code>&lt;xtorch/regulariztions/&gt;</code> for detailed usage instructions and available options.</p>"},{"location":"api/trainers/","title":"Trainers and Callbacks","text":"<p>The <code>Trainer</code> module is the centerpiece of xTorch's high-level API. It encapsulates the entire training and validation loop, abstracting away the boilerplate code required to iterate over data, perform forward and backward passes, update model weights, and run validation checks.</p> <p>This allows you to focus on the high-level architecture of your model and experiment, rather than the low-level mechanics of the training process.</p>"},{"location":"api/trainers/#xttrainer","title":"<code>xt::Trainer</code>","text":"<p>The <code>xt::Trainer</code> class is the main engine for model training. It is designed with a fluent, chainable interface (a builder pattern) that makes configuration clean and readable.</p>"},{"location":"api/trainers/#core-responsibilities","title":"Core Responsibilities","text":"<p>The <code>Trainer</code> handles all of the following automatically: -   Iterating over the dataset for a specified number of epochs. -   Iterating over batches from the <code>DataLoader</code>. -   Moving data and models to the correct device (<code>CPU</code> or <code>CUDA</code>). -   Setting the model to the correct mode (<code>train()</code> or <code>eval()</code>). -   Zeroing gradients (<code>optimizer.zero_grad()</code>). -   Performing the forward pass (<code>model.forward(data)</code>). -   Calculating the loss. -   Performing the backward pass (<code>loss.backward()</code>). -   Updating the model's weights (<code>optimizer.step()</code>). -   Executing custom logic at specific points via a callback system.</p>"},{"location":"api/trainers/#configuration","title":"Configuration","text":"<p>You configure a <code>Trainer</code> instance by chaining its setter methods.</p> Method Description <code>set_max_epochs(int epochs)</code> Required. Sets the total number of epochs to train for. <code>set_optimizer(torch::optim::Optimizer&amp; optim)</code> Required. Sets the optimizer to use for updating weights. <code>set_loss_fn(LossFn loss_fn)</code> Required. Sets the loss function. This can be a <code>torch::nn::Module</code> (like <code>torch::nn::CrossEntropyLoss</code>) or a lambda function. <code>add_callback(std::shared_ptr&lt;Callback&gt; cb)</code> Optional. Adds a callback to inject custom logic into the training loop."},{"location":"api/trainers/#execution","title":"Execution","text":"<p>Once configured, you start the training process by calling the <code>fit()</code> method.</p> <p><code>fit(torch::nn::Module&amp; model, dataloaders::ExtendedDataLoader&amp; train_loader, dataloaders::ExtendedDataLoader* val_loader, torch::Device device)</code></p> Parameter Type Description <code>model</code> <code>torch::nn::Module&amp;</code> The model to be trained. <code>train_loader</code> <code>ExtendedDataLoader&amp;</code> The data loader for the training dataset. <code>val_loader</code> <code>ExtendedDataLoader*</code> Optional. A pointer to the data loader for the validation dataset. If provided (<code>nullptr</code> otherwise), a validation loop will be run at the end of each epoch. <code>device</code> <code>torch::Device</code> The device (<code>torch::kCPU</code> or <code>torch::kCUDA</code>) on which to run the training."},{"location":"api/trainers/#callbacks","title":"Callbacks","text":"<p>Callbacks are the primary mechanism for extending the <code>Trainer</code>'s functionality. A callback is an object that can perform actions at various stages of the training loop (e.g., at the end of an epoch, at the beginning of a batch).</p> <p>This powerful system allows you to add custom logic for: -   Logging metrics to the console or a file. -   Saving model checkpoints. -   Implementing early stopping. -   Adjusting the learning rate.</p>"},{"location":"api/trainers/#creating-a-custom-callback","title":"Creating a Custom Callback","text":"<p>To create your own callback, you inherit from the base class <code>xt::Callback</code> and override any of its virtual methods.</p> <p>Available Hooks (Methods to Override): - <code>on_train_begin()</code> - <code>on_train_end()</code> - <code>on_epoch_begin()</code> - <code>on_epoch_end()</code> - <code>on_batch_begin()</code> - <code>on_batch_end()</code></p>"},{"location":"api/trainers/#built-in-callbacks","title":"Built-in Callbacks","text":"<p>xTorch provides a set of common callbacks to handle standard tasks.</p>"},{"location":"api/trainers/#xtloggingcallback","title":"<code>xt::LoggingCallback</code>","text":"<p>This is the most essential callback. It prints a formatted progress log to the console, showing the current epoch, batch, loss, and timing information.</p> <p>Constructor: <code>LoggingCallback(std::string name, int log_every_N_batches = 50, bool log_time = true)</code></p>"},{"location":"api/trainers/#complete-usage-example","title":"Complete Usage Example","text":"<p>This snippet demonstrates how all the pieces fit together.</p> <pre><code>#include &lt;xtorch/xtorch.h&gt;\n\nint main() {\n    // 1. Initialize Model, DataLoaders, and Optimizer\n    xt::models::LeNet5 model(10);\n    torch::Device device(torch::kCUDA);\n    model.to(device);\n\n    auto dataset = xt::datasets::MNIST(\"./data\");\n    xt::dataloaders::ExtendedDataLoader train_loader(dataset, 64, true);\n\n    torch::optim::Adam optimizer(model.parameters(), torch::optim::AdamOptions(1e-3));\n\n    // 2. Create a Logging Callback\n    auto logger = std::make_shared&lt;xt::LoggingCallback&gt;(\"[MNIST-TRAIN]\", /*log_every*/ 100);\n\n    // 3. Instantiate and Configure the Trainer\n    xt::Trainer trainer;\n    trainer.set_max_epochs(10)\n           .set_optimizer(optimizer)\n           .set_loss_fn(torch::nll_loss) // Using a standard functional loss\n           .add_callback(logger);        // Add the logger\n\n    // 4. Start the training process\n    trainer.fit(model, train_loader, /*val_loader=*/nullptr, device);\n\n    std::cout &lt;&lt; \"Training complete.\" &lt;&lt; std::endl;\n}\n</code></pre>"},{"location":"api/utils/","title":"Utilities","text":"<p>The <code>xt::utils</code> namespace provides a collection of standalone helper functions and classes for common tasks that frequently appear in deep learning workflows.</p> <p>These utilities are designed to reduce boilerplate and simplify operations related to file handling, data conversion, and model inference.</p>"},{"location":"api/utils/#model-serialization-and-inference","title":"Model Serialization and Inference","text":"<p>These functions streamline the process of saving, loading, and running inference with models.</p>"},{"location":"api/utils/#xtload_modelconst-stdstring-model_path","title":"<code>xt::load_model(const std::string&amp; model_path)</code>","text":"<p>Loads a TorchScript model from a file.</p> <ul> <li>Parameters:<ul> <li><code>model_path</code>: The file path to the serialized <code>.pt</code> or <code>.pth</code> model.</li> </ul> </li> <li>Returns: A <code>torch::jit::script::Module</code> ready for inference.</li> </ul>"},{"location":"api/utils/#xtutilspredicttorchjitscriptmodule-model-torchtensor-tensor","title":"<code>xt::utils::predict(torch::jit::script::Module&amp; model, torch::Tensor&amp; tensor)</code>","text":"<p>Performs a forward pass on a loaded TorchScript model.</p> <ul> <li>Parameters:<ul> <li><code>model</code>: The loaded TorchScript module.</li> <li><code>tensor</code>: The input tensor for the model.</li> </ul> </li> <li>Returns: An <code>at::Tensor</code> containing the model's output logits.</li> </ul>"},{"location":"api/utils/#xtutilsargmaxconst-attensor-tensor","title":"<code>xt::utils::argmax(const at::Tensor&amp; tensor)</code>","text":"<p>Finds the index of the maximum value in a 1D tensor. This is commonly used to get the predicted class index from the output logits.</p> <ul> <li>Parameters:<ul> <li><code>tensor</code>: A 1D tensor of scores or probabilities.</li> </ul> </li> <li>Returns: An <code>int</code> representing the index of the highest value.</li> </ul>"},{"location":"api/utils/#example-inference-pipeline","title":"Example Inference Pipeline","text":"<pre><code>#include &lt;xtorch/xtorch.hh&gt;\n\nint main() {\n    // 1. Load the exported TorchScript model\n    auto model = xt::load_model(\"resnet18_script.pt\");\n\n    // 2. Load and preprocess an image\n    auto tensor = xt::utils::image_to_tensor(\"input.jpg\");\n\n    // 3. Get model predictions\n    auto output_logits = xt::utils::predict(model, tensor);\n\n    // 4. Find the class with the highest score\n    int predicted_class = xt::utils::argmax(output_logits);\n\n    std::cout &lt;&lt; \"Predicted class = \" &lt;&lt; predicted_class &lt;&lt; std::endl;\n}\n</code></pre>"},{"location":"api/utils/#image-handling","title":"Image Handling","text":"<p>These utilities simplify the process of reading image files and converting them into tensors suitable for model input.</p>"},{"location":"api/utils/#xtutilsimage_to_tensorconst-stdstring-image_path","title":"<code>xt::utils::image_to_tensor(const std::string&amp; image_path)</code>","text":"<p>Reads an image from a file path and converts it into a <code>torch::Tensor</code>.</p> <ul> <li>Details: The function uses OpenCV internally. It reads the image, converts it to a <code>CHW</code> (Channels, Height, Width) tensor of type <code>kFloat</code>, and normalizes pixel values to the <code>[0, 1]</code> range. It does not add a batch dimension.</li> <li>Parameters:<ul> <li><code>image_path</code>: The file path to the image (<code>.jpg</code>, <code>.png</code>, etc.).</li> </ul> </li> <li>Returns: A 3D <code>torch::Tensor</code> with the shape <code>[Channels, Height, Width]</code>.</li> </ul>"},{"location":"api/utils/#filesystem-and-data-handling","title":"Filesystem and Data Handling","text":"<p>These utilities help manage files and directories, which is especially useful when downloading datasets or handling file paths.</p>"},{"location":"api/utils/#xtutilsdownloaderconst-stdstring-url-const-stdstring-dest_path","title":"<code>xt::utils::downloader(const std::string&amp; url, const std::string&amp; dest_path)</code>","text":"<p>Downloads a file from a URL to a specified destination.</p>"},{"location":"api/utils/#xtutilsextractconst-stdstring-archive_path-const-stdstring-dest_dir","title":"<code>xt::utils::extract(const std::string&amp; archive_path, const std::string&amp; dest_dir)</code>","text":"<p>Extracts a compressed archive (e.g., <code>.zip</code>, <code>.tar.gz</code>) to a destination directory.</p>"},{"location":"api/utils/#xtutilsmkdirconst-stdstring-dir_path","title":"<code>xt::utils::mkdir(const std::string&amp; dir_path)</code>","text":"<p>Creates a directory if it does not already exist.</p>"},{"location":"api/utils/#xtutilspath_joinconst-stdstring-p1-const-stdstring-p2","title":"<code>xt::utils::path_join(const std::string&amp; p1, const std::string&amp; p2)</code>","text":"<p>Joins two path components with the correct platform-specific separator.</p>"},{"location":"api/utils/#xtutilsmd5const-stdstring-file_path","title":"<code>xt::utils::md5(const std::string&amp; file_path)</code>","text":"<p>Computes the MD5 checksum of a file, which is useful for verifying the integrity of downloaded datasets.</p>"},{"location":"api/datasets/","title":"Datasets","text":"<p>The <code>Dataset</code> classes are responsible for accessing and preprocessing individual samples of data from a source (like a directory of images or a text file). They are the foundation of the data loading pipeline in xTorch.</p> <p>A <code>Dataset</code> object is passed to a <code>DataLoader</code>, which then handles the more complex logic of batching, shuffling, and parallel data fetching.</p>"},{"location":"api/datasets/#the-xtdatasetsdataset-base-class","title":"The <code>xt::datasets::Dataset</code> Base Class","text":"<p>All dataset classes in xTorch, both built-in and custom, inherit from a common base class: <code>xt::datasets::Dataset</code>. This class establishes a standard interface that the <code>DataLoader</code> knows how to interact with.</p> <p>The two core methods of any dataset are: -   <code>get(size_t index)</code>: Returns the data sample at the given index. This is typically a <code>torch::data::Example&lt;&gt;</code> containing a data tensor and a target tensor. -   <code>size()</code>: Returns the total number of samples in the dataset as an <code>optional&lt;size_t&gt;</code>.</p>"},{"location":"api/datasets/#standard-usage","title":"Standard Usage","text":"<p>The typical workflow involves three steps: 1.  Define Transforms: Create a pipeline of data augmentation or preprocessing steps (optional). 2.  Instantiate a Dataset: Create an instance of a specific dataset class, providing the path to the data and the transform pipeline. 3.  Pass to a DataLoader: Pass the initialized dataset object to the <code>ExtendedDataLoader</code>.</p> <pre><code>#include &lt;xtorch/xtorch.h&gt;\n\nint main() {\n    // 1. Define a transform pipeline\n    auto compose = std::make_unique&lt;xt::transforms::Compose&gt;(\n        std::make_shared&lt;xt::transforms::general::Normalize&gt;(0.5, 0.5)\n    );\n\n    // 2. Instantiate a built-in dataset for CIFAR-10\n    auto cifar_dataset = xt::datasets::CIFAR10(\n        \"./data\",\n        xt::datasets::DataMode::TRAIN,\n        /*download=*/true,\n        std::move(compose)\n    );\n\n    // The dataset can report its size\n    std::cout &lt;&lt; \"CIFAR-10 training set size: \" &lt;&lt; *cifar_dataset.size() &lt;&lt; std::endl;\n\n    // 3. Pass the dataset to a data loader\n    xt::dataloaders::ExtendedDataLoader data_loader(cifar_dataset, 64, true);\n\n    // The data loader can now be used for training\n    for (auto&amp; batch : data_loader) {\n        // ...\n    }\n}\n</code></pre>"},{"location":"api/datasets/#built-in-datasets-by-domain","title":"Built-in Datasets by Domain","text":"<p>xTorch provides an extensive collection of built-in dataset handlers for many popular public datasets, saving you the effort of writing boilerplate loading code. These are organized by machine learning domain.</p> <p>Follow the links below for a detailed list of available datasets in each category.</p> <ul> <li> <p>Computer Vision: Includes datasets for image classification (<code>MNIST</code>, <code>CIFAR</code>, <code>ImageNet</code>), object detection (<code>COCO</code>), and segmentation (<code>Cityscapes</code>).</p> </li> <li> <p>Natural Language Processing: Includes datasets for text classification (<code>IMDB</code>, <code>AG_NEWS</code>), question answering (<code>SQuAD</code>), and machine translation (<code>WMT</code>).</p> </li> <li> <p>Audio Processing: Includes datasets for speech recognition (<code>LibriSpeech</code>), sound event classification (<code>UrbanSound8K</code>), and music analysis.</p> </li> <li> <p>Time Series: Includes datasets for time series forecasting and classification.</p> </li> <li> <p>Tabular Data: Includes a variety of classic small-scale datasets for classification and regression tasks.</p> </li> <li> <p>Graph Data: Includes datasets for node and graph classification tasks (<code>Cora</code>).</p> </li> <li> <p>General Purpose: Contains flexible dataset classes like <code>ImageFolderDataset</code> and <code>CSVDataset</code> that allow you to easily load your own custom data without writing a new dataset class from scratch.</p> </li> <li> <p>Other Domains: Includes datasets from other domains like biomedical data and recommendation systems.</p> </li> </ul>"},{"location":"api/datasets/audio/","title":"Audio &amp; Speech Datasets","text":"<p>xTorch provides a rich collection of built-in dataset handlers for a wide range of audio and speech processing tasks, from recognition and classification to synthesis and source separation.</p> <p>All audio datasets are located under the <code>xt::datasets</code> namespace and can be found in the <code>&lt;xtorch/datasets/audio_processing/&gt;</code> header directory.</p>"},{"location":"api/datasets/audio/#general-usage","title":"General Usage","text":"<p>The workflow for using an audio dataset is similar to that of other domains. You typically instantiate the dataset class with the path to your data and an optional pipeline of audio-specific transformations.</p> <pre><code>#include &lt;xtorch/xtorch.h&gt;\n\nint main() {\n    // 1. Define audio-specific transformations (optional)\n    // For example, creating a Mel Spectrogram from the raw audio waveform.\n    auto transforms = std::make_unique&lt;xt::transforms::Compose&gt;(\n        std::make_shared&lt;xt::transforms::signal::MelSpectrogram&gt;()\n    );\n\n    // 2. Instantiate a dataset for the Speech Commands task\n    // The dataset will handle downloading and loading the data.\n    auto dataset = xt::datasets::SpeechCommands(\n        \"./data\",\n        xt::datasets::DataMode::TRAIN,\n        /*download=*/true,\n        std::move(transforms)\n    );\n\n    std::cout &lt;&lt; \"Speech Commands dataset size: \" &lt;&lt; *dataset.size() &lt;&lt; std::endl;\n\n    // 3. Pass the dataset to a DataLoader\n    xt::dataloaders::ExtendedDataLoader data_loader(dataset, 32, true);\n\n    // The data loader is now ready for use in a training loop\n    for (auto&amp; batch : data_loader) {\n        auto spectrograms = batch.first;\n        auto labels = batch.second;\n        // ... training step ...\n    }\n}\n</code></pre> <p>Dataset Constructors</p> <p>Most dataset constructors follow a standard pattern: <code>DatasetName(const std::string&amp; root, DataMode mode, bool download, TransformPtr transforms)</code> - <code>root</code>: The directory where the data is stored or will be downloaded. - <code>mode</code>: <code>DataMode::TRAIN</code>, <code>DataMode::TEST</code>, or <code>DataMode::VALIDATION</code>. - <code>download</code>: If <code>true</code>, the dataset will be downloaded if not found in the root directory. - <code>transforms</code>: A <code>unique_ptr</code> to a transform pipeline to be applied to the data.</p>"},{"location":"api/datasets/audio/#available-datasets-by-task","title":"Available Datasets by Task","text":""},{"location":"api/datasets/audio/#audio-event-detection","title":"Audio Event Detection","text":"Dataset Class Description Header File <code>AudioSet</code> A large-scale dataset of manually annotated audio events from YouTube videos. <code>audio_event_detection/audioset.h</code>"},{"location":"api/datasets/audio/#binary-speech-classification","title":"Binary Speech Classification","text":"Dataset Class Description Header File <code>YesNo</code> A small dataset of speech recordings saying \"yes\" or \"no\". <code>binary_speech_classification/yes_no.h</code>"},{"location":"api/datasets/audio/#emotion-recognition","title":"Emotion Recognition","text":"Dataset Class Description Header File <code>IEMOCAP</code> Interactive Emotional Dyadic Motion Capture database for emotion analysis. <code>emotion_recognition/iemocap.h</code>"},{"location":"api/datasets/audio/#environmental-sound-classification","title":"Environmental Sound Classification","text":"Dataset Class Description Header File <code>ESC</code> Dataset for Environmental Sound Classification (ESC-50 and ESC-10). <code>environmental_sound_classification/esc.h</code> <code>UrbanSound</code> The UrbanSound8K dataset, containing urban sound recordings. <code>environmental_sound_classification/urban_sound.h</code>"},{"location":"api/datasets/audio/#intent-classification","title":"Intent Classification","text":"Dataset Class Description Header File <code>Snips</code> The Snips Natural Language Understanding benchmark dataset. <code>intent_classification/snips.h</code>"},{"location":"api/datasets/audio/#music-genre-classification","title":"Music Genre Classification","text":"Dataset Class Description Header File <code>GTZAN</code> A popular dataset for music genre recognition. <code>music_genre_classification/gtzan.h</code>"},{"location":"api/datasets/audio/#music-information-retrieval","title":"Music Information Retrieval","text":"Dataset Class Description Header File <code>MillionSongDataset</code> A freely-available collection of audio features and metadata for a million contemporary popular music tracks. <code>music_information_retrieval/million_song_dataset.h</code>"},{"location":"api/datasets/audio/#music-source-separation","title":"Music Source Separation","text":"Dataset Class Description Header File <code>MUSDBHQ</code> A high-quality dataset for music source separation. <code>music_source_separation/mus_db_hq.h</code>"},{"location":"api/datasets/audio/#music-tagging","title":"Music Tagging","text":"Dataset Class Description Header File <code>MagnaTagATune</code> A dataset of audio clips with associated tags. <code>music_tagging/magna_tag_a_tune.h</code>"},{"location":"api/datasets/audio/#sound-event-detection","title":"Sound Event Detection","text":"Dataset Class Description Header File <code>FSD50K</code> An open dataset of human-labeled sound events. <code>sound_event_detection/fsd50k.h</code>"},{"location":"api/datasets/audio/#speaker-identification-and-verification","title":"Speaker Identification and Verification","text":"Dataset Class Description Header File <code>VoxCeleb</code> An audio-visual dataset consisting of short clips of human speech, extracted from interview videos uploaded to YouTube. <code>speaker_identification_and_verification/vox_celeb.h</code>"},{"location":"api/datasets/audio/#speech-command-recognition","title":"Speech Command Recognition","text":"Dataset Class Description Header File <code>FluentSpeechCommands</code> An audio dataset for spoken language understanding. <code>speech_command_recognition/fluent_speech_commands.h</code> <code>SpeechCommands</code> A dataset of one-second audio clips of people saying thirty-five different words. <code>speech_command_recognition/speech_commands.h</code>"},{"location":"api/datasets/audio/#speech-recognition","title":"Speech Recognition","text":"Dataset Class Description Header File <code>CommonVoice</code> A large, multi-language dataset of transcribed speech. <code>speech_recognition/common_voice.h</code> <code>LibriSpeech</code> A corpus of read English speech suitable for training and evaluating speech recognition systems. <code>speech_recognition/librispeech.h</code> <code>TEDLIUM</code> Audio recordings of TED talks with transcriptions. <code>speech_recognition/tedlium.h</code> <code>TIMIT</code> A corpus of phonemically and lexically transcribed speech of American English speakers. <code>speech_recognition/timit.h</code>"},{"location":"api/datasets/audio/#speech-separation","title":"Speech Separation","text":"Dataset Class Description Header File <code>LibriMix</code> A dataset for source separation derived from LibriSpeech. <code>speech_separation/libri_mix.h</code>"},{"location":"api/datasets/audio/#speech-synthesis","title":"Speech Synthesis","text":"Dataset Class Description Header File <code>CMUArctic</code> Speech synthesis databases from Carnegie Mellon University. <code>speech_synthesis/cmu_arctic.h</code> <code>VCTK092</code> Speech data uttered by 110 English speakers with various accents. <code>speech_synthesis/vctk_092.h</code> <code>LJSpeech</code> A public domain speech dataset consisting of 13,100 short audio clips of a single speaker reading passages from 7 non-fiction books. <code>speech_synthesis/lj_speech.h</code> <code>LibriTTS</code> A large-scale, multi-speaker English corpus designed for TTS research. <code>speech_synthesis/libritts.h</code>"},{"location":"api/datasets/computer-vision/","title":"Computer Vision Datasets","text":"<p>xTorch provides an extensive collection of built-in dataset handlers for a wide variety of computer vision tasks, from image classification and object detection to semantic segmentation and beyond. This allows you to easily benchmark models on standard academic datasets without writing custom data loading code.</p> <p>All computer vision datasets are located under the <code>xt::datasets</code> namespace and can be found within the <code>&lt;xtorch/datasets/computer_vision/&gt;</code> header directory.</p>"},{"location":"api/datasets/computer-vision/#general-usage","title":"General Usage","text":"<p>The standard workflow for using any computer vision dataset involves defining a pipeline of image transformations, instantiating the desired dataset class, and then passing it to a data loader.</p> <pre><code>#include &lt;xtorch/xtorch.h&gt;\n\nint main() {\n    // 1. Define a pipeline of image transformations for data augmentation\n    auto transforms = std::make_unique&lt;xt::transforms::Compose&gt;(\n        std::make_shared&lt;xt::transforms::image::RandomHorizontalFlip&gt;(),\n        std::make_shared&lt;xt::transforms::image::Resize&gt;(std::vector&lt;int64_t&gt;{32, 32}),\n        std::make_shared&lt;xt::transforms::general::Normalize&gt;(\n            std::vector&lt;float&gt;{0.5, 0.5, 0.5},\n            std::vector&lt;float&gt;{0.5, 0.5, 0.5}\n        )\n    );\n\n    // 2. Instantiate a dataset for CIFAR-10\n    auto dataset = xt::datasets::CIFAR10(\n        \"./data\",\n        xt::datasets::DataMode::TRAIN,\n        /*download=*/true,\n        std::move(transforms)\n    );\n\n    std::cout &lt;&lt; \"CIFAR-10 dataset size: \" &lt;&lt; *dataset.size() &lt;&lt; std::endl;\n\n    // 3. Pass the dataset to a DataLoader\n    xt::dataloaders::ExtendedDataLoader data_loader(dataset, 128, true, 4);\n\n    // The data loader is now ready for use in a training loop\n    for (auto&amp; batch : data_loader) {\n        auto images = batch.first;\n        auto labels = batch.second;\n        // ... training step ...\n    }\n}\n</code></pre> <p>Standard Dataset Constructors</p> <p>Most dataset constructors follow a standard pattern: <code>DatasetName(const std::string&amp; root, DataMode mode, bool download, TransformPtr transforms)</code> - <code>root</code>: The directory where the data is stored or will be downloaded. - <code>mode</code>: <code>DataMode::TRAIN</code>, <code>DataMode::TEST</code>, or <code>DataMode::VALIDATION</code>. - <code>download</code>: If <code>true</code>, the dataset will be downloaded if not found in the root directory. - <code>transforms</code>: A <code>unique_ptr</code> to a transform pipeline to be applied to the data.</p>"},{"location":"api/datasets/computer-vision/#available-datasets-by-task","title":"Available Datasets by Task","text":""},{"location":"api/datasets/computer-vision/#image-classification","title":"Image Classification","text":"Dataset Class Description Header File <code>MNIST</code> Grayscale handwritten digits (0-9). <code>image_classification/mnist.h</code> <code>FashionMNIST</code> Grayscale images of 10 fashion categories. <code>image_classification/fashion_mnist.h</code> <code>KMNIST</code> Kuzushiji-MNIST, a dataset of classical Japanese characters. <code>image_classification/kmnist.h</code> <code>EMNIST</code> Extended MNIST, a larger set of handwritten letters and digits. <code>image_classification/emnist.h</code> <code>QMNIST</code> A larger, cleaner version of the MNIST dataset. <code>image_classification/qmnist.h</code> <code>USPS</code> A dataset of handwritten digits from the USPS. <code>image_classification/usps.h</code> <code>CIFAR10</code> 32x32 color images in 10 classes. <code>image_classification/cifar_10.h</code> <code>CIFAR100</code> 32x32 color images in 100 classes. <code>image_classification/cifar_100.h</code> <code>ImageNet</code> The large-scale ImageNet (ILSVRC) dataset. <code>image_classification/imagenet.h</code> <code>CelebA</code> Large-scale CelebFaces Attributes dataset. <code>image_classification/celeba.h</code> <code>STL10</code> An image recognition dataset with 10 classes, with fewer labeled images than CIFAR-10. <code>image_classification/stl.h</code> <code>SVHN</code> Street View House Numbers dataset. <code>image_classification/svhn.h</code> <code>Caltech101</code> Images of objects belonging to 101 categories. <code>image_classification/caltech101.h</code> <code>Caltech256</code> An improved version of Caltech101 with 256 categories. <code>image_classification/caltech256.h</code> <code>Food101</code> A challenging dataset of 101 food categories. <code>image_classification/food.h</code> <code>Flowers102</code> A dataset of 102 flower categories. <code>image_classification/flowers.h</code> <code>StanfordCars</code> A dataset of 196 classes of cars. <code>image_classification/stanford_cars.h</code> <code>FGVCAircraft</code> A fine-grained dataset of aircraft variants. <code>image_classification/fgvc_aircraft.h</code> <code>DTD</code> Describable Textures Dataset for texture recognition. <code>image_classification/dtd.h</code> <code>EuroSAT</code> A dataset of Sentinel-2 satellite images covering 10 land use classes. <code>image_classification/euro_sat.h</code> <code>GTSRB</code> German Traffic Sign Recognition Benchmark. <code>image_classification/gtsrb.h</code> <code>PCAM</code> PatchCamelyon, a medical imaging dataset for metastasis detection. <code>image_classification/pcam.h</code> <code>LFWPeople</code> Labeled Faces in the Wild, a dataset for face recognition. <code>image_classification/lfw_people.h</code>"},{"location":"api/datasets/computer-vision/#object-detection","title":"Object Detection","text":"Dataset Class Description Header File <code>COCODetection</code> The popular COCO (Common Objects in Context) dataset for detection. <code>object_detection/coco_detection.h</code> <code>VOCDetection</code> The PASCAL VOC dataset for object detection. <code>object_detection/voc_detection.h</code> <code>KITTI</code> A popular dataset for autonomous driving research, including object detection. <code>object_detection/kitti.h</code> <code>OpenImages</code> A large-scale dataset with millions of images and bounding boxes. <code>object_detection/open_images.h</code> <code>WIDERFace</code> A face detection benchmark dataset. <code>face_detection/wider_face.h</code>"},{"location":"api/datasets/computer-vision/#semantic-instance-segmentation","title":"Semantic &amp; Instance Segmentation","text":"Dataset Class Description Header File <code>VOCSegmentation</code> The PASCAL VOC dataset for semantic segmentation. <code>semantic_segmentation/voc_segmentation.h</code> <code>Cityscapes</code> A large-scale dataset focusing on semantic understanding of urban street scenes. <code>semantic_segmentation/cityscapes.h</code> <code>ADE20K</code> A scene parsing benchmark for semantic segmentation and scene recognition. <code>semantic_segmentation/ade20k.h</code> <code>OxfordIIITPet</code> A 37 category pet dataset with pixel-level segmentation masks. <code>semantic_segmentation/oxfordIII_t_pet.h</code> <code>LVIS</code> A large vocabulary instance segmentation dataset. <code>instance_segmentation/lvis.h</code>"},{"location":"api/datasets/computer-vision/#image-generation","title":"Image Generation","text":"Dataset Class Description Header File <code>FFHQ</code> Flickr-Faces-HQ, a high-quality image dataset of human faces. <code>image_generation/ffhq.h</code> <code>CelebA</code> The CelebA dataset, also commonly used for training GANs. <code>image_classification/celeba.h</code>"},{"location":"api/datasets/computer-vision/#image-captioning","title":"Image Captioning","text":"Dataset Class Description Header File <code>COCOCaptions</code> The COCO dataset with its associated image captions. <code>image_captioning/coco_captions.h</code> <code>Flickr8k</code> A dataset of 8,000 captioned images. <code>image_classification/flickr_8k.h</code> <code>Flickr30k</code> A larger version of the Flickr dataset with 30,000 images. <code>image_classification/flickr_30k.h</code>"},{"location":"api/datasets/computer-vision/#autonomous-driving-3d-vision","title":"Autonomous Driving &amp; 3D Vision","text":"Dataset Class Description Header File <code>WaymoOpenDataset</code> A large and diverse dataset for autonomous driving research. <code>autonomous_driving_perception/waymo_open_dataset.h</code> <code>nuScenes</code> A large-scale public dataset for autonomous driving. <code>autonomous_driving_perception/nu_scenes.h</code> <code>ModelNet40</code> A dataset of 3D CAD models for point cloud analysis. <code>3d_point_cloud_analysis/model_net40.h</code> <code>ShapeNet</code> A large repository of 3D shapes. <code>3d_shape_generation/shapenet.h</code>"},{"location":"api/datasets/computer-vision/#optical-flow","title":"Optical Flow","text":"Dataset Class Description Header File <code>FlyingChairs</code> A synthetic dataset for training optical flow networks. <code>optical_flow_estimation/flying_chairs.h</code> <code>Sintel</code> A popular benchmark for optical flow, with realistic rendering. <code>optical_flow_estimation/sintel.h</code>"},{"location":"api/datasets/general/","title":"General Purpose Datasets","text":"<p>While xTorch provides handlers for many public datasets, the most common use case is training a model on your own custom data. To make this process as simple as possible, xTorch provides a set of general-purpose dataset classes designed to work with common data formats and directory structures.</p> <p>These classes save you from writing a custom C++ <code>Dataset</code> class from scratch for many standard scenarios. They are all located under the <code>xt::datasets</code> namespace and can be found in the <code>&lt;xtorch/datasets/general/&gt;</code> header directory.</p>"},{"location":"api/datasets/general/#xtdatasetsimagefolderdataset","title":"<code>xt::datasets::ImageFolderDataset</code>","text":"<p>This is one of the most useful dataset classes in the library. <code>ImageFolderDataset</code> allows you to load a custom image classification dataset from a directory, provided it follows a specific structure.</p>"},{"location":"api/datasets/general/#required-directory-structure","title":"Required Directory Structure","text":"<p>The data must be organized into a root folder, with one subdirectory for each class. Each subdirectory should contain all the images belonging to that class.</p> <pre><code>/path/to/your/data/\n\u251c\u2500\u2500 class_a/\n\u2502   \u251c\u2500\u2500 xxx.png\n\u2502   \u251c\u2500\u2500 xxy.png\n\u2502   \u2514\u2500\u2500 ...\n\u2502\n\u251c\u2500\u2500 class_b/\n\u2502   \u251c\u2500\u2500 zzz.png\n\u2502   \u251c\u2500\u2500 zzy.png\n\u2502   \u2514\u2500\u2500 ...\n\u2502\n\u2514\u2500\u2500 class_c/\n\u251c\u2500\u2500 123.png\n\u251c\u2500\u2500 456.png\n\u2514\u2500\u2500 ...\n</code></pre> <p>The dataset will automatically discover the classes based on the subdirectory names and assign an integer label to each class.</p>"},{"location":"api/datasets/general/#usage","title":"Usage","text":"<p>You instantiate <code>ImageFolderDataset</code> with the path to the root data folder and an optional transform pipeline. It handles all the file discovery and label assignment internally.</p> <pre><code>#include &lt;xtorch/xtorch.h&gt;\n\nint main() {\n    // 1. Define image transformations\n    auto transforms = std::make_unique&lt;xt::transforms::Compose&gt;(\n        std::make_shared&lt;xt::transforms::image::Resize&gt;(std::vector&lt;int64_t&gt;{224, 224}),\n        std::make_shared&lt;xt::transforms::general::Normalize&gt;(\n            std::vector&lt;float&gt;{0.5, 0.5, 0.5},\n            std::vector&lt;float&gt;{0.5, 0.5, 0.5}\n        )\n    );\n\n    // 2. Instantiate the ImageFolderDataset with the path to the root directory\n    auto my_dataset = xt::datasets::ImageFolderDataset(\n        \"/path/to/your/data/\", // Path to the root folder shown above\n        std::move(transforms)\n    );\n\n    std::cout &lt;&lt; \"Found \" &lt;&lt; *my_dataset.size() &lt;&lt; \" images in \"\n              &lt;&lt; my_dataset.classes().size() &lt;&lt; \" classes.\" &lt;&lt; std::endl;\n\n    // 3. Pass to a DataLoader as usual\n    xt::dataloaders::ExtendedDataLoader data_loader(my_dataset, 32, true);\n\n    // Ready for training\n    for (auto&amp; batch : data_loader) {\n        // ...\n    }\n}\n</code></pre>"},{"location":"api/datasets/general/#xtdatasetscsvdataset","title":"<code>xt::datasets::CSVDataset</code>","text":"<p>This class provides a simple way to load datasets from a <code>.csv</code> file. It is highly flexible, allowing you to specify which columns contain features and which column contains the target label.</p>"},{"location":"api/datasets/general/#usage_1","title":"Usage","text":"<pre><code>#include &lt;xtorch/xtorch.h&gt;\n\nint main() {\n    // Assume you have a CSV file \"my_data.csv\" with columns:\n    // feature_1, feature_2, feature_3, target_class\n\n    // Specify the names of the columns to be used as features\n    std::vector&lt;std::string&gt; feature_cols = {\"feature_1\", \"feature_2\", \"feature_3\"};\n\n    // Specify the name of the column to be used as the target\n    std::string target_col = \"target_class\";\n\n    // Instantiate the CSVDataset\n    auto csv_dataset = xt::datasets::CSVDataset(\n        \"path/to/my_data.csv\",\n        feature_cols,\n        target_col\n    );\n\n    xt::dataloaders::ExtendedDataLoader data_loader(csv_dataset, 16);\n\n    for (auto&amp; batch : data_loader) {\n        auto features = batch.first;\n        auto targets = batch.second;\n        // ... training step ...\n    }\n}\n</code></pre>"},{"location":"api/datasets/general/#xtdatasetstensordataset","title":"<code>xt::datasets::TensorDataset</code>","text":"<p>This is a utility dataset that wraps one or more existing tensors. It is useful when your entire dataset already fits in memory as <code>torch::Tensor</code> objects. Each sample will be a slice along the first dimension of the given tensors.</p>"},{"location":"api/datasets/general/#usage_2","title":"Usage","text":"<pre><code>// Create some random data and targets\nauto all_features = torch::randn({1000, 20}); // 1000 samples, 20 features each\nauto all_targets = torch::randint(0, 5, {1000}); // 1000 target labels\n\n// Wrap the tensors in a TensorDataset\nauto tensor_dataset = xt::datasets::TensorDataset({all_features, all_targets});\n\nstd::cout &lt;&lt; \"TensorDataset size: \" &lt;&lt; *tensor_dataset.size() &lt;&lt; std::endl; // Prints 1000\n\nxt::dataloaders::ExtendedDataLoader data_loader(tensor_dataset, 100);\n</code></pre>"},{"location":"api/datasets/general/#other-general-purpose-handlers","title":"Other General-Purpose Handlers","text":"<p>xTorch includes several other <code>Folder</code>-style datasets for different data modalities.</p> Dataset Class Description Header File <code>AudioFolder</code> Loads audio files from a directory structure similar to <code>ImageFolder</code>. <code>general/audio_folder.h</code> <code>TextFolder</code> Loads text files from a directory structure similar to <code>ImageFolder</code>. <code>general/text_folder.h</code> <code>VideoFolder</code> Loads video files from a directory structure similar to <code>ImageFolder</code>. <code>general/video_folder.h</code> <code>PairedImageDataset</code> Loads paired images (e.g., for style transfer or image-to-image translation) from two corresponding directories. <code>general/paired_image_dataset.h</code>"},{"location":"api/datasets/graph/","title":"Graph Datasets","text":"<p>xTorch provides support for graph-based machine learning tasks with a collection of standard graph datasets. These are essential for developing and benchmarking Graph Neural Networks (GNNs).</p> <p>Graph datasets are located under the <code>xt::datasets</code> namespace and can be found in the <code>&lt;xtorch/datasets/graph_data/&gt;</code> header directory.</p>"},{"location":"api/datasets/graph/#graph-data-representation","title":"Graph Data Representation","text":"<p>Unlike image or text data, which is typically represented as a pair of <code>(data, target)</code> tensors, graph data has a more complex structure. In xTorch, a graph dataset typically returns a <code>torch::data::Example</code> containing multiple components: - <code>x</code>: A <code>[num_nodes, num_node_features]</code> tensor of node features. - <code>edge_index</code>: A <code>[2, num_edges]</code> tensor representing the graph's connectivity in COO (coordinate) format. Each column is an edge. - <code>y</code>: A tensor of node or graph labels, depending on the task.</p> <p>The <code>DataLoader</code> for graph data is designed to handle this structure and create mini-batches appropriately.</p>"},{"location":"api/datasets/graph/#general-usage","title":"General Usage","text":"<p>The workflow for using a graph dataset involves instantiating the dataset class and passing it to a data loader. Due to the nature of graph data, complex transformations are less common but still possible.</p> <pre><code>#include &lt;xtorch/xtorch.h&gt;\n\nint main() {\n    // 1. Instantiate a dataset for the Cora citation network\n    // This dataset is commonly used for node classification.\n    auto dataset = xt::datasets::Cora(\n        \"./data\",\n        /*download=*/true\n    );\n\n    // Note: Graph datasets often represent a single large graph.\n    // The \"size\" might be 1, and batching is handled differently by specialized GNN data loaders.\n    std::cout &lt;&lt; \"Cora dataset loaded.\" &lt;&lt; std::endl;\n\n    // For demonstration, let's get the single graph object from the dataset\n    auto graph_data = dataset.get(0);\n    auto node_features = graph_data.data;\n    auto edge_index = graph_data.target; // Example structure, might differ per dataset\n\n    std::cout &lt;&lt; \"Node feature shape: \" &lt;&lt; node_features.sizes() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Edge index shape: \" &lt;&lt; edge_index.sizes() &lt;&lt; std::endl;\n\n    // 2. Pass the dataset to a DataLoader\n    // For GNNs, you might use a specialized graph data loader or a standard one with a batch size of 1\n    // if you are doing full-graph training.\n    xt::dataloaders::ExtendedDataLoader data_loader(dataset, /*batch_size=*/1, /*shuffle=*/false);\n\n    // The data loader is now ready for use in a training loop\n    for (auto&amp; batch : data_loader) {\n        // ... training step with a GNN model ...\n    }\n}\n</code></pre> <p>Graph Batching</p> <p>Batching multiple graphs into a single larger graph (a common technique in GNNs) is a specialized process. While the <code>ExtendedDataLoader</code> can iterate over datasets, you may need custom collation logic for advanced GNN training scenarios. For full-graph training (where the entire graph is processed at once), a batch size of 1 is appropriate.</p>"},{"location":"api/datasets/graph/#available-datasets-by-task","title":"Available Datasets by Task","text":""},{"location":"api/datasets/graph/#node-classification","title":"Node Classification","text":"<p>Node classification is the task of predicting a label for each node in a graph, given the labels of some nodes.</p> Dataset Class Description Header File <code>Cora</code> A citation network dataset where nodes are documents and edges are citation links. The task is to classify each document into one of seven classes. <code>node_classification/cora.h</code>"},{"location":"api/datasets/graph/#graph-level-tasks-graph-classificationregression","title":"Graph-Level Tasks (Graph Classification/Regression)","text":"<p>Graph-level tasks involve predicting a single property for an entire graph.</p> Dataset Class Description Header File <code>OGBMolHIV</code> A molecular property prediction dataset from the Open Graph Benchmark. The task is to predict whether a molecule inhibits HIV virus replication. <code>molecular_property_prediction/ogb_mo_ihiv.h</code>"},{"location":"api/datasets/graph/#knowledge-graph-reasoning","title":"Knowledge Graph Reasoning","text":"Dataset Class Description Header File <code>Freebase</code> A subset of the Freebase knowledge graph used for link prediction tasks. <code>knowledge_graph_reasoning/freebase.h</code> <code>Wikidata5M</code> A large-scale knowledge graph distilled from Wikidata and Wikipedia. <code>knowledge_graph_reasoning/wikidata_5m.h</code>"},{"location":"api/datasets/nlp/","title":"Natural Language Processing Datasets","text":"<p>xTorch includes a comprehensive suite of built-in dataset handlers for a wide variety of Natural Language Processing (NLP) tasks. These handlers manage the downloading and parsing of standard text corpora, allowing you to focus on model development.</p> <p>All NLP datasets are located under the <code>xt::datasets</code> namespace and can be found in the <code>&lt;xtorch/datasets/natural_language_processing/&gt;</code> header directory.</p>"},{"location":"api/datasets/nlp/#general-usage","title":"General Usage","text":"<p>Working with NLP datasets requires an additional preprocessing step compared to other modalities: tokenization and numericalization. Raw text must be converted into a sequence of integer IDs that can be fed into a model. This is typically handled by a text <code>Transform</code>.</p> <p>The general workflow is: 1.  Build or load a vocabulary. 2.  Create a text transformation pipeline for tokenizing and numericalizing the text. 3.  Instantiate the desired dataset with this pipeline. 4.  Pass the dataset to a <code>DataLoader</code>.</p> <pre><code>#include &lt;xtorch/xtorch.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    // 1. &amp; 2. Define a text transformation pipeline\n    // This is a simplified example. In a real scenario, you would build a vocabulary\n    // from the training data and use it to convert tokens to integers.\n    // xTorch's text transforms help with this process.\n    // For now, let's assume we have a simple tokenizer.\n    // auto text_transforms = std::make_unique&lt;xt::transforms::text::...&gt;();\n\n    // 3. Instantiate a dataset for the IMDB movie review task\n    auto dataset = xt::datasets::IMDB(\n        \"./data\",\n        xt::datasets::DataMode::TRAIN,\n        /*download=*/true\n        // std::move(text_transforms) // Pass transforms here\n    );\n\n    std::cout &lt;&lt; \"IMDB dataset size: \" &lt;&lt; *dataset.size() &lt;&lt; std::endl;\n\n    // 4. Pass the dataset to a DataLoader\n    // Note: For NLP, you often need a custom collate function to handle padding\n    // of sequences with different lengths.\n    xt::dataloaders::ExtendedDataLoader data_loader(dataset, 32, true);\n\n    // The data loader is now ready for use in a training loop\n    for (auto&amp; batch : data_loader) {\n        auto numericalized_text = batch.first;\n        auto labels = batch.second;\n        // ... training step with an RNN or Transformer ...\n    }\n}\n</code></pre> <p>Standard Dataset Constructors</p> <p>Most dataset constructors follow a standard pattern: <code>DatasetName(const std::string&amp; root, DataMode mode, bool download, TextTransformPtr transforms)</code> - <code>root</code>: The directory where the data is stored or will be downloaded. - <code>mode</code>: <code>DataMode::TRAIN</code>, <code>DataMode::TEST</code>, or <code>DataMode::VALIDATION</code>. - <code>download</code>: If <code>true</code>, the dataset will be downloaded if not found in the root directory. - <code>transforms</code>: A <code>unique_ptr</code> to a text transform pipeline.</p>"},{"location":"api/datasets/nlp/#available-datasets-by-task","title":"Available Datasets by Task","text":""},{"location":"api/datasets/nlp/#text-classification","title":"Text Classification","text":"<p>This is the task of assigning a category or label to a piece of text.</p> Dataset Class Description Header File <code>IMDB</code> A large dataset of movie reviews for binary sentiment classification. <code>text_classification/imdb.h</code> <code>AG_NEWS</code> A dataset of news articles from 4 categories. <code>text_classification/ag_news.h</code> <code>SST2</code> Stanford Sentiment Treebank, for sentiment analysis. <code>text_classification/sst2.h</code> <code>YelpReviewPolarity</code> A large dataset of Yelp reviews for binary sentiment analysis. <code>text_classification/yelp_review_polarity.h</code> <code>AmazonReviewPolarity</code> A dataset of Amazon product reviews for binary sentiment analysis. <code>text_classification/amazon_review_polarity.h</code> <code>DBpedia</code> A large-scale, multi-class text classification dataset with 14 classes. <code>text_classification/db_pedia.h</code> <code>SNLI</code> Stanford Natural Language Inference corpus. <code>text_classification/snli.h</code> <code>MNLI</code> Multi-Genre Natural Language Inference corpus. <code>text_classification/mnli.h</code>"},{"location":"api/datasets/nlp/#language-modeling","title":"Language Modeling","text":"<p>The task of predicting the next word in a sequence.</p> Dataset Class Description Header File <code>WikiText2</code> A small, high-quality corpus from Wikipedia articles. <code>language_modeling/wiki_text_2.h</code> <code>WikiText103</code> A larger version of the WikiText corpus. <code>language_modeling/wiki_text103.h</code> <code>PennTreebank</code> A classic, smaller dataset widely used for language modeling. <code>language_modeling/penn_treebank.h</code>"},{"location":"api/datasets/nlp/#machine-translation","title":"Machine Translation","text":"<p>The task of translating a sequence of text from a source language to a target language.</p> Dataset Class Description Header File <code>WMT14</code> The dataset from the 2014 Workshop on Machine Translation. <code>machine_translation/wmt14.h</code> <code>IWSLT2017</code> The dataset from the International Workshop on Spoken Language Translation 2017. <code>machine_translation/iwslt2017.h</code> <code>Multi30k</code> A dataset of 30,000 sentences with English, German, French, and Czech translations. <code>machine_translation/multi30k.h</code>"},{"location":"api/datasets/nlp/#question-answering","title":"Question Answering","text":"<p>The task of answering a question based on a given context passage.</p> Dataset Class Description Header File <code>SQuAD1</code> The Stanford Question Answering Dataset, version 1.1. <code>question_answering/squad1_0.h</code> <code>SQuAD2</code> Version 2.0 of SQuAD, which includes unanswerable questions. <code>question_answering/squad2_0.h</code> <code>NaturalQuestions</code> A large-scale QA dataset from Google. <code>question_answering/natural_questions.h</code> <code>TriviaQA</code> A challenging QA dataset with questions authored by trivia enthusiasts. <code>question_answering/trivia_qa.h</code>"},{"location":"api/datasets/nlp/#text-summarization","title":"Text Summarization","text":"<p>The task of generating a short summary from a longer document.</p> Dataset Class Description Header File <code>CNNDailyMail</code> A large dataset of news articles and their summaries. <code>text_summarization/cnn_daily_mail.h</code> <code>XSum</code> A dataset for extreme summarization, with highly abstractive summaries. <code>text_summarization/xsum.h</code>"},{"location":"api/datasets/nlp/#sequence-tagging","title":"Sequence Tagging","text":"Dataset Class Description Header File <code>CoNLL2000Chunking</code> A dataset for the task of chunking (shallow parsing). <code>sequence_tagging/co_nll2000_chunking.h</code> <code>UDPOS</code> Universal Dependencies dataset for Part-of-Speech tagging. <code>sequence_tagging/udpos.h</code>"},{"location":"api/datasets/nlp/#dialogue-generation","title":"Dialogue Generation","text":"Dataset Class Description Header File <code>DailyDialog</code> A high-quality, multi-turn dialogue dataset. <code>dialogue_generation/daily_dialog.h</code> <code>PersonaChat</code> A conversational dataset where models are primed with a \"persona\". <code>dialogue_generation/persona_chat.h</code>"},{"location":"api/datasets/nlp/#math-word-problems","title":"Math Word Problems","text":"Dataset Class Description Header File <code>GSM8K</code> A dataset of 8,000 grade-school math word problems. <code>math_word_problems/gsm8k.h</code>"},{"location":"api/datasets/other/","title":"Other Specialized Datasets","text":"<p>Beyond the primary domains of vision, language, and audio, xTorch also provides support for datasets from more specialized fields. This allows researchers and developers to work on a diverse range of tasks using the same consistent data loading interface.</p> <p>This section covers datasets for: -   Biomedical Data Analysis -   Recommendation Systems -   Reinforcement Learning</p>"},{"location":"api/datasets/other/#general-usage","title":"General Usage","text":"<p>The usage pattern for these specialized datasets is similar to others: you instantiate the dataset class and pass it to a <code>DataLoader</code>. However, the structure of the data and the required preprocessing can be highly specific to the domain.</p> <p>For example, reinforcement learning datasets might represent an entire environment, while recommendation system datasets often consist of user-item interaction pairs. Always refer to the specific dataset's documentation or header file for details on the data format.</p> <pre><code>#include &lt;xtorch/xtorch.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    // Example: Loading the MovieLens dataset for a recommendation task\n    auto dataset = xt::datasets::MovieLens(\n        \"./data\",\n        /*download=*/true\n    );\n\n    std::cout &lt;&lt; \"MovieLens dataset loaded.\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Number of ratings: \" &lt;&lt; *dataset.size() &lt;&lt; std::endl;\n\n    // The data loader will provide batches of user-item-rating triplets\n    xt::dataloaders::ExtendedDataLoader data_loader(dataset, 256, true);\n\n    for (auto&amp; batch : data_loader) {\n        // Data structure depends on the dataset, check the implementation.\n        // For MovieLens, it could be a tensor of user IDs, item IDs, and ratings.\n        // auto user_ids = batch.first;\n        // auto item_ids = batch.second;\n        // auto ratings = ...\n    }\n}\n</code></pre>"},{"location":"api/datasets/other/#available-datasets-by-domain","title":"Available Datasets by Domain","text":""},{"location":"api/datasets/other/#biomedical-data","title":"Biomedical Data","text":"<p>These datasets are used for tasks like disease classification and genomic analysis.</p> Dataset Class Description Header File <code>ADNI</code> The Alzheimer's Disease Neuroimaging Initiative dataset, used for classifying stages of Alzheimer's from medical imaging and clinical data. <code>biomedical_data/alzheimers_classification/adni.h</code> <code>TCGA</code> The Cancer Genome Atlas (TCGA) dataset, containing genomic and clinical data for cancer research. <code>biomedical_data/cancer_genomics_classification/tcga.h</code>"},{"location":"api/datasets/other/#recommendation-systems","title":"Recommendation Systems","text":"<p>These datasets contain user-item interaction data (e.g., ratings, reviews) and are used to train recommender models.</p> Dataset Class Description Header File <code>MovieLens</code> A classic dataset family containing movie ratings from users. Different versions (e.g., 100K, 1M, 20M) are available. <code>recommendation_systems/recommendation/movie_lens.h</code> <code>AmazonProductReviews</code> A large dataset of product reviews from Amazon, useful for training recommendation and sentiment analysis models. <code>recommendation_systems/recommendation/amazon_product_reviews.h</code>"},{"location":"api/datasets/other/#reinforcement-learning","title":"Reinforcement Learning","text":"<p>These are not traditional datasets but rather environments or collections of recorded experiences used to train reinforcement learning agents.</p> Dataset Class Description Header File <code>Atari2600ALE</code> Provides an interface to the Arcade Learning Environment (ALE), allowing agents to be trained on a wide variety of Atari 2600 games. <code>reinforcement_learning/reinforcement_learning/atari_2600_ale.h</code> <code>MuJoCoGym</code> Provides an interface to continuous control environments from OpenAI Gym powered by the MuJoCo physics engine (e.g., Hopper, Walker, Humanoid). <code>reinforcement_learning/continuous_control/mu_jo_co_gym.h</code>"},{"location":"api/datasets/tabular/","title":"Tabular Datasets","text":"<p>Tabular datasets are one of the most common forms of data, organized in a table-like structure with rows representing individual samples and columns representing features. These datasets are foundational to \"classic\" machine learning tasks like classification and regression.</p> <p>xTorch provides a collection of well-known, small-scale tabular datasets, which are extremely useful for educational purposes, debugging models, and experimenting with algorithms on well-understood data.</p> <p>All tabular datasets are located under the <code>xt::datasets</code> namespace and can be found in the <code>&lt;xtorch/datasets/tabular_data/&gt;</code> header directory.</p>"},{"location":"api/datasets/tabular/#general-usage","title":"General Usage","text":"<p>Tabular datasets in xTorch are typically pre-processed and do not require complex transformations like image or audio data. The features are often numerical, and the targets are provided as class indices or continuous values.</p> <p>The workflow is straightforward: instantiate the dataset and pass it to a <code>DataLoader</code>.</p> <pre><code>#include &lt;xtorch/xtorch.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    // 1. Instantiate a dataset for the Iris flower classification task.\n    // These datasets typically only require a root path and whether to download.\n    auto dataset = xt::datasets::Iris(\n        \"./data\",\n        xt::datasets::DataMode::TRAIN, // Mode might not be used if the dataset isn't split\n        /*download=*/true\n    );\n\n    std::cout &lt;&lt; \"Iris dataset size: \" &lt;&lt; *dataset.size() &lt;&lt; std::endl;\n    // Iris has 4 features per sample\n    std::cout &lt;&lt; \"Sample feature shape: \" &lt;&lt; dataset.get(0).data.sizes() &lt;&lt; std::endl;\n\n    // 2. Pass the dataset to a DataLoader\n    xt::dataloaders::ExtendedDataLoader data_loader(dataset, 16, true);\n\n    // The data loader is now ready for use in a training loop\n    for (auto&amp; batch : data_loader) {\n        auto features = batch.first;  // Tensor of shape\n        auto labels = batch.second;   // Tensor of shape\n        // ... training step with a simple Multi-Layer Perceptron (MLP) ...\n    }\n}\n</code></pre> <p>Simplified Constructors</p> <p>Many tabular datasets do not have official train/test splits and are not large enough to warrant complex transforms. As such, their constructors are often simpler than those for other domains, sometimes only requiring a <code>root</code> path.</p>"},{"location":"api/datasets/tabular/#available-datasets-by-task","title":"Available Datasets by Task","text":""},{"location":"api/datasets/tabular/#binary-classification","title":"Binary Classification","text":"<p>These datasets involve predicting one of two possible outcomes.</p> Dataset Class Description Header File <code>AdultCensusIncome</code> Predict whether income exceeds $50K/yr based on census data. <code>binary_classification/adult_census_income.h</code> <code>BanknoteAuthentication</code> Predict whether a banknote is genuine or forged from image features. <code>binary_classification/banknote_authentication.h</code> <code>BreastCancerWisconsin</code> Predict whether a breast mass is benign or malignant from digitized image features. <code>binary_classification/breast_cancer_wisconsin.h</code> <code>HabermansSurvival</code> Predict the survival status of patients who had undergone surgery for breast cancer. <code>binary_classification/habermans_survival.h</code> <code>Ionosphere</code> Classify radar returns from the ionosphere as \"good\" or \"bad\". <code>binary_classification/ionosphere.h</code> <code>Mushroom</code> Predict whether a mushroom is edible or poisonous based on its characteristics. <code>binary_classification/mushroom_dataset.h</code> <code>PimaIndiansDiabetes</code> Predict the onset of diabetes based on diagnostic measures. <code>binary_classification/pima_indians_diabetes.h</code> <code>SonarMinesVsRocks</code> Discriminate between sonar signals bounced off a metal cylinder and those bounced off a rock. <code>binary_classification/sonar_mines_vs_rocks.h</code> <code>Titanic</code> Predict survival on the Titanic. <code>binary_classification/titanic_dataset.h</code>"},{"location":"api/datasets/tabular/#multi-class-classification","title":"Multi-Class Classification","text":"<p>These datasets involve predicting one of more than two possible outcomes.</p> Dataset Class Description Header File <code>Iris</code> The classic dataset for classifying iris flowers into one of three species. <code>classification/iris.h</code> <code>CarEvaluation</code> Evaluate the acceptability of a car based on six input attributes. <code>classification/car_evaluation.h</code> <code>Ecoli</code> Classify the localization site of proteins in E. coli bacteria. <code>classification/ecoli.h</code> <code>GlassIdentification</code> Classify types of glass based on their chemical composition. <code>classification/glass_identification.h</code> <code>PalmerPenguin</code> A modern alternative to Iris for data exploration and classification. <code>classification/palmer_penguin.h</code> <code>VertebralColumn</code> Classify patients into 3 classes (normal, disk hernia, spondylolisthesis) based on orthopedic features. <code>classification/vertebral_column.h</code> <code>WheatSeeds</code> Classify kernels belonging to three different varieties of wheat. <code>classification/wheat_seeds.h</code> <code>Wine</code> Classify wines into one of three cultivars using chemical analysis. <code>classification/wine_dataset.h</code> <code>Yeast</code> Predict the cellular localization sites of proteins in yeast. <code>classification/yeast.h</code> <code>Zoo</code> Classify animals into one of seven types based on their attributes. <code>classification/zoo_dataset.h</code>"},{"location":"api/datasets/tabular/#regression","title":"Regression","text":"<p>These datasets involve predicting a continuous numerical value.</p> Dataset Class Description Header File <code>BostonHousing</code> Predict the median value of owner-occupied homes in Boston suburbs. <code>regression/boston_housing.h</code> <code>Abalone</code> Predict the age of abalone from physical measurements. <code>regression_classification/abalone.h</code>"},{"location":"api/datasets/time-series/","title":"Time Series Datasets","text":"<p>Time-series data consists of sequences of data points indexed in time order. It is a fundamental data type in many domains, including finance, weather forecasting, and sensor data analysis. Common tasks include forecasting future values, classifying sequences, and detecting anomalies.</p> <p>xTorch provides handlers for popular time-series datasets to facilitate research and development in this area. These datasets are located under the <code>xt::datasets</code> namespace and can be found in the <code>&lt;xtorch/datasets/time_series/&gt;</code> header directory.</p>"},{"location":"api/datasets/time-series/#general-usage","title":"General Usage","text":"<p>Working with time-series data often involves specific preprocessing steps like creating sliding windows of data, normalization, and feature engineering. These can be applied using xTorch's <code>Transform</code> pipeline.</p> <p>The general workflow involves instantiating the dataset, which handles loading the raw sequences, and then passing it to a <code>DataLoader</code> for batching.</p> <pre><code>#include &lt;xtorch/xtorch.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    // 1. Define any necessary transformations (optional)\n    // For example, normalizing the time-series values.\n    // auto transforms = std::make_unique&lt;xt::transforms::...&gt;();\n\n    // 2. Instantiate a dataset for the M4 Forecasting Competition.\n    auto dataset = xt::datasets::M4Competition(\n        \"./data\",\n        /*download=*/true\n        // std::move(transforms)\n    );\n\n    std::cout &lt;&lt; \"M4 Competition dataset loaded.\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Number of time series: \" &lt;&lt; *dataset.size() &lt;&lt; std::endl;\n\n    // 3. Pass the dataset to a DataLoader\n    // Batching time-series data can depend heavily on the model architecture (e.g., RNNs vs Transformers).\n    xt::dataloaders::ExtendedDataLoader data_loader(dataset, 32, true);\n\n    // The data loader is now ready for use in a training loop\n    for (auto&amp; batch : data_loader) {\n        auto history = batch.first;   // Input sequence\n        auto future = batch.second;  // Sequence to predict\n        // ... training step with a forecasting model like an LSTM or Informer ...\n    }\n}\n</code></pre> <p>Data Structure</p> <p>The exact structure of the data returned by <code>get(index)</code> can vary depending on the dataset and task. For forecasting, it's often a pair of tensors representing historical context and future values. For classification, it's a sequence and a single class label.</p>"},{"location":"api/datasets/time-series/#available-datasets-by-task","title":"Available Datasets by Task","text":""},{"location":"api/datasets/time-series/#time-series-forecasting","title":"Time Series Forecasting","text":"<p>The task of predicting future values in a sequence given past values.</p> Dataset Class Description Header File <code>M4Competition</code> The dataset from the 4th Makridakis Forecasting Competition, containing a large and diverse set of time series from different domains. <code>time_series_forecasting/m4_competition.h</code> <code>ElectricityLoadDiagrams</code> The ElectricityLoadDiagrams20112014 Data Set, which contains the electricity consumption of 370 clients. <code>time_series_forecasting/electricity_load_diagrams.h</code>"},{"location":"api/datasets/time-series/#time-series-classification","title":"Time Series Classification","text":"<p>The task of assigning a categorical label to an entire time-series sequence.</p> Dataset Class Description Header File <code>UCRTimeSeriesArchive</code> A large collection of datasets from the UCR Time Series Classification Archive, widely used for benchmarking classification algorithms. <code>time_series_classification/ucr_time_series_archive.h</code>"},{"location":"api/datasets/time-series/#anomaly-detection","title":"Anomaly Detection","text":"<p>The task of identifying rare items, events, or observations which raise suspicions by differing significantly from the majority of the data.</p> Dataset Class Description Header File <code>NAB</code> The Numenta Anomaly Benchmark, a benchmark for evaluating algorithms for streaming anomaly detection. <code>anomaly_detection/nab.h</code>"},{"location":"api/models/","title":"Models","text":"<p>The <code>xt::models</code> module provides a comprehensive \"Model Zoo\" containing pre-built, ready-to-use implementations of many popular and state-of-the-art deep learning architectures.</p> <p>This allows you to get started on standard tasks quickly without having to implement well-known models from scratch. It is perfect for benchmarking, transfer learning, and as a starting point for your own custom architectures.</p>"},{"location":"api/models/#core-concept","title":"Core Concept","text":"<p>All models in the <code>xt::models</code> namespace are implemented as standard <code>torch::nn::Module</code>s. This means they integrate seamlessly with the entire LibTorch and xTorch ecosystem. You can inspect their parameters, move them between devices, and pass them to any xTorch <code>Trainer</code> or standard LibTorch <code>Optimizer</code>.</p>"},{"location":"api/models/#general-usage","title":"General Usage","text":"<p>Using a pre-built model from xTorch is straightforward. The typical workflow is: 1.  Include the header for the specific model you want to use. 2.  Instantiate the model class, providing any necessary configuration (e.g., number of classes). 3.  Move the model to the desired device (<code>CPU</code> or <code>CUDA</code>). 4.  Use the model for training or inference.</p> <pre><code>#include &lt;xtorch/xtorch.h&gt;\n\nint main() {\n    // Define the device\n    torch::Device device(torch::cuda::is_available() ? torch::kCUDA : torch::kCPU);\n    std::cout &lt;&lt; \"Using device: \" &lt;&lt; device &lt;&lt; std::endl;\n\n    // 1. &amp; 2. Instantiate a pre-built ResNet18 model for a 100-class problem\n    xt::models::ResNet model(\n        xt::models::ResNetImpl::BlockType::BASIC, // BASIC block for ResNet18/34\n        {2, 2, 2, 2},                             // Layers in each stage for ResNet18\n        /*num_classes=*/100\n    );\n\n    // 3. Move the model to the GPU\n    model.to(device);\n\n    // 4. Perform a dummy forward pass\n    // Create a random input tensor: Batch size 16, 3 channels, 224x224 image\n    auto input = torch::randn({16, 3, 224, 224}).to(device);\n    auto output = model.forward(input);\n\n    std::cout &lt;&lt; \"Model instantiated successfully.\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Output shape: \" &lt;&lt; output.sizes() &lt;&lt; std::endl; // Should be\n\n    // The model can now be passed to an optimizer and the xt::Trainer\n    // torch::optim::Adam optimizer(model.parameters());\n    // xt::Trainer trainer;\n    // trainer.fit(model, ...);\n}\n</code></pre> <p>Model Variants</p> <p>Many model families (like ResNet or EfficientNet) have multiple variants (e.g., ResNet18, ResNet50). These are typically configured through constructor arguments. Please refer to the specific model's header file for details on the available options.</p>"},{"location":"api/models/#available-models-by-domain","title":"Available Models by Domain","text":"<p>The xTorch Model Zoo is organized by machine learning domain. Follow the links below for a detailed list of available architectures in each category.</p> <ul> <li> <p>Computer Vision: Includes classic and modern architectures for image classification (<code>ResNet</code>, <code>VGG</code>, <code>EfficientNet</code>), object detection (<code>YOLO</code>, <code>Faster R-CNN</code>), segmentation (<code>U-Net</code>, <code>DeepLab</code>), and Vision Transformers (<code>ViT</code>, <code>Swin</code>).</p> </li> <li> <p>Generative Models: Contains implementations of popular generative architectures, including Generative Adversarial Networks (<code>DCGAN</code>, <code>StyleGAN</code>), Variational Autoencoders (<code>VAE</code>), and Diffusion Models (<code>DDPM</code>).</p> </li> <li> <p>Natural Language Processing: A collection of models for text-based tasks, including recurrent architectures (<code>Seq2Seq</code>) and a wide range of Transformer-based models (<code>BERT</code>, <code>GPT</code>, <code>T5</code>, <code>Llama</code>).</p> </li> <li> <p>Graph Neural Networks: Implementations of common GNN architectures for learning on graph-structured data, such as <code>GCN</code>, <code>GraphSAGE</code>, and <code>GAT</code>.</p> </li> <li> <p>Reinforcement Learning: A collection of models and policies for reinforcement learning, including <code>DQN</code>, <code>A3C</code>, and <code>PPO</code>.</p> </li> <li> <p>Multimodal: Models designed to process and fuse information from multiple data types, such as <code>CLIP</code> (text and image) and <code>ViLBERT</code>.</p> </li> </ul>"},{"location":"api/models/computer-vision/","title":"Computer Vision Models","text":"<p>xTorch provides a rich and diverse model zoo for a wide array of computer vision tasks. These pre-built models allow you to quickly apply powerful, state-of-the-art architectures to your data.</p> <p>All computer vision models are located under the <code>xt::models</code> namespace and their headers can be found in the <code>&lt;xtorch/models/computer_vision/&gt;</code> directory.</p>"},{"location":"api/models/computer-vision/#general-usage","title":"General Usage","text":"<p>Using a pre-built computer vision model is straightforward. You instantiate the model, typically providing task-specific parameters like the number of output classes, and then it's ready for training or inference.</p> <pre><code>#include &lt;xtorch/xtorch.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    torch::Device device(torch::cuda::is_available() ? torch::kCUDA : torch::kCPU);\n\n    // Example: Instantiate a VGG16 model for a 10-class classification problem\n    xt::models::VGGNet model(\n        xt::models::VGGNetImpl::VGGType::VGG16,\n        /*num_classes=*/10\n    );\n\n    model.to(device);\n    model.train(); // Set to training mode\n\n    // Create a dummy input batch (Batch=4, Channels=3, Height=224, Width=224)\n    auto input_tensor = torch::randn({4, 3, 224, 224}).to(device);\n\n    // Perform a forward pass\n    auto output = model.forward(input_tensor);\n\n    std::cout &lt;&lt; \"VGG16 Model Instantiated.\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Output shape: \" &lt;&lt; output.sizes() &lt;&lt; std::endl; // Should be\n}\n</code></pre> <p>Model Variants</p> <p>Many model families like <code>ResNet</code>, <code>VGGNet</code>, and <code>EfficientNet</code> have multiple variants (e.g., <code>ResNet18</code> vs. <code>ResNet50</code>). These are typically selected via an <code>enum</code> or by passing configuration arguments to the constructor. Please refer to the specific model's header file for all available options.</p>"},{"location":"api/models/computer-vision/#available-models-by-task","title":"Available Models by Task","text":""},{"location":"api/models/computer-vision/#image-classification","title":"Image Classification","text":"<p>These models are designed to take an image as input and output a probability distribution over a set of classes.</p> Model Family Description Header File <code>LeNet5</code> The classic LeNet-5 architecture, foundational for CNNs. <code>image_classification/lenet5.h</code> <code>AlexNet</code> The breakthrough deep CNN from the 2012 ImageNet competition. <code>image_classification/alexnet.h</code> <code>VGGNet</code> A simple and effective architecture with very small (3x3) convolution filters. <code>image_classification/vggnet.h</code> <code>ResNet</code> Residual Networks, which introduced skip connections to enable much deeper models. <code>image_classification/resnet.h</code> <code>ResNeXt</code> An evolution of ResNet that uses grouped convolutions. <code>image_classification/resnext.h</code> <code>WideResNet</code> A ResNet variant that is wider (more channels) but shallower. <code>image_classification/wide_resnet.h</code> <code>GoogLeNet</code> A deep CNN that introduced the \"Inception\" module. <code>image_classification/google_net.h</code> <code>Inception</code> Later versions of the Inception architecture (e.g., InceptionV3). <code>image_classification/inception.h</code> <code>InceptionResNet</code> A hybrid architecture combining Inception modules with residual connections. <code>image_classification/inception_resnet.h</code> <code>DenseNet</code> Densely Connected Convolutional Networks, where each layer is connected to every other layer. <code>image_classification/dense_net.h</code> <code>MobileNet</code> A family of efficient models for mobile and embedded vision applications. <code>image_classification/mobilenet.h</code> <code>EfficientNet</code> A family of models that scales depth, width, and resolution in a principled way. <code>image_classification/efficient_net.h</code> <code>Xception</code> An architecture based on depthwise separable convolutions. <code>image_classification/xception.h</code> <code>SENet</code> Squeeze-and-Excitation Networks that adaptively recalibrate channel-wise feature responses. <code>image_classification/se_net.h</code> <code>CBAM</code> Convolutional Block Attention Module. <code>image_classification/cbam.h</code> <code>NetworkInNetwork</code> A model that uses micro neural networks in place of linear filters. <code>image_classification/network_in_network.h</code> <code>PyramidalNet</code> A variant of ResNet that gradually increases feature map dimensions. <code>image_classification/pyramidal_net.h</code> <code>HighwayNetwork</code> A deep network with learnable gating mechanisms. <code>image_classification/highway_network.h</code> <code>AmoebaNet</code> An architecture discovered through evolutionary neural architecture search. <code>image_classification/amoeba_net.h</code> <code>ZefNet</code> A visualization-driven model, an early winner of the ImageNet competition. <code>image_classification/zefnet.h</code>"},{"location":"api/models/computer-vision/#object-detection","title":"Object Detection","text":"<p>These models identify and locate multiple objects within an image by outputting bounding boxes and class labels.</p> Model Family Description Header File <code>RCNN</code> Region-based CNN, the original groundbreaking model for this task. <code>object_detection/rcnn.h</code> <code>FastRCNN</code> An improved version of R-CNN that is faster to train and test. <code>object_detection/fast_rcnn.h</code> <code>FasterRCNN</code> Introduces a Region Proposal Network (RPN) for end-to-end training. <code>object_detection/faster_rcnn.h</code> <code>MaskRCNN</code> An extension of Faster R-CNN that also adds a branch for predicting segmentation masks. <code>object_detection/mask_rcnn.h</code> <code>SSD</code> Single Shot MultiBox Detector, a one-stage detector that is very fast. <code>object_detection/ssd.h</code> <code>RetinaNet</code> A one-stage detector that introduced the Focal Loss to address class imbalance. <code>object_detection/retina_net.h</code> <code>YOLO</code> You Only Look Once, a family of extremely fast one-stage detectors. <code>object_detection/yolo.h</code> <code>YOLOX</code> An anchor-free version of YOLO. <code>object_detection/yolox.h</code> <code>DETR</code> Detection Transformer, which frames object detection as a set prediction problem. <code>object_detection/detr.h</code> <code>EfficientDet</code> A family of scalable and efficient object detectors. <code>object_detection/efficient_det.h</code>"},{"location":"api/models/computer-vision/#image-segmentation","title":"Image Segmentation","text":"<p>These models classify each pixel in an image to create a segmentation map.</p> Model Family Description Header File <code>FCN</code> Fully Convolutional Network, a foundational model for semantic segmentation. <code>image_segmentation/fcn.h</code> <code>UNet</code> An architecture with a U-shaped encoder-decoder structure, popular for biomedical imaging. <code>image_segmentation/unet.h</code> <code>SegNet</code> A deep encoder-decoder architecture for semantic pixel-wise segmentation. <code>image_segmentation/segnet.h</code> <code>DeepLab</code> A family of models (e.g., DeepLabV3+) using atrous convolutions for segmentation. <code>image_segmentation/deep_lab.h</code> <code>HRNet</code> High-Resolution Network, which maintains high-resolution representations through the network. <code>image_segmentation/hrnet.h</code> <code>PANet</code> Path Aggregation Network, which enhances feature fusion. <code>image_segmentation/panet.h</code>"},{"location":"api/models/computer-vision/#vision-transformers","title":"Vision Transformers","text":"<p>These models apply the Transformer architecture, originally designed for NLP, to computer vision tasks.</p> Model Family Description Header File <code>ViT</code> Vision Transformer, the original model that applies a pure Transformer to image patches. <code>vision_transformers/vit.h</code> <code>DeiT</code> Data-efficient Image Transformer, which uses knowledge distillation. <code>vision_transformers/deit.h</code> <code>SwinTransformer</code> A hierarchical Vision Transformer using shifted windows. <code>vision_transformers/swin_transformer.h</code> <code>PVT</code> Pyramid Vision Transformer, which introduces a pyramid structure to ViT. <code>vision_transformers/pvt.h</code> <code>T2TViT</code> Token-to-Token Vision Transformer. <code>vision_transformers/t2t_vit.h</code> <code>MViT</code> Multiscale Vision Transformer. <code>vision_transformers/mvit.h</code> <code>BEiT</code> Bidirectional Encoder representation from Image Transformers (BERT pre-training for vision). <code>vision_transformers/beit.h</code> <code>CLIPViT</code> The Vision Transformer backbone used in the CLIP model. <code>vision_transformers/clip_vit.h</code>"},{"location":"api/models/generative/","title":"Generative Models","text":"<p>Generative models are a fascinating class of neural networks that learn to create new data samples that resemble the training data. They can be used for a wide range of creative and practical applications, including image synthesis, data augmentation, and unsupervised feature learning.</p> <p>xTorch provides implementations of several major families of generative models, allowing you to easily experiment with this cutting-edge field.</p> <p>All generative models are located under the <code>xt::models</code> namespace and their headers can be found in the <code>&lt;xtorch/models/generative_models/&gt;</code> directory.</p>"},{"location":"api/models/generative/#general-usage","title":"General Usage","text":"<p>The usage of generative models often involves more complex training loops than standard supervised learning. For example, training a Generative Adversarial Network (GAN) requires managing two separate models (a Generator and a Discriminator) and their respective optimizers.</p> <p>The example below shows how to instantiate the Generator and Discriminator for a DCGAN.</p> <p>Training Generative Models</p> <p>Due to their often complex training dynamics (e.g., alternating between training a generator and a discriminator), the standard <code>xt::Trainer</code> might not be suitable for all generative models out-of-the-box. Many of the examples for these models will feature a custom training loop.</p> <pre><code>#include &lt;xtorch/xtorch.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    torch::Device device(torch::cuda::is_available() ? torch::kCUDA : torch::kCPU);\n\n    // --- Hyperparameters for a DCGAN ---\n    const int latent_vector_size = 100; // Size of the input noise vector (nz)\n    const int generator_feature_maps = 64; // Size of feature maps in generator (ngf)\n    const int discriminator_feature_maps = 64; // Size of feature maps in discriminator (ndf)\n    const int num_channels = 3; // Number of image channels (nc)\n\n    // --- Instantiate the Generator and Discriminator ---\n    xt::models::DCGAN::Generator generator(\n        latent_vector_size,\n        generator_feature_maps,\n        num_channels\n    );\n    generator.to(device);\n\n    xt::models::DCGAN::Discriminator discriminator(\n        num_channels,\n        discriminator_feature_maps\n    );\n    discriminator.to(device);\n\n    std::cout &lt;&lt; \"DCGAN Generator and Discriminator instantiated.\" &lt;&lt; std::endl;\n\n    // --- Perform a dummy forward pass ---\n    // Create a random noise tensor to feed the generator\n    auto noise = torch::randn({16, latent_vector_size, 1, 1}).to(device);\n    // Generate a batch of fake images\n    auto fake_images = generator.forward(noise);\n\n    // Pass the fake images to the discriminator\n    auto discriminator_output = discriminator.forward(fake_images);\n\n    std::cout &lt;&lt; \"Generated image batch shape: \" &lt;&lt; fake_images.sizes() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Discriminator output shape: \" &lt;&lt; discriminator_output.sizes() &lt;&lt; std::endl;\n\n    // --- Setup Optimizers (for a real training loop) ---\n    torch::optim::Adam generator_optimizer(generator.parameters(), torch::optim::AdamOptions(0.0002).betas({0.5, 0.999}));\n    torch::optim::Adam discriminator_optimizer(discriminator.parameters(), torch::optim::AdamOptions(0.0002).betas({0.5, 0.999}));\n}\n</code></pre>"},{"location":"api/models/generative/#available-models-by-family","title":"Available Models by Family","text":""},{"location":"api/models/generative/#generative-adversarial-networks-gans","title":"Generative Adversarial Networks (GANs)","text":"<p>GANs consist of a generator that creates data and a discriminator that tries to distinguish between real and generated data.</p> Model Family Description Header File <code>GAN</code> A basic, foundational GAN implementation. <code>gans/gan.h</code> <code>DCGAN</code> Deep Convolutional GAN, a stable and effective architecture for image generation. <code>gans/dcgan.h</code> <code>WGAN</code> Wasserstein GAN, which uses the Wasserstein distance to improve training stability. <code>gans/wgan.h</code> <code>WGAN-GP</code> WGAN with a Gradient Penalty, further improving stability over the original WGAN. <code>gans/wgan_gp.h</code> <code>CycleGAN</code> A model for unpaired image-to-image translation. <code>gans/cycle_gan.h</code> <code>Pix2Pix</code> A model for paired image-to-image translation. <code>gans/pix2pix.h</code> <code>ProGAN</code> Progressively Growing GANs, for generating high-resolution images. <code>gans/pro_gan.h</code> <code>StyleGAN</code> A powerful GAN architecture that allows for style-based control over the generated images. <code>gans/style_gan.h</code> <code>BigGAN</code> A large-scale GAN known for generating high-fidelity and diverse images. <code>gans/big_gan.h</code> <code>StarGAN</code> A GAN capable of multi-domain image-to-image translation. <code>gans/star_gan.h</code>"},{"location":"api/models/generative/#autoencoders","title":"Autoencoders","text":"<p>Autoencoders learn a compressed representation (encoding) of data and can be used for generative tasks, dimensionality reduction, and anomaly detection.</p> Model Family Description Header File <code>AE</code> A standard, basic Autoencoder. <code>autoencoders/ae.h</code> <code>DAE</code> Denoising Autoencoder, trained to reconstruct a clean image from a corrupted one. <code>autoencoders/dae.h</code> <code>VAE</code> Variational Autoencoder, a probabilistic generative model that learns a latent space. <code>autoencoders/vae.h</code> <code>CAE</code> Convolutional Autoencoder. <code>autoencoders/cae.h</code> <code>SparseAutoencoder</code> An autoencoder with a sparsity penalty on the latent representation. <code>autoencoders/sparse_autoencoder.h</code>"},{"location":"api/models/generative/#diffusion-models","title":"Diffusion Models","text":"<p>Diffusion models are a powerful new class of generative models that work by progressively adding noise to data and then learning to reverse the process.</p> Model Family Description Header File <code>DDPM</code> Denoising Diffusion Probabilistic Models. <code>diffusion/ddpm.h</code> <code>DDIM</code> Denoising Diffusion Implicit Models, a faster sampling variant of DDPM. <code>diffusion/ddim.h</code> <code>StableDiffusion</code> A latent diffusion model capable of generating high-quality images from text prompts. <code>diffusion/stable_diffusion.h</code> <code>Imagen</code> Google's text-to-image diffusion model. <code>diffusion/imagen.h</code> <code>GLIDE</code> A text-guided diffusion model from OpenAI. <code>diffusion/glide.h</code> <code>DALL-E</code> A multimodal model that can generate images from text. <code>diffusion/dall_e.h</code>"},{"location":"api/models/generative/#other-generative-architectures","title":"Other Generative Architectures","text":"Model Family Description Header File <code>PixelCNN</code> An autoregressive model that generates images pixel by pixel. <code>others/pixel_cnn.h</code> <code>PixelRNN</code> Similar to PixelCNN but uses an RNN-based architecture. <code>others/pixel_rnn.h</code> <code>VQ-VAE</code> Vector Quantised-Variational Autoencoder, which uses a discrete latent space. <code>others/vq_vae.h</code> <code>Glow</code> A type of normalizing flow model that uses invertible neural networks. <code>others/glow.h</code>"},{"location":"api/models/gnn/","title":"Graph Neural Networks (GNNs)","text":"<p>Graph Neural Networks (GNNs) are a specialized class of neural networks designed to perform inference on data structured as graphs. They are essential for tasks like node classification, link prediction, and graph classification, with applications in social networks, molecular chemistry, and recommendation systems.</p> <p>xTorch provides implementations of several foundational and popular GNN architectures, enabling you to apply deep learning to graph-structured data.</p> <p>All GNN models are located under the <code>xt::models</code> namespace and their headers can be found in the <code>&lt;xtorch/models/graph_neural_networks/&gt;</code> directory.</p>"},{"location":"api/models/gnn/#general-usage","title":"General Usage","text":"<p>While GNN models are standard <code>torch::nn::Module</code>s, their usage differs slightly from traditional models like CNNs or MLPs, particularly in the signature of the <code>forward</code> method.</p> <p>A GNN's <code>forward</code> pass typically requires two main inputs: 1.  <code>x</code>: A 2D tensor of node features, with shape <code>[num_nodes, num_node_features]</code>. 2.  <code>edge_index</code>: A 2D tensor representing the graph's connectivity (the adjacency list), with shape <code>[2, num_edges]</code>.</p> <p>The model then uses message passing along the edges to update the node representations.</p> <pre><code>#include &lt;xtorch/xtorch.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    torch::Device device(torch::cuda::is_available() ? torch::kCUDA : torch::kCPU);\n\n    // --- Define Graph Properties ---\n    const int num_nodes = 1000;\n    const int num_node_features = 64;\n    const int num_classes = 7; // E.g., for a node classification task\n    const int num_edges = 4500;\n\n    // --- Instantiate a GCN Model ---\n    // A simple 2-layer Graph Convolutional Network\n    xt::models::GCN model(num_node_features, /*hidden_channels=*/16, num_classes);\n    model.to(device);\n    model.train();\n\n    std::cout &lt;&lt; \"GCN Model Instantiated.\" &lt;&lt; std::endl;\n\n    // --- Create Dummy Graph Data ---\n    // Node feature matrix\n    auto x = torch::randn({num_nodes, num_node_features}).to(device);\n    // Edge index tensor (random edges for demonstration)\n    auto edge_index = torch::randint(0, num_nodes, {2, num_edges}).to(device);\n\n    // --- Perform a Forward Pass ---\n    // Pass both node features and edge index to the model\n    auto output_node_embeddings = model.forward(x, edge_index);\n\n    std::cout &lt;&lt; \"Output shape: \" &lt;&lt; output_node_embeddings.sizes() &lt;&lt; std::endl; // Should be\n}\n</code></pre> <p>Training GNNs</p> <p>Training GNNs can involve different strategies. For small graphs that fit in memory (\"full-batch\" training), you process the entire graph at once. For large graphs, you use neighborhood sampling to create mini-batches. Refer to the xTorch examples for practical training loops.</p>"},{"location":"api/models/gnn/#available-gnn-models","title":"Available GNN Models","text":"<p>xTorch provides the following GNN architectures:</p> Model Description Header File <code>GCN</code> Graph Convolutional Network. A foundational GNN model that learns node features by aggregating information from their local neighborhoods. <code>gcn.h</code> <code>GraphSAGE</code> Graph SAmple and aggreGatE. An inductive framework that generates node embeddings by sampling and aggregating features from a node's local neighborhood. <code>graph_sage.h</code> <code>GAT</code> Graph Attention Network. A GNN that leverages masked self-attention to assign different importance weights to nodes within a neighborhood. <code>gat.h</code> <code>GIN</code> Graph Isomorphism Network. A powerful GNN shown to be as discriminative as the Weisfeiler-Lehman graph isomorphism test. <code>gin.h</code> <code>DiffPool</code> Differentiable Pooling. A module that learns a hierarchical representation of graphs, allowing GNNs to be used for graph classification. <code>diff_pool.h</code> <code>GraphUNet</code> A GNN architecture inspired by U-Net, using graph pooling and unpooling to learn multi-scale node features. <code>graph_unet.h</code>"},{"location":"api/models/multimodal/","title":"Multimodal Models","text":"<p>Multimodal models are a sophisticated class of neural networks designed to process and relate information from two or more different data types (modalities), such as images, text, and audio. These models can learn rich, joint representations that capture the relationships between different modalities.</p> <p>A key application is connecting images with text, enabling tasks like text-to-image retrieval, image captioning, and zero-shot image classification.</p> <p>xTorch provides implementations of several powerful multimodal architectures. All multimodal models are located under the <code>xt::models</code> namespace and their headers can be found in the <code>&lt;xtorch/models/multimodal/&gt;</code> directory.</p>"},{"location":"api/models/multimodal/#general-usage","title":"General Usage","text":"<p>The usage of multimodal models is highly specific to the architecture, as they require multiple, distinct inputs. The <code>forward</code> method is often replaced by more descriptive methods like <code>encode_image</code> and <code>encode_text</code>.</p> <p>For instance, a model like CLIP (Contrastive Language-Image Pre-training) has two main components: 1.  An Image Encoder: Typically a Vision Transformer (ViT) that processes an image and outputs a single feature vector (embedding). 2.  A Text Encoder: Typically a Transformer that processes a sequence of text tokens and outputs a single feature vector.</p> <p>These models are trained to map related images and text descriptions to nearby points in a shared embedding space.</p> <pre><code>#include &lt;xtorch/xtorch.hh&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    torch::Device device(torch::cuda::is_available() ? torch::kCUDA : torch::kCPU);\n\n    // --- Instantiate a CLIP Model ---\n    // The constructor might take configuration for the vision and text models.\n    xt::models::CLIP model(/*vision_config*/, /*text_config*/);\n    model.to(device);\n    model.eval(); // Pre-trained models are often used in evaluation mode\n\n    std::cout &lt;&lt; \"CLIP Model Instantiated.\" &lt;&lt; std::endl;\n\n    // --- Create Dummy Input Data ---\n    // A batch of preprocessed images\n    auto images = torch::randn({4, 3, 224, 224}).to(device);\n    // A batch of tokenized and numericalized text\n    auto text = torch::randint(0, 49408, {4, 77}).to(device); // Vocab size, sequence length\n\n    // --- Perform Encoding ---\n    // Note: The actual method names might differ. Check the header file.\n    auto image_features = model.encode_image(images);\n    auto text_features = model.encode_text(text);\n\n    // The features are normalized to have a unit norm\n    image_features /= image_features.norm(2, -1, true);\n    text_features /= text_features.norm(2, -1, true);\n\n    std::cout &lt;&lt; \"Image features shape: \" &lt;&lt; image_features.sizes() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Text features shape: \" &lt;&lt; text_features.sizes() &lt;&lt; std::endl;\n\n    // --- Calculate Similarity ---\n    // The dot product between the image and text features gives the cosine similarity.\n    // This can be used to find the best text description for each image.\n    auto similarity_logits = torch::matmul(image_features, text_features.t()) * model.logit_scale.exp();\n\n    std::cout &lt;&lt; \"Similarity matrix shape: \" &lt;&lt; similarity_logits.sizes() &lt;&lt; std::endl;\n}\n</code></pre>"},{"location":"api/models/multimodal/#available-multimodal-models","title":"Available Multimodal Models","text":"<p>xTorch provides the following multimodal architectures:</p> Model Description Header File <code>CLIP</code> Contrastive Language-Image Pre-training. Learns a joint embedding space for images and text, enabling powerful zero-shot classification and text-to-image retrieval. <code>clip.h</code> <code>ViLBERT</code> Vision-and-Language BERT. A model for learning task-agnostic joint representations of image content and natural language. <code>vilbert.h</code> <code>BLIP</code> Bootstrapping Language-Image Pre-training. A model for unified vision-language understanding and generation that introduces a novel bootstrapping method. <code>blip.h</code> <code>Flamingo</code> A family of Visual Language Models (VLM) that can perform few-shot learning on a variety of vision-language tasks. <code>flamingo.h</code> <code>LLaVA</code> Large Language and Vision Assistant. A model that connects a vision encoder with a large language model to enable visual instruction following. <code>llava.h</code> <code>MERT</code> A self-supervised representation learning model for music understanding that can process both audio and symbolic (e.g., MIDI) music data. <code>mert.h</code>"},{"location":"api/models/nlp/","title":"Natural Language Processing (NLP) Models","text":"<p>The field of Natural Language Processing has been revolutionized by deep learning, particularly by the advent of the Transformer architecture. To empower developers to build state-of-the-art NLP applications, xTorch provides a comprehensive zoo of pre-built models, ranging from classic recurrent architectures to a wide variety of modern Transformers.</p> <p>All NLP models are located under the <code>xt::models</code> namespace and their headers can be found in the <code>&lt;xtorch/models/natural_language_processing/&gt;</code> directory.</p>"},{"location":"api/models/nlp/#general-usage","title":"General Usage","text":"<p>NLP models do not operate on raw text. Instead, they require the input text to be preprocessed into a numerical format. This typically involves: 1.  Tokenization: Breaking the text into sub-word units (tokens). 2.  Numericalization: Converting each token into a unique integer ID from a vocabulary. 3.  Formatting: Adding special tokens (like <code>[CLS]</code>, <code>[SEP]</code>), creating an attention mask to handle padding, and arranging the data into tensors.</p> <p>The <code>forward</code> pass of a typical Transformer-based model takes these tensors as input.</p> <pre><code>#include &lt;xtorch/xtorch.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    torch::Device device(torch::cuda::is_available() ? torch::kCUDA : torch::kCPU);\n\n    // --- Model Configuration (Example for a BERT-like model) ---\n    const int vocab_size = 30522;\n    const int hidden_size = 768;\n    const int num_attention_heads = 12;\n    const int num_hidden_layers = 12;\n    const int max_position_embeddings = 512;\n\n    // --- Instantiate a BERT Model ---\n    xt::models::BERT model(\n        vocab_size,\n        hidden_size,\n        num_hidden_layers,\n        num_attention_heads,\n        max_position_embeddings\n    );\n    model.to(device);\n    model.train();\n\n    std::cout &lt;&lt; \"BERT Model Instantiated.\" &lt;&lt; std::endl;\n\n    // --- Create Dummy Input Data ---\n    const int batch_size = 8;\n    const int sequence_length = 128;\n\n    // Batch of token IDs\n    auto input_ids = torch::randint(0, vocab_size, {batch_size, sequence_length}).to(device);\n    // Attention mask to indicate which tokens are real vs. padding\n    auto attention_mask = torch::ones({batch_size, sequence_length}).to(device);\n\n    // --- Perform a Forward Pass ---\n    auto output = model.forward(input_ids, attention_mask);\n    // Output is often a tuple or struct containing last_hidden_state, pooler_output, etc.\n    auto last_hidden_state = output.last_hidden_state;\n\n    std::cout &lt;&lt; \"Output hidden state shape: \" &lt;&lt; last_hidden_state.sizes() &lt;&lt; std::endl;\n}\n</code></pre>"},{"location":"api/models/nlp/#available-models-by-family","title":"Available Models by Family","text":""},{"location":"api/models/nlp/#transformer-architectures","title":"Transformer Architectures","text":"<p>This is the largest and most powerful family of models, forming the basis of modern NLP.</p> Model Family Description Header File <code>BERT</code> Bidirectional Encoder Representations from Transformers, a powerful pre-trained encoder model. <code>transformers/bert.h</code> <code>RoBERTa</code> A Robustly Optimized BERT Pretraining Approach. <code>transformers/roberta.h</code> <code>ALBERT</code> A Lite BERT for Self-supervised Learning of Language Representations. <code>transformers/albert.h</code> <code>DistilBERT</code> A smaller, faster, and lighter version of BERT, trained using knowledge distillation. <code>transformers/distil_bert.h</code> <code>ELECTRA</code> A pre-training method that is more efficient than masked language modeling. <code>transformers/electra.h</code> <code>GPT</code> Generative Pre-trained Transformer, a family of powerful auto-regressive language models. <code>transformers/gpt.h</code> <code>Llama</code> A family of large language models released by Meta AI. <code>transformers/llama.h</code> <code>Mistral</code> A family of high-performance large language models. <code>transformers/mistral.h</code> <code>Grok</code> The open-source version of xAI's large language model. <code>transformers/grok.h</code> <code>DeepSeek</code> A family of open-source LLMs from DeepSeek AI. <code>transformers/deepseek.h</code> <code>T5</code> Text-To-Text Transfer Transformer, which frames all NLP tasks as a text-to-text problem. <code>transformers/t5.h</code> <code>BART</code> A denoising autoencoder for pretraining sequence-to-sequence models. <code>transformers/bart.h</code> <code>XLNet</code> A generalized autoregressive pretraining method that combines ideas from autoregressive and autoencoding models. <code>transformers/xlnet.h</code> <code>Longformer</code> A Transformer variant with an attention mechanism that scales linearly with sequence length. <code>transformers/long_former.h</code> <code>Reformer</code> An efficient Transformer variant that uses locality-sensitive hashing. <code>transformers/reformer.h</code> <code>BigBird</code> A sparse attention mechanism that can handle long sequences. <code>transformers/big_bird.h</code>"},{"location":"api/models/nlp/#recurrent-architectures-rnns","title":"Recurrent Architectures (RNNs)","text":"<p>These models process sequences step-by-step and are foundational to sequence-based tasks.</p> Model Description Header File <code>Seq2Seq</code> A standard sequence-to-sequence model using an Encoder-Decoder architecture with RNNs. <code>rnn/seq2seq.h</code> <code>AttentionBasedSeq2Seq</code> An extension of Seq2Seq that incorporates an attention mechanism to improve performance. <code>rnn/attention_based_seq2seq.h</code>"},{"location":"api/models/nlp/#other-classic-models","title":"Other &amp; Classic Models","text":"<p>These models are primarily used for learning static word embeddings.</p> Model Description Header File <code>Word2Vec</code> A classic model that learns word associations from a large corpus of text. <code>others/word2vec.h</code> <code>GloVe</code> Global Vectors for Word Representation, an unsupervised learning algorithm for obtaining vector representations for words. <code>others/glove.h</code> <code>FastText</code> An extension of Word2Vec that learns vectors for n-grams of characters, allowing it to handle out-of-vocabulary words. <code>others/fast_text.h</code> <code>ELMo</code> Embeddings from Language Models, a deep contextualized word representation. <code>others/elmo.h</code>"},{"location":"api/models/rl/","title":"Reinforcement Learning Models","text":"<p>Reinforcement Learning (RL) is a paradigm of machine learning where an \"agent\" learns to make decisions by performing actions in an \"environment\" to maximize a cumulative reward.</p> <p>Unlike supervised learning, RL models are not trained on a fixed dataset. Instead, they are policies and/or value functions that an agent uses to interact with an environment and learn from the feedback it receives.</p> <p>xTorch provides implementations of several major RL algorithms, encapsulating the underlying neural network architectures (the policies and value functions) that power them.</p> <p>All RL models are located under the <code>xt::models</code> namespace and their headers can be found in the <code>&lt;xtorch/models/reinforcement_learning/&gt;</code> directory.</p>"},{"location":"api/models/rl/#general-usage","title":"General Usage","text":"<p>RL models are used differently from standard supervised models. Instead of a single <code>forward</code> pass on a batch of data, they are typically used within an \"agent-environment loop.\"</p> <ul> <li>A Policy Network takes the current state (observation) from the environment and outputs a probability distribution over possible actions.</li> <li>A Value Network (or Q-Network) takes the current state and outputs an estimated value for each possible action (the expected future reward).</li> </ul> <p>The examples below show how to instantiate and use these two core components.</p>"},{"location":"api/models/rl/#example-using-a-q-network-for-dqn","title":"Example: Using a Q-Network (for DQN)","text":"<pre><code>#include &lt;xtorch/xtorch.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    torch::Device device(torch::cuda::is_available() ? torch::kCUDA : torch::kCPU);\n\n    // --- Environment &amp; Model Properties ---\n    const int num_observations = 4; // E.g., state space size for CartPole\n    const int num_actions = 2;      // E.g., number of possible actions\n\n    // --- Instantiate a DQN Model ---\n    // This is the Q-Network that approximates the action-value function.\n    xt::models::DQN model(num_observations, num_actions);\n    model.to(device);\n    model.eval();\n\n    std::cout &lt;&lt; \"DQN Model (Q-Network) Instantiated.\" &lt;&lt; std::endl;\n\n    // --- Get Action Values for a Given State ---\n    // Create a dummy observation from the environment\n    auto state = torch::randn({1, num_observations}).to(device); // Batch size of 1\n\n    // The model's forward pass returns the Q-value for each action\n    auto action_values = model.forward(state);\n\n    // The agent would then use an epsilon-greedy strategy to select an action\n    auto best_action = torch::argmax(action_values, /*dim=*/1);\n\n    std::cout &lt;&lt; \"Action values: \" &lt;&lt; action_values &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Best action: \" &lt;&lt; best_action.item&lt;long&gt;() &lt;&lt; std::endl;\n}\n</code></pre>"},{"location":"api/models/rl/#example-using-a-policy-network-for-actor-critic","title":"Example: Using a Policy Network (for Actor-Critic)","text":"<pre><code>// --- Instantiate an Actor-Critic Model (like A3C) ---\n// Note: A3C often has a shared body with two heads (policy and value).\n// xt::models::A3C model(num_observations, num_actions);\n// model.to(device);\n// auto [policy_logits, value_estimate] = model.forward(state);\n\n// The policy head gives logits, which are converted to a probability distribution\n// auto action_probabilities = torch::softmax(policy_logits, /*dim=*/-1);\n// torch::distributions::Categorical dist(action_probabilities);\n// auto action = dist.sample(); // Sample an action from the policy\n</code></pre>"},{"location":"api/models/rl/#available-models-by-family","title":"Available Models by Family","text":""},{"location":"api/models/rl/#value-based-methods","title":"Value-Based Methods","text":"<p>These methods learn a value function that estimates the expected return for taking an action in a given state. The policy is often implicit (e.g., \"always take the action with the highest value\").</p> Model Description Header File <code>DQN</code> Deep Q-Network. A foundational algorithm that uses a deep neural network to approximate the optimal action-value function, Q*. <code>dqn.h</code> <code>DoubleDQN</code> An improvement over DQN that decouples action selection from action evaluation to reduce overestimation of Q-values. <code>double_dqn.h</code> <code>DuelingDQN</code> An architecture that separates the estimation of state values and action advantages, leading to better policy evaluation. <code>dueling_dqn.h</code> <code>Rainbow</code> A combination of seven improvements to DQN (including Double, Dueling, Prioritized Replay, etc.) into a single, high-performing agent. <code>rainbow.h</code>"},{"location":"api/models/rl/#policy-based-actor-critic-methods","title":"Policy-Based &amp; Actor-Critic Methods","text":"<p>These methods directly learn a policy that maps states to actions. Actor-Critic methods learn both a policy (the actor) and a value function (the critic) simultaneously.</p> Model Description Header File <code>A3C</code> Asynchronous Advantage Actor-Critic. A classic parallel RL algorithm. <code>a3c.h</code> <code>PPO</code> Proximal Policy Optimization. A highly effective and stable actor-critic method, often a default choice for many continuous control problems. <code>pro.h</code> <code>DDPG</code> Deep Deterministic Policy Gradient. An actor-critic, model-free algorithm for continuous action spaces. <code>ddpg.h</code> <code>TD3</code> Twin Delayed DDPG. An improvement over DDPG that addresses Q-value overestimation by using two critic networks. <code>td3.h</code> <code>SAC</code> Soft Actor-Critic. An off-policy actor-critic algorithm based on the maximum entropy framework, known for its sample efficiency and stability. <code>sac.h</code>"},{"location":"api/models/rl/#model-based-planning-methods","title":"Model-Based &amp; Planning Methods","text":"<p>These methods learn a model of the environment and use it to plan future actions.</p> Model Description Header File <code>AlphaGo</code> The pioneering deep RL program that defeated the world champion Go player, combining Monte Carlo tree search with deep neural networks. <code>alpha_go.h</code> <code>AlphaZero</code> A more generalized and powerful version of AlphaGo that learns entirely from self-play and mastered Go, chess, and shogi. <code>alpha_zero.h</code> <code>MuZero</code> A powerful successor to AlphaZero that achieves superhuman performance by learning a model of the environment and applying tree-based search. <code>mu_zero.h</code>"},{"location":"api/models/rl/#other-architectures","title":"Other Architectures","text":"Model Description Header File <code>IMPALA</code> Importance Weighted Actor-Learner Architecture. A scalable, distributed agent that can be used for both single and multi-task reinforcement learning. <code>impala.h</code>"},{"location":"api/transforms/","title":"Transforms","text":"<p>Transforms are a fundamental component of any deep learning data pipeline. They are functions that take in a data sample (e.g., an image, a piece of text, or an audio clip) and return a modified version of it.</p> <p>This process is essential for two primary reasons: 1.  Preprocessing: To convert data into a format that the neural network can accept. This includes resizing images to a fixed size, normalizing pixel values, or converting text tokens into numerical IDs. 2.  Data Augmentation: To artificially increase the diversity of the training dataset by applying random transformations (like random rotations or flips). This is a powerful regularization technique that helps the model generalize better to unseen data.</p> <p>xTorch provides an extensive library of transforms for a wide variety of data types, mirroring and significantly extending the functionality found in popular Python libraries like <code>torchvision.transforms</code>.</p>"},{"location":"api/transforms/#xttransformscompose","title":"<code>xt::transforms::Compose</code>","text":"<p>A single transform performs one operation. To build a complete preprocessing or augmentation pipeline, you need to chain multiple transforms together. This is the job of <code>xt::transforms::Compose</code>.</p> <p><code>Compose</code> takes a list of transform modules and applies them sequentially to the data.</p>"},{"location":"api/transforms/#general-usage","title":"General Usage","text":"<p>The standard workflow is to create a <code>Compose</code> object containing a list of the desired transform instances. This <code>Compose</code> object is then passed to a <code>Dataset</code> during its construction. The dataset will automatically apply this pipeline to each data sample it retrieves.</p> <pre><code>#include &lt;xtorch/xtorch.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    // --- 1. Create a list of transform instances ---\n    // This pipeline performs common data augmentation for image classification.\n    std::vector&lt;std::shared_ptr&lt;xt::Module&gt;&gt; transform_list;\n    transform_list.push_back(\n        std::make_shared&lt;xt::transforms::image::Resize&gt;(std::vector&lt;int64_t&gt;{256, 256})\n    );\n    transform_list.push_back(\n        std::make_shared&lt;xt::transforms::image::RandomCrop&gt;(std::vector&lt;int64_t&gt;{224, 224})\n    );\n    transform_list.push_back(\n        std::make_shared&lt;xt::transforms::image::RandomHorizontalFlip&gt;(/*p=*/0.5)\n    );\n    transform_list.push_back(\n        std::make_shared&lt;xt::transforms::general::Normalize&gt;(\n            std::vector&lt;float&gt;{0.485, 0.456, 0.406}, // Mean for ImageNet\n            std::vector&lt;float&gt;{0.229, 0.224, 0.225}  // Std for ImageNet\n        )\n    );\n\n    // --- 2. Create the Compose object ---\n    auto transform_pipeline = std::make_unique&lt;xt::transforms::Compose&gt;(transform_list);\n\n    // --- 3. Pass the pipeline to a Dataset ---\n    // The ImageFolderDataset will now apply these augmentations to every image it loads.\n    auto dataset = xt::datasets::ImageFolderDataset(\n        \"/path/to/your/image/data/\",\n        std::move(transform_pipeline)\n    );\n\n    // The rest of the workflow (DataLoader, Trainer) remains the same.\n    xt::dataloaders::ExtendedDataLoader data_loader(dataset, 32);\n    // ...\n}\n</code></pre>"},{"location":"api/transforms/#transforms-by-data-modality","title":"Transforms by Data Modality","text":"<p>The xTorch transform library is organized by the type of data it operates on. Follow the links below for a detailed list of available transforms in each category.</p> <ul> <li> <p>Appliers: Meta-transforms that control how other transforms are applied, such as <code>RandomApply</code> or <code>OneOf</code>.</p> </li> <li> <p>Image: The largest collection, containing transforms for geometric adjustments (resize, crop, rotate, flip), color jittering, normalization, and advanced augmentations like Cutout and MixUp.</p> </li> <li> <p>Signal (Audio): Transforms for processing audio waveforms, including creating spectrograms (<code>MelSpectrogram</code>), applying time and frequency masking, and changing pitch or speed.</p> </li> <li> <p>Text: Transforms for NLP, including tokenizers (<code>BertTokenizer</code>, <code>SentencePieceTokenizer</code>), and utilities for padding and truncating sequences.</p> </li> <li> <p>Graph: Transforms for augmenting graph-structured data, such as dropping nodes (<code>NodeDrop</code>) or edges (<code>EdgeDrop</code>).</p> </li> <li> <p>Video: Transforms for processing sequences of images, such as temporal subsampling.</p> </li> <li> <p>Weather: A unique collection of transforms that can simulate various weather conditions (rain, fog, snow) on images, useful for training robust autonomous driving models.</p> </li> </ul>"},{"location":"api/transforms/appliers/","title":"Transform Appliers","text":"<p>Appliers, or meta-transforms, are a special category of transformations that do not modify the data directly. Instead, they control how and when other transforms are applied.</p> <p>They are the building blocks for creating complex, dynamic, and probabilistic data augmentation pipelines. By combining simple transforms with appliers, you can construct sophisticated augmentation strategies.</p> <p>All applier transforms are located under the <code>xt::transforms</code> namespace and can be found in the <code>&lt;xtorch/transforms/appliers/&gt;</code> header directory.</p>"},{"location":"api/transforms/appliers/#compose","title":"<code>Compose</code>","text":"<p>This is the most fundamental and widely used applier. It chains a list of transforms together and applies them sequentially. It is the primary tool for building any multi-step data processing pipeline.</p> <p><code>Compose</code> is covered in detail on the main Transforms page, but its usage is recapped here for completeness.</p>"},{"location":"api/transforms/appliers/#usage","title":"Usage","text":"<pre><code>auto pipeline = std::make_unique&lt;xt::transforms::Compose&gt;(\n    std::make_shared&lt;xt::transforms::image::Resize&gt;(std::vector&lt;int64_t&gt;{256, 256}),\n    std::make_shared&lt;xt::transforms::image::CenterCrop&gt;(std::vector&lt;int64_t&gt;{224, 224}),\n    std::make_shared&lt;xt::transforms::general::Normalize&gt;(mean, std)\n);\n</code></pre>"},{"location":"api/transforms/appliers/#randomapply","title":"<code>RandomApply</code>","text":"<p>Applies a given transform with a specific probability. This is essential for data augmentation, as you often don't want to apply a transformation to every single sample.</p>"},{"location":"api/transforms/appliers/#usage_1","title":"Usage","text":"<pre><code>// This pipeline will apply Gaussian Blur to 30% of the images that pass through it.\nauto pipeline = std::make_unique&lt;xt::transforms::Compose&gt;(\n    std::make_shared&lt;xt::transforms::RandomApply&gt;(\n        std::make_shared&lt;xt::transforms::image::GaussianBlur&gt;(/*kernel_size=*/3),\n        /*p=*/0.3 // The probability of application\n    )\n);\n</code></pre>"},{"location":"api/transforms/appliers/#oneof","title":"<code>OneOf</code>","text":"<p>Takes a list of transforms and randomly selects and applies exactly one of them to the data. This is useful when you want to choose from a set of mutually exclusive augmentations.</p>"},{"location":"api/transforms/appliers/#usage_2","title":"Usage","text":"<pre><code>// This pipeline will apply EITHER Gaussian Blur OR Motion Blur to each image.\nauto pipeline = std::make_unique&lt;xt::transforms::Compose&gt;(\n    std::make_shared&lt;xt::transforms::OneOf&gt;(\n        std::vector&lt;std::shared_ptr&lt;xt::Module&gt;&gt;{\n            std::make_shared&lt;xt::transforms::image::GaussianBlur&gt;(3),\n            std::make_shared&lt;xt::transforms::image::MotionBlur&gt;(/*kernel_size=*/5)\n        }\n    )\n);\n</code></pre>"},{"location":"api/transforms/appliers/#someof","title":"<code>SomeOf</code>","text":"<p>Takes a list of transforms and an integer <code>n</code>, then randomly selects and applies exactly <code>n</code> of them to the data.</p>"},{"location":"api/transforms/appliers/#usage_3","title":"Usage","text":"<pre><code>// This pipeline will randomly select and apply exactly TWO of the three listed transforms.\nauto pipeline = std::make_unique&lt;xt::transforms::Compose&gt;(\n    std::make_shared&lt;xt::transforms::SomeOf&gt;(\n        std::vector&lt;std::shared_ptr&lt;xt::Module&gt;&gt;{\n            std::make_shared&lt;xt::transforms::image::GaussianBlur&gt;(3),\n            std::make_shared&lt;xt::transforms::image::Sharpen&gt;(),\n            std::make_shared&lt;xt::transforms::image::Emboss&gt;()\n        },\n        /*n=*/2 // Apply 2 transforms from the list\n    )\n);\n</code></pre>"},{"location":"api/transforms/appliers/#sometimes","title":"<code>Sometimes</code>","text":"<p>Applies a list of transforms sequentially, but each transform is only applied with a given probability <code>p</code>. This is like a <code>Compose</code> where every step is wrapped in <code>RandomApply</code>.</p>"},{"location":"api/transforms/appliers/#usage_4","title":"Usage","text":"<pre><code>// This pipeline will go through three steps.\n// First, it has a 50% chance of applying a flip.\n// Then, it has a 30% chance of applying a rotation.\n// Finally, it has a 25% chance of applying noise.\nauto pipeline = std::make_unique&lt;xt::transforms::Compose&gt;(\n    std::make_shared&lt;xt::transforms::Sometimes&gt;(\n        std::vector&lt;std::shared_ptr&lt;xt::Module&gt;&gt;{\n            std::make_shared&lt;xt::transforms::image::RandomHorizontalFlip&gt;(),\n            std::make_shared&lt;xt::transforms::image::RandomRotation&gt;(15),\n            std::make_shared&lt;xt::transforms::image::GaussianNoise&gt;()\n        },\n        /*p=*/0.5 // Default probability if not specified per-transform\n    )\n);\n</code></pre>"},{"location":"api/transforms/appliers/#other-appliers","title":"Other Appliers","text":"Applier Description Header File <code>Repeat</code> Applies a given transform <code>n</code> times in a row. <code>appliers/repeat.h</code> <code>ReplayCompose</code> A variant of <code>Compose</code> designed for deterministic augmentations on paired data (e.g., applying the exact same random crop to both an image and its segmentation mask). <code>appliers/replay_compose.h</code> <code>Palindrome</code> Applies a sequence of transforms and then applies them again in reverse order. <code>appliers/palindrome.h</code>"},{"location":"api/transforms/graph/","title":"Graph Transforms","text":"<p>Data augmentation for graph-structured data is a powerful technique for improving the generalization of Graph Neural Networks (GNNs). Unlike image or audio transforms, graph transforms operate on the structure and features of a graph, such as its nodes, edges, and feature matrices.</p> <p>These augmentations can help the model learn more robust representations by training on variations of the input graph, preventing it from overfitting to specific structural or feature patterns.</p> <p>All graph transforms are located under the <code>xt::transforms</code> namespace and can be found in the <code>&lt;xtorch/transforms/graph/&gt;</code> header directory.</p>"},{"location":"api/transforms/graph/#general-usage","title":"General Usage","text":"<p>Graph transforms are designed to be used within a <code>Compose</code> pipeline, just like transforms for other data modalities. They are typically applied within a <code>Dataset</code>'s preprocessing step. A graph transform takes a graph data object (which usually contains node features <code>x</code> and an <code>edge_index</code> tensor) and returns a modified version.</p> <pre><code>#include &lt;xtorch/xtorch.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    // 1. Define a pipeline of graph augmentations\n    auto transform_pipeline = std::make_unique&lt;xt::transforms::Compose&gt;(\n        // Randomly drop 10% of the nodes during training\n        std::make_shared&lt;xt::transforms::graph::NodeDrop&gt;(/*p=*/0.1),\n        // Randomly drop 20% of the edges\n        std::make_shared&lt;xt::transforms::graph::EdgeDrop&gt;(/*p=*/0.2)\n    );\n\n    // 2. Pass the pipeline to a graph Dataset\n    // The dataset will apply these augmentations to the graph data each time a sample is requested.\n    auto dataset = xt::datasets::Cora(\n        \"./data\",\n        /*download=*/true,\n        std::move(transform_pipeline)\n    );\n\n    // 3. The DataLoader will now provide augmented graphs during training\n    xt::dataloaders::ExtendedDataLoader data_loader(dataset, /*batch_size=*/1);\n\n    for (auto&amp; batch : data_loader) {\n        // The graph data in this batch has been randomly modified by the transforms\n        auto augmented_graph_data = batch;\n        // ... training step ...\n    }\n}\n</code></pre>"},{"location":"api/transforms/graph/#available-transforms","title":"Available Transforms","text":"<p>The graph transforms can be grouped into two main categories: those that modify the graph's structure and those that modify its features.</p>"},{"location":"api/transforms/graph/#structural-augmentations","title":"Structural Augmentations","text":"<p>These transforms alter the topology of the graph by adding, removing, or modifying nodes and edges.</p> Transform Description Header File <code>NodeDrop</code> Randomly removes a fraction of nodes (and their connections) from the graph. <code>node_drop.h</code> <code>EdgeDrop</code> Randomly removes a fraction of edges from the graph. <code>edge_drop.h</code> <code>EdgeAdd</code> Randomly adds new edges to the graph. <code>edge_add.h</code> <code>EdgePerturbation</code> Randomly adds some edges and removes others. <code>edge_perturbation.h</code> <code>Subgraph</code> Samples a subgraph from the input graph based on a subset of nodes. <code>subgraph.h</code> <code>RandomWalkSubgraph</code> Samples a subgraph by performing random walks from a set of starting nodes. <code>random_walk_subgraph.h</code> <code>GraphCoarsening</code> Reduces the size of the graph by merging nodes. <code>graph_coarsening.h</code>"},{"location":"api/transforms/graph/#feature-augmentations","title":"Feature Augmentations","text":"<p>These transforms alter the feature matrices of the nodes or edges.</p> Transform Description Header File <code>NodeFeatureMasking</code> Randomly masks (sets to zero) the features of some nodes. <code>node_feature_masking.h</code> <code>NodeFeatureShuffling</code> Randomly shuffles the features across different nodes. <code>node_feature_shuffling.h</code> <code>EdgeFeatureMasking</code> Randomly masks the features of some edges. <code>edge_feature_masking.h</code> <code>FeatureDropout</code> Applies dropout to the node or edge feature matrix. <code>feature_dropout.h</code> <code>FeatureAugmentation</code> Augments node features by adding random noise or other perturbations. <code>feature_augmentation.h</code>"},{"location":"api/transforms/graph/#hybrid-augmentations","title":"Hybrid Augmentations","text":"<p>These transforms modify both the structure and features of the graph simultaneously.</p> Transform Description Header File <code>DropEdgeAndFeature</code> Randomly drops edges and masks node features. <code>drop_edge_and_feature.h</code> <code>GraphMixUp</code> A graph-based implementation of MixUp, creating new graphs by interpolating between two existing graphs. <code>graph_mix_up.h</code> <code>NodeMixUp</code> Creates new node features by interpolating between features of other nodes. <code>node_mix_up.h</code> <code>GraphDiffusion</code> Applies a diffusion process to the graph's features. <code>graph_diffusion.h</code> <p>Constructor Options</p> <p>Many of these transforms have tunable parameters, such as the probability <code>p</code> of dropping a node/edge. These are typically passed as arguments to the constructor. Please refer to the specific header file for details on the available settings.</p>"},{"location":"api/transforms/image/","title":"Image Transforms","text":"<p>Image transforms are essential for nearly every computer vision task. They are used for preprocessing (e.g., resizing images to a uniform size, normalizing pixel values) and, critically, for data augmentation (e.g., applying random rotations, crops, and color shifts to the training data). Data augmentation is a key technique for preventing overfitting and improving model robustness.</p> <p>xTorch provides a massive library of image transforms, from basic operations to advanced, state-of-the-art augmentation techniques.</p> <p>All image transforms are located under the <code>xt::transforms::image</code> namespace and their headers can be found in the <code>&lt;xtorch/transforms/image/&gt;</code> directory.</p>"},{"location":"api/transforms/image/#general-usage","title":"General Usage","text":"<p>The most common way to use image transforms is to chain them together into a pipeline using <code>xt::transforms::Compose</code>. This pipeline is then passed to your <code>Dataset</code> during construction, which will apply the transformations to each image as it is loaded.</p> <pre><code>#include &lt;xtorch/xtorch.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    // This is a standard data augmentation pipeline for training on ImageNet\n    auto training_transforms = std::make_unique&lt;xt::transforms::Compose&gt;(\n        // Resize the smaller edge to 256, maintaining aspect ratio\n        std::make_shared&lt;xt::transforms::image::Resize&gt;(256),\n        // Randomly crop a 224x224 patch\n        std::make_shared&lt;xt::transforms::image::RandomCrop&gt;(std::vector&lt;int64_t&gt;{224, 224}),\n        // Randomly flip the image horizontally with a 50% probability\n        std::make_shared&lt;xt::transforms::image::RandomHorizontalFlip&gt;(/*p=*/0.5),\n        // Apply some color jitter\n        std::make_shared&lt;xt::transforms::image::ColorJitter&gt;(),\n        // Normalize the image using ImageNet mean and stddev\n        std::make_shared&lt;xt::transforms::general::Normalize&gt;(\n            std::vector&lt;float&gt;{0.485, 0.456, 0.406},\n            std::vector&lt;float&gt;{0.229, 0.224, 0.225}\n        )\n    );\n\n    // This pipeline would be passed to a dataset\n    // auto dataset = xt::datasets::ImageFolderDataset(\"./data\", std::move(training_transforms));\n}\n</code></pre> <p>Constructor Options</p> <p>Nearly all transforms are configurable through their constructors. This includes parameters like sizes, probabilities (<code>p</code>), rotation degrees, and more. Always refer to the specific header file in <code>&lt;xtorch/transforms/image/&gt;</code> for a full list of available settings.</p>"},{"location":"api/transforms/image/#available-transforms-by-category","title":"Available Transforms by Category","text":""},{"location":"api/transforms/image/#geometric-transforms","title":"Geometric Transforms","text":"<p>These transforms alter the spatial properties of the image.</p> Transform Description <code>Resize</code> Resizes the input image to a given size. <code>Scale</code> An alias for <code>Resize</code>. <code>LongestMaxSize</code> Resizes the longest edge to a max size, maintaining aspect ratio. <code>SmallestMaxSize</code> Resizes the smallest edge to a max size, maintaining aspect ratio. <code>Crop</code> Crops the image at a specified location and size. <code>CenterCrop</code> Crops the central part of the image. <code>RandomCrop</code> Crops a random part of the image. <code>RandomResizedCrop</code> Crops a random part of the image and resizes it to a specific size. A common training augmentation. <code>Flip</code> Flips the image vertically, horizontally, or both. <code>HorizontalFlip</code> Flips the image horizontally. <code>VerticalFlip</code> Flips the image vertically. <code>RandomHorizontalFlip</code> Randomly flips the image horizontally with a given probability. <code>RandomVerticalFlip</code> Randomly flips the image vertically with a given probability. <code>RandomFlip</code> Randomly flips the image horizontally and/or vertically. <code>Pad</code> Pads the image on all sides with a given value. <code>PadIfNeeded</code> Pads the image to a minimum height and width. <code>Rotation</code> Rotates the image by a specified angle. <code>RandomRotation</code> Rotates the image by a random angle within a given range. <code>Affine</code> Applies a general affine transformation to the image. <code>RandomAffine</code> Applies a random affine transformation. <code>Perspective</code> Applies a perspective transformation. <code>RandomPerspective</code> Applies a random perspective transformation. <code>ElasticTransform</code> Applies an elastic deformation to the image. <code>GridDistortion</code> Applies a grid distortion effect. <code>OpticalDistortion</code> Applies an optical barrel/pincushion distortion."},{"location":"api/transforms/image/#color-photometric-transforms","title":"Color &amp; Photometric Transforms","text":"<p>These transforms alter the pixel values, colors, brightness, and contrast of the image.</p> Transform Description <code>ColorJitter</code> Randomly changes the brightness, contrast, saturation, and hue of an image. <code>RandomBrightnessContrast</code> Randomly changes the brightness and contrast. <code>Grayscale</code> Converts the image to grayscale. <code>RandomGrayscale</code> Randomly converts the image to grayscale with a given probability. <code>Posterize</code> Reduces the number of bits for each color channel. <code>RandomPosterize</code> Randomly applies posterization. <code>Solarize</code> Inverts all pixel values above a threshold. <code>RandomSolarize</code> Randomly applies solarization. <code>Invert</code> Inverts the colors of the image. <code>RandomInvert</code> Randomly inverts the colors. <code>Equalize</code> Applies histogram equalization to the image. <code>RandomEqualize</code> Randomly applies histogram equalization. <code>CLAHE</code> Applies Contrast Limited Adaptive Histogram Equalization. <code>ChannelShuffle</code> Randomly shuffles the color channels of the image. <code>RandomGamma</code> Applies random gamma correction. <code>RandomAdjustSharpness</code> Randomly adjusts the sharpness of the image. <code>RandomAutoContrast</code> Randomly applies automatic contrast adjustment. <code>FancyPCA</code> Applies PCA-based color augmentation."},{"location":"api/transforms/image/#augmentation-erasing-transforms","title":"Augmentation &amp; Erasing Transforms","text":"<p>These are advanced augmentation techniques that often involve erasing or mixing parts of images.</p> Transform Description <code>Cutout</code> Randomly erases one or more rectangular patches from an image. <code>CoarseDropout</code> An alternative name for <code>Cutout</code>. <code>GridDropout</code> Erases a grid of patches from an image. <code>MaskDropout</code> Applies dropout to a mask. <code>MixUp</code> Creates a new image by taking a weighted linear interpolation of two images. <code>CutMix</code> Creates a new image by cutting a patch from one image and pasting it onto another. <code>RandomMosaic</code> Combines four images into a single mosaic. <code>RandomAugment</code> Automatically applies a sequence of randomly selected augmentations (similar to AutoAugment). <code>GridShuffle</code> Shuffles patches of the image arranged in a grid."},{"location":"api/transforms/image/#blur-noise-transforms","title":"Blur &amp; Noise Transforms","text":"Transform Description <code>Blur</code> Blurs the image using a normalized box filter. <code>GaussianBlur</code> Blurs the image using a Gaussian filter. <code>MedianBlur</code> Blurs the image using a median filter. <code>MotionBlur</code> Applies motion blur to the image. <p>| <code>GlassBlur</code> | Applies a glass-like blur effect. | | <code>ZoomBlur</code> | Applies a blur that simulates zooming. | | <code>GaussianNoise</code>| Adds Gaussian noise to the image. | | <code>NoiseInjection</code>| Injects random noise into the image. |</p>"},{"location":"api/transforms/image/#stylistic-filter-based-transforms","title":"Stylistic &amp; Filter-Based Transforms","text":"Transform Description <code>Sharpen</code> Sharpens the image. <code>Emboss</code> Applies an embossing filter to the image. <code>ToSepia</code> Applies a sepia filter to the image. <code>BlackWhite</code> Converts the image to black and white. <code>Spatter</code> Adds a \"spatter\" effect to the image, like drops on a camera lens."},{"location":"api/transforms/signal/","title":"Signal (Audio) Transforms","text":"<p>Signal transforms are a critical part of any audio-based deep learning pipeline. They are used to convert raw audio waveforms into representations that are more suitable for neural networks, and to perform data augmentation to improve model robustness.</p> <p>Common tasks include converting a time-domain waveform into a time-frequency representation (like a spectrogram) and applying augmentations like pitch shifting or adding background noise.</p> <p>All signal transforms are located under the <code>xt::transforms::signal</code> namespace and can be found in the <code>&lt;xtorch/transforms/signal/&gt;</code> header directory.</p>"},{"location":"api/transforms/signal/#general-usage","title":"General Usage","text":"<p>Audio transforms are designed to be chained together in a <code>Compose</code> pipeline and passed to an audio <code>Dataset</code>. The dataset will then apply this pipeline to each raw audio waveform it loads.</p> <pre><code>#include &lt;xtorch/xtorch.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    // 1. Define a pipeline of audio transformations\n    // This pipeline converts a raw waveform to a Mel Spectrogram and then applies augmentation.\n    auto training_transforms = std::make_unique&lt;xt::transforms::Compose&gt;(\n        // Convert the waveform to a Mel Spectrogram\n        std::make_shared&lt;xt::transforms::signal::MelSpectrogram&gt;(\n            /*sample_rate=*/16000,\n            /*n_fft=*/400,\n            /*n_mels=*/128\n        ),\n        // Apply Frequency Masking for data augmentation\n        std::make_shared&lt;xt::transforms::signal::FrequencyMasking&gt;(\n            /*freq_mask_param=*/80\n        ),\n        // Apply Time Masking for data augmentation\n        std::make_shared&lt;xt::transforms::signal::TimeMasking&gt;(\n            /*time_mask_param=*/100\n        )\n    );\n\n    // 2. Pass the pipeline to an audio Dataset\n    // The SpeechCommands dataset will now apply these transforms to each audio clip.\n    auto dataset = xt::datasets::SpeechCommands(\n        \"./data\",\n        xt::datasets::DataMode::TRAIN,\n        /*download=*/true,\n        std::move(training_transforms)\n    );\n\n    // The data loader will yield batches of augmented spectrograms\n    xt::dataloaders::ExtendedDataLoader data_loader(dataset, 32);\n    // ...\n}\n</code></pre> <p>Constructor Options</p> <p>Audio transforms are often highly configurable with parameters like <code>sample_rate</code>, <code>n_fft</code> (FFT window size), <code>hop_length</code>, and <code>n_mels</code> (number of Mel bands). Always refer to the specific header file in <code>&lt;xtorch/transforms/signal/&gt;</code> for a full list of available settings.</p>"},{"location":"api/transforms/signal/#available-transforms-by-category","title":"Available Transforms by Category","text":""},{"location":"api/transforms/signal/#time-frequency-representations","title":"Time-Frequency Representations","text":"<p>These are the most common preprocessing steps, converting a 1D waveform into a 2D image-like representation.</p> Transform Description Header File <code>Spectrogram</code> Creates a standard spectrogram from a waveform. <code>spectrogram.h</code> <code>MelSpectrogram</code> Creates a Mel-scaled spectrogram, which is a perceptually relevant representation of audio. <code>mel_spectrogram.h</code> <code>MFCC</code> Mel-Frequency Cepstral Coefficients, a compact representation of the spectral envelope. <code>mfcc.h</code> <code>InverseMelScale</code> Converts a Mel-spectrogram to a regular spectrogram. <code>inverse_mel_scale.h</code> <code>GriffinLim</code> An algorithm to estimate a waveform from a spectrogram (phase reconstruction). <code>griffin_lim.h</code>"},{"location":"api/transforms/signal/#data-augmentation","title":"Data Augmentation","text":"<p>These transforms modify the audio to create new training samples, improving model generalization.</p> Transform Description Header File <code>TimeMasking</code> Randomly masks a range of consecutive time steps in a spectrogram. A key component of SpecAugment. <code>time_masking.h</code> <code>FrequencyMasking</code> Randomly masks a range of consecutive frequency channels in a spectrogram. A key component of SpecAugment. <code>frequency_masking.h</code> <code>AddNoise</code> Adds random noise to the audio waveform. <code>add_noise.h</code> <code>BackgroundNoiseAddition</code> Mixes the audio with random clips from a provided set of background noise files. <code>background_noise_addition.h</code> <code>PitchShift</code> Shifts the pitch of the audio up or down without changing the tempo. <code>pitch_shift.h</code> <code>TimeStretch</code> Stretches or compresses the audio in time without changing the pitch. <code>time_stretch.h</code> <code>SpeedPerturbation</code> Changes the speed of the audio, which affects both pitch and tempo. Commonly used in ASR. <code>speed_perturbation.h</code> <code>DeReverberation</code> Applies a de-reverberation effect to the audio. <code>de_reverberation.h</code> <code>TimeWarping</code> Applies a non-linear warp along the time axis of a spectrogram. <code>time_warping.h</code> <code>Vol</code> Changes the volume of the audio. <code>vol.h</code>"},{"location":"api/transforms/signal/#other-utility-transforms","title":"Other Utility Transforms","text":"Transform Description Header File <code>Resample</code> Resamples the audio waveform from one sampling rate to another. <code>resample.h</code> <code>SlidingWindowCMN</code> Cepstral Mean and Variance Normalization, a common technique in speech recognition. <code>sliding_window_cmn.h</code> <code>WaveletTransforms</code> Applies wavelet transforms to the signal. <code>wavelet_transforms.h</code>"},{"location":"api/transforms/text/","title":"Text Transforms","text":"<p>Text transforms are essential for preparing raw text data for use in neural networks. Unlike images, which are already numerical, text is a sequence of characters that must be converted into a structured, numerical format\u2014typically a tensor of integer IDs.</p> <p>The two primary steps in any NLP data pipeline are: 1.  Tokenization: The process of breaking a raw string of text into smaller pieces called \"tokens\". These can be words, subwords, or characters. 2.  Numericalization: The process of converting each token into a unique integer ID based on a pre-defined \"vocabulary\".</p> <p>xTorch provides a suite of transforms to handle these steps, from powerful tokenizers to utilities for managing sequence length.</p> <p>All text transforms are located under the <code>xt::transforms::text</code> namespace and can be found in the <code>&lt;xtorch/transforms/text/&gt;</code> header directory.</p>"},{"location":"api/transforms/text/#general-usage","title":"General Usage","text":"<p>Text transforms are used within a <code>Compose</code> pipeline, which is then passed to an NLP <code>Dataset</code>. A key difference from other modalities is the reliance on a vocabulary, a mapping from tokens to integers. This vocabulary is often built from the training corpus or loaded from a pre-trained model's files.</p> <pre><code>#include &lt;xtorch/xtorch.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    // In a real application, you would load a vocabulary from a file.\n    // For example, for a BERT model, this would be a 'vocab.txt' file.\n    // std::string vocab_path = \"path/to/bert/vocab.txt\";\n\n    // 1. Define a pipeline of text transformations.\n    auto text_pipeline = std::make_unique&lt;xt::transforms::Compose&gt;(\n        // Tokenize the input string using the BERT WordPiece tokenizer\n        std::make_shared&lt;xt::transforms::text::BertTokenizer&gt;(/*vocab_path=*/vocab_path),\n        // Truncate sequences to a maximum length of 512 tokens\n        std::make_shared&lt;xt::transforms::text::Truncate&gt;(512),\n        // Add special tokens like [CLS] and [SEP]\n        std::make_shared&lt;xt::transforms::text::AddToken&gt;(\"[CLS]\", /*at_beginning=*/true),\n        std::make_shared&lt;xt::transforms::text::AddToken&gt;(\"[SEP]\", /*at_beginning=*/false)\n        // Note: Padding is often handled by the DataLoader's collate function,\n        // but can also be a transform.\n    );\n\n    // 2. Pass the pipeline to an NLP Dataset\n    auto dataset = xt::datasets::IMDB(\n        \"./data\",\n        xt::datasets::DataMode::TRAIN,\n        /*download=*/true,\n        std::move(text_pipeline)\n    );\n\n    // 3. The DataLoader will now yield batches of tokenized and numericalized text\n    xt::dataloaders::ExtendedDataLoader data_loader(dataset, 16);\n    // ...\n}\n</code></pre>"},{"location":"api/transforms/text/#available-transforms-by-category","title":"Available Transforms by Category","text":""},{"location":"api/transforms/text/#tokenizers","title":"Tokenizers","text":"<p>These modules are responsible for the first step: converting a string into a sequence of tokens.</p> Transform Description Header File <code>BertTokenizer</code> Implements the WordPiece tokenization algorithm used by BERT. It requires a <code>vocab.txt</code> file. <code>bert_tokenizer.h</code> <code>SentencePieceTokenizer</code> A tokenizer that uses the SentencePiece library, common in models like XLNet and T5. It requires a <code>spm.model</code> file. <code>sentence_piece_tokenizer.h</code>"},{"location":"api/transforms/text/#vocabulary-and-numericalization","title":"Vocabulary and Numericalization","text":"<p>These transforms handle the conversion between tokens and integer IDs.</p> Transform Description Header File <code>VocabTransform</code> A transform that takes a vocabulary object and converts a sequence of string tokens into a sequence of integer IDs. <code>vocab_transform.h</code> <code>StrToIntTransform</code> A lower-level transform for converting strings to integers, often used internally by <code>VocabTransform</code>. <code>str_to_int_transform.h</code>"},{"location":"api/transforms/text/#sequence-utilities","title":"Sequence Utilities","text":"<p>These transforms are used to format the token sequences to meet the model's requirements.</p> Transform Description Header File <code>PadTransform</code> Pads a sequence to a specified length with a given padding token ID. <code>pad_transform.h</code> <code>Truncate</code> Truncates a sequence to a maximum specified length. <code>truncate.h</code> <code>AddToken</code> Adds a special token (e.g., <code>[CLS]</code>, <code>[SEP]</code>, <code>[EOS]</code>) to the beginning or end of a sequence. <code>add_token.h</code>"},{"location":"api/transforms/text/#data-augmentation","title":"Data Augmentation","text":"<p>These transforms modify the input text to create new training samples, which can help improve model robustness.</p> Transform Description Header File <code>SynonymReplacement</code> Randomly replaces words in a sentence with their synonyms. <code>synonym_replacement.h</code> <code>BackTranslation</code> Augments text by translating it to another language and then translating it back to the original language. (Note: May require an external translation API). <code>back_translation.h</code> <code>TextStyleTransfer</code> A transform for altering the style of the text. <code>text_style_transfer.h</code>"},{"location":"api/transforms/video/","title":"Video Transforms","text":"<p>Video data adds a temporal dimension to the challenges of computer vision. A video is a sequence of image frames, and processing this data requires handling both the spatial content of each frame and the temporal relationships between them.</p> <p>Video transforms are designed to operate on these sequences of frames. They are essential for preparing video clips for input to models like video classifiers or action recognition networks.</p> <p>All video transforms are located under the <code>xt::transforms</code> namespace and can be found in the <code>&lt;xtorch/transforms/video/&gt;</code> header directory.</p>"},{"location":"api/transforms/video/#general-usage","title":"General Usage","text":"<p>Video transforms are used in a <code>Compose</code> pipeline, just like image transforms. However, they expect their input to be a tensor with a temporal dimension, typically with a shape like <code>[Time, Channels, Height, Width]</code>.</p> <p>A common workflow involves: 1.  Loading a video and decoding it into a sequence of frames. 2.  Applying temporal transforms to select or sample frames. 3.  Applying spatial (image) transforms to each of the selected frames.</p> <pre><code>#include &lt;xtorch/xtorch.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    // 1. Define a pipeline of video and image transformations\n    auto video_pipeline = std::make_unique&lt;xt::transforms::Compose&gt;(\n        // --- Temporal Transforms ---\n        // Sample 16 frames uniformly from the video clip\n        std::make_shared&lt;xt::transforms::video::UniformTemporalSubsample&gt;(16),\n        // Randomly reverse the sequence of the 16 frames with a 50% probability\n        std::make_shared&lt;xt::transforms::video::RandomClipReverse&gt;(0.5),\n\n        // --- Spatial Transforms (applied to each frame) ---\n        // Note: You would typically wrap spatial transforms to apply them per-frame.\n        // For simplicity, we assume the dataset handles this application logic.\n        std::make_shared&lt;xt::transforms::image::Resize&gt;(std::vector&lt;int64_t&gt;{128, 128}),\n        std::make_shared&lt;xt::transforms::general::Normalize&gt;(\n            std::vector&lt;float&gt;{0.5, 0.5, 0.5},\n            std::vector&lt;float&gt;{0.5, 0.5, 0.5}\n        )\n    );\n\n    // 2. Pass the pipeline to a video Dataset\n    // The dataset will apply these transforms to each video it loads.\n    // auto dataset = xt::datasets::UCF101(\"./data\", std::move(video_pipeline));\n\n    // 3. The DataLoader will yield batches of processed video clips\n    // xt::dataloaders::ExtendedDataLoader data_loader(dataset, 8);\n    // for (auto&amp; batch : data_loader) {\n    //     auto video_clips = batch.first; // Shape e.g.,\n    // }\n}\n</code></pre>"},{"location":"api/transforms/video/#available-video-transforms","title":"Available Video Transforms","text":"<p>xTorch provides the following transforms for video data:</p>"},{"location":"api/transforms/video/#temporal-transforms","title":"Temporal Transforms","text":"<p>These transforms operate along the time dimension of a video clip.</p> Transform Description Header File <code>UniformTemporalSubsample</code> Subsamples a fixed number of frames from a video clip at a uniform frame rate. This is a common way to create a fixed-size input from videos of varying lengths. <code>uniform_temporal_subsample.h</code> <code>RandomClipReverse</code> Randomly reverses the order of frames in a clip with a given probability. A simple form of temporal data augmentation. <code>random_clip_reverse.h</code>"},{"location":"api/transforms/video/#spatio-temporal-transforms","title":"Spatio-Temporal Transforms","text":"<p>These transforms modify both the spatial and temporal aspects of the video.</p> Transform Description Header File <code>FrameInterpolation</code> Generates intermediate frames between existing ones, which can be used to increase the frame rate or for slow-motion effects. <code>frame_interpolation.h</code> <code>OpticalFlowWarping</code> Warps video frames based on calculated optical flow, a technique used in video compression and frame rate conversion. <code>optical_flow_warping.h</code> <p>Applying Image Transforms to Videos</p> <p>To apply a standard image transform (like <code>RandomCrop</code> or <code>ColorJitter</code>) to every frame of a video, you typically need to iterate over the time dimension and apply the transform to each <code>[C, H, W]</code> frame individually. The <code>Compose</code> applier can be used to chain these operations. Support for per-frame application wrappers may be included in the future.</p>"},{"location":"api/transforms/weather/","title":"Weather Transforms","text":"<p>Weather transforms are a unique and powerful category of image augmentation designed to simulate realistic adverse weather and environmental conditions.</p> <p>Their primary application is to create a more robust training dataset for models that need to operate in the real world, such as those used in autonomous driving and outdoor robotics. By training a model on images with simulated rain, fog, and snow, you can significantly improve its performance and reliability when it encounters these conditions after deployment.</p> <p>All weather transforms are located under the <code>xt::transforms</code> namespace and can be found in the <code>&lt;xtorch/transforms/weather/&gt;</code> header directory.</p>"},{"location":"api/transforms/weather/#general-usage","title":"General Usage","text":"<p>Weather transforms are used just like any other image transform. You can add them to a <code>Compose</code> pipeline to apply weather effects to your training images. Since these are often complex and computationally intensive, they are typically applied with a certain probability using <code>RandomApply</code> or <code>OneOf</code>.</p> <pre><code>#include &lt;xtorch/xtorch.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    // 1. Define a pipeline that can apply various weather augmentations\n    auto weather_augmentation_pipeline = std::make_unique&lt;xt::transforms::Compose&gt;(\n        // For each image, randomly choose ONE of the following weather effects to apply\n        std::make_shared&lt;xt::transforms::OneOf&gt;(\n            std::vector&lt;std::shared_ptr&lt;xt::Module&gt;&gt;{\n                // Add particle-based rain to the image\n                std::make_shared&lt;xt::transforms::weather::ParticleRain&gt;(),\n                // Add patchy fog\n                std::make_shared&lt;xt::transforms::weather::PatchyFog&gt;(),\n                // Add falling snow\n                std::make_shared&lt;xt::transforms::weather::FallingSnow&gt;(),\n                // Add sun flare\n                std::make_shared&lt;xt::transforms::weather::SunFlare&gt;()\n            }\n        )\n    );\n\n    // 2. Pass the pipeline to a Dataset\n    // This is especially useful for datasets like Cityscapes or KITTI.\n    // auto dataset = xt::datasets::Cityscapes(\"./data\", std::move(weather_augmentation_pipeline));\n\n    // 3. The DataLoader will now provide images with realistic, simulated weather.\n    // xt::dataloaders::ExtendedDataLoader data_loader(dataset, 4);\n}\n</code></pre> <p>Computational Cost</p> <p>Simulating realistic weather effects can be more computationally expensive than simple geometric or color transforms. This may increase the data loading time. Consider the trade-off between the complexity of the augmentation and your training speed.</p>"},{"location":"api/transforms/weather/#available-weather-transforms","title":"Available Weather Transforms","text":"<p>xTorch provides a diverse set of simulations for various weather and environmental conditions.</p>"},{"location":"api/transforms/weather/#fog","title":"Fog","text":"Transform Description Header File <code>HomogeneousFog</code> Adds a uniform layer of fog across the entire image. <code>homogeneous_fog.h</code> <code>PatchyFog</code> Simulates fog with varying density across the image. <code>patchy_fog.h</code> <code>DepthBasedFog</code> Adds fog whose density increases with estimated distance from the camera (requires depth information). <code>depth_based_fog.h</code>"},{"location":"api/transforms/weather/#rain","title":"Rain","text":"Transform Description Header File <code>ParticleRain</code> Simulates rain by adding particle-like streaks to the image. <code>particle_rain.h</code> <code>StreakBasedRain</code> Simulates heavy rain with more prominent streak effects. <code>streak_based_rain.h</code> <code>FoggyRain</code> A combined effect that simulates rain occurring within a foggy environment. <code>foggy_rain.h</code> <code>WetGround</code> Simulates the appearance of wet, reflective ground surfaces caused by rain. <code>wet_ground.h</code>"},{"location":"api/transforms/weather/#snow-winter","title":"Snow &amp; Winter","text":"Transform Description Header File <code>FallingSnow</code> Simulates the effect of actively falling snow. <code>falling_snow.h</code> <code>AccumulatedSnow</code> Simulates the appearance of snow accumulated on surfaces. <code>accumulated_snow.h</code> <code>Blizzard</code> Simulates heavy, wind-blown snow with reduced visibility. <code>blizzard.h</code>"},{"location":"api/transforms/weather/#other-environmental-effects","title":"Other Environmental Effects","text":"Transform Description Header File <code>SunFlare</code> Simulates lens flare caused by a bright light source like the sun. <code>sun_flare.h</code> <code>DynamicShadows</code> Adds or modifies shadows in the image to simulate different times of day. <code>dynamic_shadows.h</code> <code>DustSandClouds</code> Simulates the effect of a dust storm or sandstorm, reducing visibility and adding a color cast. <code>dust_sand_clouds.h</code> <code>VegetationMotion</code> Simulates the motion blur of trees and plants caused by wind. <code>vegetation_motion.h</code> <p>Constructor Options</p> <p>Many of these effects are highly customizable. You can often control parameters like the intensity, density, direction, and color of the effect. Please refer to the specific header file for a full list of available settings.</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide provides a complete walkthrough for installing xTorch and its dependencies on Ubuntu.</p> <p>The process involves three main stages: 1.  Install System Dependencies: Set up the required development tools and libraries. 2.  Build the Project: Compile the xTorch library from the source code. 3.  Install the Library: Make xTorch available system-wide for your C++ projects.</p>"},{"location":"getting-started/installation/#1-prerequisites-system-dependencies","title":"1. Prerequisites: System Dependencies","text":"<p>Before building xTorch, you must install several essential system-level packages.</p>"},{"location":"getting-started/installation/#step-1-update-package-lists","title":"Step 1: Update Package Lists","text":"<p>First, ensure your package manager has the latest list of available software.</p> <pre><code>sudo apt-get update\n</code></pre>"},{"location":"getting-started/installation/#step-2-install-core-build-tools","title":"Step 2: Install Core Build Tools","text":"<p>Install <code>build-essential</code> (which includes the g++ compiler) and <code>cmake</code>.</p> <pre><code>sudo apt-get install -y build-essential cmake\n</code></pre>"},{"location":"getting-started/installation/#step-3-install-required-libraries","title":"Step 3: Install Required Libraries","text":"<p>Install Python and all other libraries required by xTorch with a single command.</p> <pre><code>sudo apt-get install -y \\\n    python3 \\\n    python3-venv \\\n    libopencv-dev \\\n    libhdf5-dev \\\n    libcurl4-openssl-dev \\\n    zlib1g-dev \\\n    libssl-dev \\\n    liblzma-dev \\\n    libarchive-dev \\\n    libtar-dev \\\n    libzip-dev \\\n    libglfw3-dev \\\n    libeigen3-dev \\\n    libsamplerate0-dev\n</code></pre>"},{"location":"getting-started/installation/#2-clone-and-build-xtorch","title":"2. Clone and Build xTorch","text":"<p>The build process is managed by CMake, which will automatically download and configure LibTorch and ONNX Runtime for you.</p>"},{"location":"getting-started/installation/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<p>Get the latest source code from the official GitHub repository.</p> <pre><code>git clone https://github.com/kamisaberi/xtorch\ncd xtorch\n</code></pre>"},{"location":"getting-started/installation/#step-2-create-a-build-directory","title":"Step 2: Create a Build Directory","text":"<p>It's standard practice to build the project in a separate directory to keep the source tree clean.</p> <pre><code>mkdir build\ncd build\n</code></pre>"},{"location":"getting-started/installation/#step-3-configure-with-cmake","title":"Step 3: Configure with CMake","text":"<p>Run <code>cmake</code> from the <code>build</code> directory. This will check for dependencies and prepare the build files.</p> <pre><code>cmake ..\n</code></pre> <p>Custom CUDA Path</p> <p>If your NVIDIA CUDA Toolkit is installed in a non-standard location, you'll need to tell CMake where to find the compiler (<code>nvcc</code>). First, find the path with <code>which nvcc</code>, then run cmake with the following flag: <pre><code>cmake -DCMAKE_CUDA_COMPILER='/path/to/your/nvcc' ..\n</code></pre></p>"},{"location":"getting-started/installation/#step-4-compile-the-library","title":"Step 4: Compile the Library","text":"<p>Use <code>make</code> to compile the entire project. The <code>-j$(nproc)</code> flag uses all available CPU cores to significantly speed up the process.</p> <pre><code>make -j$(nproc)\n</code></pre> <p>Once complete, the compiled shared library (<code>libxTorch.so</code>) will be available in the <code>build</code> directory.</p>"},{"location":"getting-started/installation/#3-install-the-library","title":"3. Install the Library","text":"<p>To make xTorch easily accessible to other C++ projects, install it system-wide. This command copies the header files, the compiled library, and CMake configuration files to standard system locations (like <code>/usr/local/lib</code> and <code>/usr/local/include</code>).</p> <p>From within the <code>build</code> directory, run:</p> <pre><code>sudo make install\n</code></pre>"},{"location":"getting-started/installation/#4-verify-the-installation","title":"4. Verify the Installation","text":"<p>You can verify that everything was built correctly by running the included unit tests and examples.</p>"},{"location":"getting-started/installation/#running-unit-tests","title":"Running Unit Tests","text":"<ol> <li>Navigate to the <code>test</code> directory in the source folder.     <pre><code>cd ../test\n</code></pre></li> <li>Create a build directory and configure with CMake. This will download the Google Test framework.     <pre><code>mkdir build\ncd build\ncmake ..\n</code></pre></li> <li>Compile and run the tests.     <pre><code>make\n./run_test\n</code></pre></li> </ol>"},{"location":"getting-started/installation/#building-and-running-examples","title":"Building and Running Examples","text":"<p>The main repository includes a few key examples to get you started.</p> <ol> <li>Navigate to the main <code>examples</code> directory (create it if it doesn't exist or use the one from the repo).</li> <li>Follow the same build process:     <pre><code>mkdir build\ncd build\ncmake ..\nmake\n</code></pre></li> <li>You can now run any of the compiled examples, such as:     <pre><code># Run the LeNet MNIST example\n./classifying_handwritten_digits_with_lenet_on_mnist\n\n# Run the DCGAN image generation example\n./generating_images_with_dcgan\n</code></pre></li> </ol> <p>Installation Complete!</p> <p>xTorch is now successfully installed and ready to be used in your C++ applications. For a comprehensive collection of advanced examples, check out the dedicated xtorch-examples repository.</p>"},{"location":"getting-started/quick-start-cnn/","title":"Quick Start: Training a CNN","text":"<p>This guide will walk you through a complete, end-to-end example of training a simple Convolutional Neural Network (CNN) using xTorch. It is designed to be your first practical step after completing the Installation.</p> <p>We will train the classic LeNet-5 model on the MNIST dataset of handwritten digits.</p> <p>This example showcases the core components of the xTorch workflow: 1.  Defining data transformations. 2.  Loading a dataset and creating a <code>DataLoader</code>. 3.  Initializing a pre-built model. 4.  Configuring the <code>Trainer</code> for a seamless training loop.</p>"},{"location":"getting-started/quick-start-cnn/#the-workflow-explained","title":"The Workflow Explained","text":""},{"location":"getting-started/quick-start-cnn/#step-1-data-transformations","title":"Step 1: Data Transformations","text":"<p>Before feeding data to the model, we need to process it. Here, we define a sequence of two transformations: - <code>Resize</code>: The LeNet-5 model expects <code>32x32</code> images, so we resize the <code>28x28</code> MNIST images. - <code>Normalize</code>: We scale the pixel values to a standard range, which helps the model converge faster.</p> <p>These are combined into a single pipeline using <code>xt::transforms::Compose</code>.</p>"},{"location":"getting-started/quick-start-cnn/#step-2-dataset-and-dataloader","title":"Step 2: Dataset and DataLoader","text":"<p>We load the built-in <code>xt::datasets::MNIST</code>. The <code>ExtendedDataLoader</code> then takes this dataset and prepares batches of data for the GPU. It handles shuffling, parallel data loading (<code>num_workers</code>), and batching automatically.</p>"},{"location":"getting-started/quick-start-cnn/#step-3-model-and-optimizer","title":"Step 3: Model and Optimizer","text":"<p>We instantiate a pre-built <code>xt::models::LeNet5</code> model from the xTorch model zoo. We then define a <code>torch::optim::Adam</code> optimizer and tell it which parameters (from our model) it should update.</p>"},{"location":"getting-started/quick-start-cnn/#step-4-the-trainer","title":"Step 4: The Trainer","text":"<p>The <code>xt::Trainer</code> is the heart of the xTorch workflow. It abstracts away the boilerplate of a typical training loop. We configure it with: - The maximum number of epochs to run. - The optimizer to use. - The loss function to calculate (<code>torch::nll_loss</code>). - A <code>LoggingCallback</code> to print progress to the console.</p> <p>Finally, we call <code>trainer.fit()</code>, passing in the model and data loaders. The trainer handles the entire process: iterating through epochs, moving data to the correct device, performing forward and backward passes, updating weights, and logging metrics.</p>"},{"location":"getting-started/quick-start-cnn/#full-c-code","title":"Full C++ Code","text":"<p>This is the complete code for our training pipeline. You can find this example in the <code>examples</code> directory of the repository.</p> <pre><code>#include &lt;xtorch/xtorch.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    // Ensure floating point numbers are printed with sufficient precision\n    std::cout.precision(10);\n\n    // --- 1. Define Data Transformations ---\n    auto compose = std::make_unique&lt;xt::transforms::Compose&gt;(\n        // Resize images from 28x28 to the 32x32 expected by LeNet\n        std::make_shared&lt;xt::transforms::image::Resize&gt;(std::vector&lt;int64_t&gt;{32, 32}),\n        // Normalize pixel values to have a mean of 0.5 and stddev of 0.5\n        std::make_shared&lt;xt::transforms::general::Normalize&gt;(std::vector&lt;float&gt;{0.5}, std::vector&lt;float&gt;{0.5})\n    );\n\n    // --- 2. Load Dataset and Create DataLoader ---\n    // Note: Replace \"/path/to/datasets/\" with the actual path on your system\n    auto dataset = xt::datasets::MNIST(\n        \"/path/to/datasets/\",\n        xt::datasets::DataMode::TRAIN,\n        /*download=*/true, // Set to true to download if not found\n        std::move(compose)\n    );\n\n    xt::dataloaders::ExtendedDataLoader data_loader(\n        dataset,\n        /*batch_size=*/64,\n        /*shuffle=*/true,\n        /*num_workers=*/2\n    );\n\n    // --- 3. Initialize Model and Optimizer ---\n    xt::models::LeNet5 model(/*num_classes=*/10);\n    torch::Device device(torch::cuda::is_available() ? torch::kCUDA : torch::kCPU);\n    model.to(device);\n    model.train(); // Set the model to training mode\n\n    torch::optim::Adam optimizer(model.parameters(), torch::optim::AdamOptions(1e-3));\n\n    // --- 4. Configure and Run the Trainer ---\n    xt::Trainer trainer;\n    auto logger = std::make_shared&lt;xt::LoggingCallback&gt;(\"[LeNet-MNIST]\", /*log_every_N_batches=*/100);\n\n    trainer.set_max_epochs(10)\n           .set_optimizer(optimizer)\n           .set_loss_fn([](const auto&amp; output, const auto&amp; target) {\n               return torch::nll_loss(output, target);\n           })\n           .add_callback(logger);\n\n    // The `fit` method starts the training process\n    trainer.fit(model, data_loader, /*validation_loader=*/nullptr, device);\n\n    std::cout &lt;&lt; \"\\nTraining finished!\" &lt;&lt; std::endl;\n\n    return 0;\n}\n</code></pre>"},{"location":"getting-started/quick-start-cnn/#how-to-compile-and-run","title":"How to Compile and Run","text":"<p>Assuming you have successfully installed xTorch system-wide, you can compile this file with a simple <code>CMakeLists.txt</code>.</p> <ol> <li> <p>Create a <code>CMakeLists.txt</code> file: <pre><code>cmake_minimum_required(VERSION 3.16)\nproject(QuickStartCNN)\n\nset(CMAKE_CXX_STANDARD 17)\n\n# Find the installed xTorch library\nfind_package(xTorch REQUIRED)\n\nadd_executable(quick_start main.cpp)\n\n# Link the executable against xTorch\ntarget_link_libraries(quick_start PRIVATE xTorch::torch xTorch::xtorch)\n</code></pre></p> </li> <li> <p>Build the project: <pre><code>mkdir build\ncd build\ncmake ..\nmake\n</code></pre></p> </li> <li> <p>Run the executable: <pre><code>./quick_start\n</code></pre></p> </li> </ol>"},{"location":"getting-started/quick-start-cnn/#expected-output","title":"Expected Output","text":"<p>When you run the program, you will see the <code>LoggingCallback</code> printing the training progress to your console, similar to this:</p> <pre><code>[LeNet-MNIST] Epoch 1/10, Batch 100/938 - Loss: 0.3125487566 - Time per batch: 5.23ms\n[LeNet-MNIST] Epoch 1/10, Batch 200/938 - Loss: 0.1568745211 - Time per batch: 4.98ms\n...\n[LeNet-MNIST] Epoch 10/10, Batch 800/938 - Loss: 0.0215478943 - Time per batch: 4.81ms\n[LeNet-MNIST] Epoch 10/10, Batch 900/938 - Loss: 0.0458712354 - Time per batch: 4.85ms\n\nTraining finished!\n</code></pre>"},{"location":"getting-started/quick-start-cnn/#next-steps","title":"Next Steps","text":"<p>Congratulations! You've successfully trained your first neural network using xTorch.</p> <p>From here, you can explore more advanced topics: -   User Guide: Dive deeper into the concepts of the Trainer and Data Handling. -   Examples: Browse the full list of Examples &amp; Tutorials for more complex tasks in computer vision, NLP, and more.</p>"},{"location":"user-guide/architecture/","title":"Architecture &amp; Design Philosophy","text":"<p>The design of xTorch is guided by a simple yet powerful principle: extend, don't reinvent. Instead of creating a new deep learning framework from scratch, xTorch is architected as a thin, non-intrusive layer on top of PyTorch's C++ API (LibTorch). This approach allows xTorch to provide a high-level, user-friendly experience without sacrificing the underlying performance and flexibility of LibTorch's computational engine.</p> <p>The core philosophy is to provide the \"batteries-included\" components that make the Python experience productive, directly in C++.</p>"},{"location":"user-guide/architecture/#core-principles","title":"Core Principles","text":"<ol> <li> <p>Leverage LibTorch's Power: xTorch relies entirely on LibTorch for its core functionality. All tensor operations, automatic differentiation (autograd), and neural network primitives (<code>torch::nn</code>) are delegated to the battle-tested LibTorch backend. We focus on the API, not the engine.</p> </li> <li> <p>Usability and Productivity: The primary goal is to reduce boilerplate and make the C++ API as expressive and intuitive as its Python counterpart. This is achieved through high-level abstractions like the <code>Trainer</code> loop and pre-built data loaders.</p> </li> <li> <p>Modularity: The library is organized into distinct, logical modules (<code>models</code>, <code>datasets</code>, <code>transforms</code>, <code>losses</code>, etc.). This makes the framework easy to navigate, learn, and contribute to. You can use as much or as little of xTorch as you need.</p> </li> </ol>"},{"location":"user-guide/architecture/#architectural-layers","title":"Architectural Layers","text":"<p>xTorch's architecture can be visualized as a simple stack. Your application code interacts with the high-level xTorch API, which in turn orchestrates the powerful but lower-level LibTorch core.</p> <pre><code>graph TD\n    subgraph User Space\n        A[Your C++ Application]\n    end\n\n    subgraph xTorch Abstraction Layer\n        B(trainer.fit(model, data_loader, ...))\n        C{Trainer, Callbacks, Logging}\n        D{Datasets, DataLoaders, Transforms}\n        E{Pre-built Models, Losses, Optimizers}\n    end\n\n    subgraph LibTorch Core Engine\n        F[torch::Tensor]\n        G[torch::autograd]\n        H[torch::nn]\n        I[torch::optim]\n    end\n\n    A --&gt; B\n    B --&gt; C\n    B --&gt; D\n    B --&gt; E\n    C &amp; D &amp; E --&gt; F &amp; G &amp; H &amp; I\n</code></pre> <p>Let's break down these layers:</p>"},{"location":"user-guide/architecture/#1-bottom-layer-the-libtorch-core-engine","title":"1. Bottom Layer: The LibTorch Core Engine","text":"<p>This is the foundation. It provides all the essential building blocks for deep learning: - <code>torch::Tensor</code>: The fundamental multi-dimensional array object. - <code>torch::autograd</code>: The engine for automatically computing gradients. - <code>torch::nn</code>: Primitives for building neural networks, like <code>Linear</code>, <code>Conv2d</code>, and activation functions. - <code>torch::optim</code>: Core optimizers like <code>Adam</code> and <code>SGD</code>.</p> <p>xTorch does not modify this layer; it simply uses it as a robust and high-performance backend.</p>"},{"location":"user-guide/architecture/#2-middle-layer-the-xtorch-abstraction-layer","title":"2. Middle Layer: The xTorch Abstraction Layer","text":"<p>This is where xTorch adds its value. It provides a set of C++ classes and functions that wrap common patterns and encapsulate complexity. This layer is designed for convenience and rapid development. Key components include: - <code>xt::Trainer</code>: A complete, abstracted training and validation loop. - <code>xt::datasets::ImageFolder</code>: A simple way to load image data from a directory structure. - <code>xt::dataloaders::ExtendedDataLoader</code>: A powerful data loader with parallel processing. - <code>xt::models::ResNet</code>: An example of a pre-built, ready-to-use model architecture. - <code>xt::transforms</code>: A rich library of data augmentation and preprocessing functions.</p>"},{"location":"user-guide/architecture/#3-top-layer-the-user-application","title":"3. Top Layer: The User Application","text":"<p>This is your code. By using xTorch, your application logic becomes cleaner and more focused on the high-level aspects of your model and experiment. Instead of manually writing loops, moving data to devices, and calculating gradients, you interact with the intuitive xTorch API.</p>"},{"location":"user-guide/architecture/#key-modules","title":"Key Modules","text":"<p>The xTorch library is organized into several key modules, each serving a specific purpose in the ML workflow.</p> Module Purpose &amp; Key Components Model Module Provides high-level model classes and a zoo of pre-built architectures. It simplifies model definition and reduces boilerplate. Includes <code>XTModule</code> and ready-made models like <code>LeNet5</code>, <code>ResNet</code>, and <code>DCGAN</code>. Data Module Streamlines data loading and preprocessing. Contains enhanced <code>Dataset</code> classes (<code>ImageFolderDataset</code>, <code>CSVDataset</code>) and a powerful <code>ExtendedDataLoader</code> with built-in support for transformations. Training Module Contains the core logic for model training and validation. The centerpiece is the <code>Trainer</code> class, which automates the training loop, supported by a system of <code>Callbacks</code> for logging, checkpointing, and metrics tracking. Transforms Module Offers a vast library of data augmentation and preprocessing functions for various modalities like images, audio, and text. Mirroring the functionality of <code>torchvision.transforms</code>. Utilities Module A collection of helper functions and classes for common tasks, including logging, device management (<code>torch::kCUDA</code> vs. <code>torch::kCPU</code>), model summary printing, and filesystem operations. Extended Optimizers &amp; Losses Provides implementations of newer or more advanced optimizers (<code>AdamW</code>, <code>RAdam</code>) and loss functions that may not be available in the core LibTorch library. <p>This modular and layered architecture allows xTorch to provide a modern, productive deep learning experience in C++, bridging the gap left by the core PyTorch C++ API.</p>"}]}